---
title: Prompt Management (Roadmap & Docs)
description: End-to-end prompt CMS for LangWatch ‚Äî roadmap, concepts, SDKs & integrations
status: work-in-progress
---

> **Info**  
> This document is a *living roadmap*. Sections flagged **Coming Soon** outline features that are not yet publicly available.

## Overview & Benefits

### What is Prompt Management?

Prompt Management in **LangWatch** is a first-class **Prompt CMS**. It lets you store, version, label, compose, test, and monitor all your prompts from a single place while keeping latency low and reliability high.

### Why Prompt Management?

* **Decouple** prompt text from code ‚Äì deploy new prompts without redeploying your app.
* **Version control** ‚Äì roll back instantly when a change under-performs.
* **Analytics & tracing** ‚Äì correlate prompt versions with downstream evaluations and costs.
* **Collaboration** ‚Äì empower non-technical team members to ship prompt updates.

See also our existing [Prompt Versioning guide](/features/prompt-versioning).

### Quick Start

Get up and running in 30 seconds:

<CodeGroup>

```ts JavaScript
// 1. Create your first prompt
const prompt = await langwatch.prompts.create({
  name: "greeting",
  configData: {
    prompt: "Hello! How can I help you today?",
    model: "gpt-3.5-turbo"
  }
});

// 2. Use it in your application
const greeting = await langwatch.getPrompt("greeting");
const response = await openai.chat.completions.create({
  model: greeting.config.model,
  messages: [{ role: "system", content: greeting.compile({}) }]
});
```

```py Python
# 1. Create your first prompt
prompt = langwatch.prompts.create(
    name="greeting",
    config_data={
        "prompt": "Hello! How can I help you today?",
        "model": "gpt-3.5-turbo"
    }
)

# 2. Use it in your application
greeting = langwatch.get_prompt("greeting")
response = openai.chat.completions.create(
    model=greeting.config["model"],
    messages=[{"role": "system", "content": greeting.compile({})}]
)
```

```bash REST
# 1. Create prompt
curl -X POST -H "Content-Type: application/json" -H "X-Auth-Token: $LW_API_KEY" \
  -d '{"name":"greeting","configData":{"prompt":"Hello! How can I help you today?","model":"gpt-3.5-turbo"}}' \
  https://app.langwatch.ai/api/prompts

# 2. Use the prompt ID returned from step 1
curl -H "X-Auth-Token: $LW_API_KEY" \
  https://app.langwatch.ai/api/prompts/greeting
```

</CodeGroup>

That's it! You now have a managed prompt that can be updated without code changes.

## Understanding Prompts

### Prompt Basics

Every prompt in LangWatch is a versioned configuration object with these key components:

```json
{
  "id": "prompt_abc123",
  "name": "movie-critic",                    // Unique prompt name
  "type": "chat",                           // "text" or "chat" format
  "version": 3,                             // Current version number
  "model": "gpt-3.5-turbo",                 // Default model
  "prompt": null,                           // Text template (for text type)
  "messages": [                             // Chat messages (for chat type)
    {
      "role": "system",
      "content": "You are a {{criticLevel}} movie critic."
    },
    {
      "role": "user",
      "content": "What do you think of {{movie}}?"
    }
  ]
}
```

Key concepts:
- **Immutable versions** ‚Äì every update creates a new version
- **Dual format support** ‚Äì text templates or chat messages
- **Variable syntax** ‚Äì `{{variableName}}` syntax for dynamic content

### Versions & Labels

**Versions** are immutable. When you update a prompt, you create a new version with a higher number.

**Labels** are pointers to specific versions:
- `latest` ‚Äì always points to the newest version
- `production` ‚Äì points to your live version
- Custom labels ‚Äì `staging`, `experimental`, etc.

<CodeGroup>

```ts JavaScript
// Create a new version
await prompt.newVersion({
  commitMessage: "Improved system prompt",
  configData: { prompt: "You are a helpful assistant." }
});

// Promote to production
await prompt.update({ labels: ["production"] });

// Fetch specific versions
const latest = await langwatch.getPrompt("movie-critic", { label: "latest" });
const production = await langwatch.getPrompt("movie-critic"); // defaults to production
```

```py Python
# Create new version
prompt.new_version(
    commit_message="Improved system prompt",
    config_data={"prompt": "You are a helpful assistant."}
)

# Promote to production
prompt.update(labels=["production"])

# Fetch versions
latest = langwatch.get_prompt("movie-critic", label="latest")
production = langwatch.get_prompt("movie-critic")  # defaults to production
```

</CodeGroup>

### Text vs Chat Types *(Coming Soon)*

**Text Type (`type: "text"`):**
- Single string template: `"Translate {{text}} from {{source}} to {{target}}"`
- Best for: completion models, simple instructions

**Chat Type (`type: "chat"`):**
- Array of messages with roles (system, user, assistant)
- Best for: chat models, conversations, complex instructions
- Variables work in any message content

## Managing Your Prompts

### Virtual Folders *(Coming Soon)*

Organize prompts in a hierarchy using forward slashes in the name:

<CodeGroup>

```ts JavaScript
await prompt.update({ name: "onboarding/welcome-email" });
await prompt.update({ name: "onboarding/follow-up" });
await prompt.update({ name: "support/troubleshooting" });

// List everything under a folder
const onboardingPrompts = await langwatch.prompts.list({ folder: "onboarding/" });
```

```py Python
prompt.update(name="onboarding/welcome-email")

# List folder contents
onboarding_prompts = langwatch.prompts.list(folder="onboarding/")
```

</CodeGroup>

### Tags *(Coming Soon)*

**Tags** let you slice prompts across dimensions (domain, language, owner):

<CodeGroup>

```ts JavaScript
await prompt.update({ tags: ["marketing", "german", "email"] });

// Query by tags
const germanPrompts = await langwatch.prompts.list({ tags: ["german"] });
const marketingEmails = await langwatch.prompts.list({ tags: ["marketing", "email"] });
```

```py Python
prompt.update(tags=["marketing", "german", "email"])

german_prompts = langwatch.prompts.list(tags=["german"])
```

</CodeGroup>

### Team Collaboration

LangWatch's prompt management system is designed to enable effective collaboration between technical and non-technical team members. Here's how we recommend structuring your workflow:

**Recommended Collaboration Workflow:**
1. **Developers** create initial prompt structure with proper input/output schemas
2. **Domain experts** (product managers, content specialists) refine prompt text and examples
3. **QA teams** test prompts using different labels before promoting to production
4. **Operations** monitor performance and coordinate rollbacks when needed

**How LangWatch Facilitates This:**
- **Commit messages** ‚Äì every version includes author, timestamp, and clear change description
- **Role-based permissions** ‚Äì restrict production deployments to authorized team members
- **Complete audit trail** ‚Äì track who changed what and when for compliance and debugging
- **Code-free editing** ‚Äì non-technical team members can iterate on prompts through the UI
- **Safe experimentation** ‚Äì use staging labels to test changes without affecting production

**Best Practices for Team Workflows:**
- Establish clear naming conventions that your whole team understands
- Use descriptive commit messages that explain the business reasoning behind changes
- Create approval processes for production deployments using role permissions
- Set up monitoring and alerts so the team knows when prompt performance changes

## Integration & Usage

### Basic Integration

<CodeGroup>

```ts JavaScript
import { LangWatch } from "langwatch/prompts";

const langwatch = new LangWatch({ apiKey: process.env.LANGWATCH_API_KEY });

// Get and use a prompt
const prompt = await langwatch.getPrompt("customer-classifier");
const compiled = prompt.compile({ query: userInput });

// Use with OpenAI
const response = await openai.chat.completions.create({
  model: prompt.config.model,
  messages: [{ role: "user", content: compiled }]
});
```

```py Python
import langwatch

# Get and use a prompt
prompt = langwatch.get_prompt("customer-classifier")
compiled = prompt.compile({"query": user_input})

# Use with OpenAI
response = openai.chat.completions.create(
    model=prompt.config["model"],
    messages=[{"role": "user", "content": compiled}]
)
```

```bash REST
# Fetch prompt
curl -H "X-Auth-Token: $LW_API_KEY" \
  https://app.langwatch.ai/api/prompts/customer-classifier

# Use the returned prompt data in your application
```

</CodeGroup>

### Variables & Dynamic Content

Define typed inputs for dynamic content in your prompts using `{{variableName}}` syntax:

<CodeGroup>

```ts JavaScript
// Create prompt with input schema
await langwatch.prompts.create({
  name: "product-review",
  configData: {
    prompt: "Review this {{productType}}: {{productName}}\nPrice: ${{price}}",
    inputs: [
      { identifier: "productType", type: "str" },
      { identifier: "productName", type: "str" },
      { identifier: "price", type: "float" }
    ],
    model: "gpt-4"
  }
});

// Compile with type validation
const prompt = await langwatch.getPrompt("product-review");
const compiled = prompt.compile({
  productType: "laptop",
  productName: "MacBook Pro",
  price: 2499.99
});
```

```py Python
# Create with input schema
langwatch.prompts.create(
    name="product-review",
    config_data={
        "prompt": "Review this {{productType}}: {{productName}}\nPrice: ${{price}}",
        "inputs": [
            {"identifier": "productType", "type": "str"},
            {"identifier": "productName", "type": "str"},
            {"identifier": "price", "type": "float"}
        ],
        "model": "gpt-4"
    }
)

# Compile with validation
compiled = prompt.compile({
    "productType": "laptop",
    "productName": "MacBook Pro",
    "price": 2499.99
})
```

</CodeGroup>

**Supported input types:** `str`, `float`, `bool`, `image`, `list[str]`, `list[float]`, `list[int]`, `list[bool]`, `dict`

### Caching & Performance *(Coming Soon)*

Zero-latency prompt fetching with intelligent caching:

<CodeGroup>

```ts JavaScript
// Configure caching
const prompt = await langwatch.getPrompt("critical-system-prompt", {
  cacheTtlSeconds: 300,  // 5 minute cache
  fallbackStrategy: "stale-while-revalidate"
});

// Pre-warm cache on startup
await langwatch.prompts.preload([
  "critical-system-prompt",
  "user-onboarding", 
  "error-handling"
]);
```

```py Python
# Configure caching
prompt = langwatch.get_prompt(
    "critical-system-prompt",
    cache_ttl_seconds=300,
    fallback_strategy="stale-while-revalidate"
)

# Pre-warm cache
langwatch.prompts.preload([
    "critical-system-prompt",
    "user-onboarding"
])
```

</CodeGroup>

### Trace Linking *(Coming Soon)*

Automatic connection between prompt versions and trace outcomes:

<CodeGroup>

```ts JavaScript
// SDK automatically captures prompt metadata in traces
const prompt = await langwatch.getPrompt("customer-classifier");
const response = await openai.chat.completions.create({
  model: prompt.config.model,
  messages: [{ role: "user", content: prompt.compile({ query: userInput }) }],
  // Prompt version automatically captured in trace
});
```

```py Python
# Automatic trace linking
prompt = langwatch.get_prompt("customer-classifier")
response = openai.chat.completions.create(
    model=prompt.config["model"],
    messages=[{"role": "user", "content": prompt.compile({"query": user_input})}],
    # Prompt version captured automatically
)
```

</CodeGroup>

### LangWatch Evaluations Integration

The Prompt Manager integrates seamlessly with LangWatch's evaluation system. All prompt nodes in the Optimization Studio use the centralized prompt management system, enabling you to:

**Evaluate Managed Prompts:**
- Run batch evaluations on specific prompt versions
- Compare performance across different prompt variants
- Automatically track which prompt version generated each evaluation result

**Optimization Studio Integration:**
- All prompt nodes automatically pull from the Prompt Manager
- Changes to prompts in the manager instantly reflect in your evaluation workflows
- Version history lets you correlate evaluation performance with specific prompt changes

<Frame caption="Prompt Manager integration in Optimization Studio">
  <img className="block" src="/images/prompt-manager-optimization-studio.png" alt="Screenshot showing prompt nodes in Optimization Studio using managed prompts" />
</Frame>

<Frame caption="Evaluation results linked to prompt versions">
  <img className="block" src="/images/evaluation-prompt-versions.png" alt="Screenshot showing evaluation results correlated with specific prompt versions" />
</Frame>

**Benefits:**
- **Consistent evaluation** ‚Äì ensure all team members evaluate the same prompt version
- **Historical analysis** ‚Äì see how prompt changes impact evaluation scores over time
- **Automated correlation** ‚Äì evaluation results automatically link to prompt versions
- **Workflow integration** ‚Äì use evaluation results to guide prompt promotion decisions

## Advanced Capabilities

### Prompt Composability *(Coming Soon)*

Build complex prompts from reusable components:

<CodeGroup>

```ts JavaScript
// Create a base system prompt
await langwatch.prompts.create({
  name: "base/helpful-assistant",
  configData: {
    prompt: "You are a helpful assistant. Always be concise and accurate."
  }
});

// Compose it into specialized prompts
await langwatch.prompts.create({
  name: "customer-support/email-reply", 
  configData: {
    prompt: `@@@langwatchPrompt:name=base/helpful-assistant|label=production@@@

You are responding to: {{customerEmail}}

Guidelines:
- Be empathetic and professional
- Provide actionable solutions`
  }
});

// References are resolved automatically
const prompt = await langwatch.getPrompt("customer-support/email-reply");
```

```py Python
# Compose prompts using references
langwatch.prompts.create(
    name="customer-support/email-reply",
    config_data={
        "prompt": """@@@langwatchPrompt:name=base/helpful-assistant|label=production@@@

You are responding to: {{customerEmail}}

Guidelines:
- Be empathetic and professional
- Provide actionable solutions"""
    }
)
```

</CodeGroup>

### A/B Testing

Test prompt variants using labels and your own traffic splitting:

<CodeGroup>

```ts JavaScript
// Create variants with different labels
await prompt.newVersion({ commitMessage: "Version A - original" });
await prompt.update({ labels: ["prod-a"] });

await prompt.newVersion({ commitMessage: "Version B - more engaging" });
await prompt.update({ labels: ["prod-b"] });

// Implement traffic splitting
const userId = getCurrentUserId();
const variant = userId % 2 === 0 ? "prod-a" : "prod-b";
const selectedPrompt = await langwatch.getPrompt("onboarding/welcome", { label: variant });

// LangWatch automatically tracks metrics by prompt version
```

```py Python
import random

# Create labeled versions
prompt.update(labels=["prod-a"])
# ... create version B with prod-b label

# Simple random split
variant = random.choice(["prod-a", "prod-b"])
selected_prompt = langwatch.get_prompt("onboarding/welcome", label=variant)

# Metrics tracked automatically by version
```

</CodeGroup>

### DSPy Optimization *(Coming Soon)*

Automatically improve prompts using DSPy's optimization framework:

<CodeGroup>

```ts JavaScript
// Create optimization task
const optimization = await langwatch.dspy.createOptimization({
  name: "customer-classifier",
  signature: "question -> category",
  trainingDataset: "customer_queries_dataset",
  optimizer: "BootstrapFewShot",
  maxDemonstrations: 8
});

// Run optimization and deploy results
await langwatch.dspy.optimize(optimization.id);
await langwatch.dspy.deploy(optimization.id, {
  promptName: "customer-classifier",
  label: "production"
});
```

```py Python
# DSPy optimization
optimization = langwatch.dspy.create_optimization(
    name="customer-classifier",
    signature="question -> category",
    training_dataset="customer_queries_dataset",
    optimizer="BootstrapFewShot"
)

langwatch.dspy.optimize(optimization.id)
langwatch.dspy.deploy(optimization.id, prompt_name="customer-classifier")
```

</CodeGroup>

### External Integrations *(Coming Soon)*

- **n8n Node** ‚Äì Automate prompt workflows without code
- **Langflow Integration** ‚Äì Visual prompt chaining and testing
- **MCP Server** ‚Äì Enable AI assistants to manage prompts directly

## Examples & Tutorials

### Building a RAG System

<CodeGroup>

```ts JavaScript
// Create RAG system prompt
await langwatch.prompts.create({
  name: "rag/qa-system",
  configData: {
    prompt: `Answer questions based on the provided context.

Context: {{context}}
Question: {{question}}

Instructions:
- Only answer based on the context
- If context lacks info, say "I don't have enough information"
- Be concise and accurate`,
    inputs: [
      { identifier: "context", type: "str" },
      { identifier: "question", type: "str" }
    ],
    model: "gpt-4"
  }
});

// Use in RAG pipeline
async function answerQuestion(question: string) {
  const context = await retrieveRelevantDocs(question);
  const prompt = await langwatch.getPrompt("rag/qa-system");
  
  return await openai.chat.completions.create({
    model: prompt.config.model,
    messages: [{
      role: "user", 
      content: prompt.compile({ context, question })
    }]
  });
}
```

```py Python
# Create RAG prompt
langwatch.prompts.create(
    name="rag/qa-system",
    config_data={
        "prompt": """Answer questions based on the provided context.

Context: {{context}}
Question: {{question}}

Instructions:
- Only answer based on the context
- If context lacks info, say "I don't have enough information"
- Be concise and accurate""",
        "inputs": [
            {"identifier": "context", "type": "str"},
            {"identifier": "question", "type": "str"}
        ],
        "model": "gpt-4"
    }
)

# Use in RAG
def answer_question(question: str):
    context = retrieve_relevant_docs(question)
    prompt = langwatch.get_prompt("rag/qa-system")
    
    return openai.chat.completions.create(
        model=prompt.config["model"],
        messages=[{
            "role": "user",
            "content": prompt.compile({"context": context, "question": question})
        }]
    )
```

</CodeGroup>

### Multi-Step Workflows

<CodeGroup>

```ts JavaScript
// Create prompts for each step
await langwatch.prompts.create({
  name: "analysis/extract-entities",
  configData: {
    prompt: "Extract key entities from: {{text}}",
    model: "gpt-3.5-turbo"
  }
});

await langwatch.prompts.create({
  name: "analysis/classify-sentiment", 
  configData: {
    prompt: "Classify sentiment of: {{text}}\nEntities: {{entities}}",
    model: "gpt-3.5-turbo"
  }
});

// Chain them together
async function analyzeText(text: string) {
  const extractPrompt = await langwatch.getPrompt("analysis/extract-entities");
  const entities = await callLLM(extractPrompt.compile({ text }));
  
  const sentimentPrompt = await langwatch.getPrompt("analysis/classify-sentiment");
  return await callLLM(sentimentPrompt.compile({ text, entities }));
}
```

```py Python
# Multi-step analysis workflow
def analyze_text(text: str):
    # Step 1: Extract entities
    extract_prompt = langwatch.get_prompt("analysis/extract-entities")
    entities = call_llm(extract_prompt.compile({"text": text}))
    
    # Step 2: Classify sentiment
    sentiment_prompt = langwatch.get_prompt("analysis/classify-sentiment") 
    return call_llm(sentiment_prompt.compile({"text": text, "entities": entities}))
```

</CodeGroup>

## Best Practices

### Naming Conventions
- Use forward slashes for hierarchy: `domain/feature/specific-use`
- Be descriptive: `customer-support/escalation-email` not `cs-esc`
- Include context: `rag/product-qa` not just `qa`

### Version Management
- Always include meaningful commit messages
- Use production labels conservatively
- Test new versions with staging labels first
- Keep a rollback plan with previous production versions

### Performance Optimization
- Enable caching for frequently-used prompts
- Pre-load critical prompts on application startup
- Use appropriate cache TTL based on update frequency
- Monitor prompt performance through trace linking

## Technical Reference

### Complete Schema Reference

The full version schema includes rich configuration for advanced features:

```json
{
  "id": "version_xyz789",
  "authorId": "user_123", 
  "projectId": "proj_456",
  "configId": "prompt_abc123",
  "schemaVersion": "1.0",
  "commitMessage": "Improved movie critic persona",
  "version": 3,
  "createdAt": "2024-01-15T10:30:00Z",
  "configData": {
    "type": "chat",
    "prompt": null,
    "messages": [
      {"role": "system", "content": "You are a {{criticLevel}} movie critic."},
      {"role": "user", "content": "What do you think of {{movie}}?"}
    ],
    "inputs": [
      {"identifier": "criticLevel", "type": "str"},
      {"identifier": "movie", "type": "str"}
    ],
    "outputs": [
      {"identifier": "review", "type": "str"}
    ],
    "model": "gpt-3.5-turbo",
    "temperature": 0.7,
    "max_tokens": 500,
    "demonstrations": {
      "columns": [
        {"id": "input", "name": "Input", "type": "string"},
        {"id": "expected", "name": "Expected Output", "type": "string"}
      ],
      "rows": [
        {
          "id": "demo_1", 
          "input": "Inception",
          "expected": "A mind-bending masterpiece that explores dreams within dreams."
        }
      ]
    }
  }
}
```

### Type System Details

**Input Types:**
- `str` ‚Äì String values
- `float` ‚Äì Numeric values (decimals)
- `bool` ‚Äì True/false values
- `image` ‚Äì Base64 encoded images
- `list[str]`, `list[float]`, `list[int]`, `list[bool]` ‚Äì Arrays of values
- `dict` ‚Äì Object/dictionary structures

**Output Schema:**
- `str` ‚Äì Plain text responses
- `float` ‚Äì Numeric scores/values
- `json_schema` ‚Äì Structured JSON with validation schema

**Validation Rules:**
- `type` field is required and must be "text" or "chat"
- For `type: "text"`: `prompt` required, `messages` must be null
- For `type: "chat"`: `messages` required, `prompt` must be null
- Variable names in `{{}}` must match input identifiers

### API Documentation

For complete REST API documentation:
- [Prompts API Reference](/api-reference/prompts/overview)
- [Create Prompt](/api-reference/prompts/create-prompt)
- [Get Prompt](/api-reference/prompts/get-prompt)
- [Update Prompt](/api-reference/prompts/update-prompt)

## Feature Roadmap

| Priority | Feature | **Available Via** | Status |
|----------|---------|-------------------|--------|
| 1 | Prompt Config & Versioning (CRUD) | API & SDK (JS, Py) | ‚úÖ |
| 2 | Prompt Type System (text vs chat) | API & SDK (JS, Py) | üöß |
| 3 | Virtual Folders | API & SDK (JS, Py) | üöß |
| 4 | Tags | API & SDK (JS, Py) | üöß |
| 5 | Labels (auto & protected) | API & SDK (JS, Py) | üöß |
| 6 | Prompt Composability | API & SDK (JS, Py) | üöß |
| 7 | Variables (`{{variable}}`) | SDK (JS, Py) | ‚úÖ (Python SDK only) |
| 8 | Caching & Guaranteed Availability | SDK (JS, Py) | üöß |
| 9 | Trace Linking | SDK (JS, Py) | üöß |
| 10 | Integrations (n8n, Langflow, MCP) | External Tools | üöß |
| 11 | DSPy Integration | API & SDK (JS, Py) | üöß |

**Status Legend:**
- ‚úÖ **available** - Feature is implemented and ready to use
- ‚òëÔ∏è **partially available** - Incomplete feature
- üöß **coming soon** - Feature is planned for future release

*Priority order reflects development sequence. Features available via API can be used directly; SDK-only features require multi-step API calls for custom implementations.*
