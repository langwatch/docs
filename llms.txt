# LangWatch

# FILE: ./introduction.mdx

---
title: Introduction
description: Welcome to LangWatch, the all-in-one [open-source](https://github.com/langwatch/langwatch) LLMops platform.
---

LangWatch allows you to track, monitor, guardrail and evaluate your LLMs apps for measuring quality and alert on issues.

For domain experts, it allows you to easily sift through conversations, see topics being discussed and annotate and score messages
for improvement in a collaborative manner with the development team.

For developers, it allows you to debug, build datasets, prompt engineer on the playground and
run batch evaluations or [DSPy experiments](./dspy-visualization/quickstart) to continuously improve the product.

Finally, for the business, it allows you to track conversation metrics and give full user and quality analytics, cost tracking, build
custom dashboards and even integrate it back on your own platform for reporting to your customers.

You can [sign up](https://app.langwatch.ai/) and already start the integration on our free tier by following the guides bellow:

<CardGroup cols={2}>
  <Card title="Python Integration Guide" icon="link" href="./integration/python/guide" />
  <Card title="TypeScript Integration Guide" icon="link" href="./integration/typescript/guide" />
  <Card title="REST API" icon="link" href="./integration/rest-api" />
</CardGroup>

You can also [open the demo project](https://app.langwatch.ai/demo) check out a [video](https://www.loom.com/share/17f827b1f5a648298779b36e2dc959e6) on our platform.

## Get in touch

Feel free to reach out to us directly at [support@langwatch.ai](mailto:support@langwatch.ai). You can also open a [GitHub issue](https://github.com/langwatch/langwatch/issues)
to report bugs and request features, or join our [Discord](https://discord.gg/kT4PhDS2gH) channel and ask questions directly for the community and the core team.
---

# FILE: ./concepts.mdx

---
title: Concepts
description: LLM tracing and observability conceptual guide
keywords: LangWatch, concepts, tracing, observability, LLM, AI, travel, blog, user, customer, labels, threads, traces, spans
---

Understanding the core concepts of LangWatch is essential for effective observability in your LLM applications. This guide explains key terms and their relationships using practical examples, like building an AI travel assistant or a text generation service.

### Threads: The Whole Conversation

Field: `thread_id`

Think of a **Thread** as the entire journey a user takes in a single session. It's the complete chat with your AI travel buddy, from "Where should I go?" to booking the flight. For the blog post generator, a `thread_id` bundles up the whole session – from brainstorming headlines to polishing the final SEO-optimized draft. It groups *all* the back-and-forth interactions (Traces) for a specific task or conversation.

### Traces: One Task, End-to-End

Field: `trace_id`

<Note>While previously LangWatch allowed you to pass in a custom `trace_id`, we now generate it for you automatically, and provide no way to pass in your own.</Note>

Zooming in from Threads, a **Trace** represents a single, complete task performed by your AI. It's one round trip.

* **Travel Bot:** A user asking, "What are the cheapest flights to Bali in July?" is one Trace. Asking, "Does the hotel allow llamas?" is another Trace.
* **Blog Tool:** Generating headline options? That's a Trace. Drafting the intro paragraph? Another Trace. Optimizing for keywords? You guessed it – a Trace.

Each `trace_id` captures an entire end-to-end generation, no matter how many internal steps (Spans) it takes.

### Spans: The Building Blocks

Field: `span_id`

<Note>While previously LangWatch allowed you to pass in a custom `span_id`, we now generate it for you automatically, and provide no way to pass in your own.</Note>

Now, let's get granular! **Spans** are the individual steps or operations *within* a single Trace. Think of them as the building blocks of getting the job done.

* **Travel Bot Trace:** Might have a Span for the LLM call figuring out destinations, another Span querying an airline API for prices, and a final Span formatting the response.
* **Blog Tool Trace:** Could involve a Span for the initial text generation, a second Span where the LLM critiques its own work (clever!), and a third Span refining the text based on that critique.

Each `span_id` pinpoints a specific action taken by your system or an LLM call.

### User ID: Who's Using the App?

Field: `user_id`

Simple but crucial: The **User ID** identifies the actual end-user interacting with your product. Whether they're planning trips or writing posts, this `user_id` (usually their account ID in your system) links the activity back to a real person, helping you see how different users experience your AI features.

### Customer ID: For Platform Builders

Field: `customer_id`

Are you building a platform *for other companies* to create *their own* LLM apps? That's where the **Customer ID** shines. If you're providing the tools for others (your customers) to build AI assistants for *their* users, the `customer_id` lets you (and them!) track usage and performance per customer account. It's perfect for offering custom analytics dashboards, showing your customers how *their* AI implementations are doing.

### Labels: Your Organizational Superpowers

Field: `labels`

Think of **Labels** as flexible tags you can slap onto Traces to organize, filter, and compare anything you want! They're your secret weapon for slicing and dicing your data.

* **Categorize Actions:** Use labels like `blogpost_title` or `blogpost_keywords`.
* **Track Versions:** Label traces with `version:v1.0.0`, then deploy an improved prompt and label new traces `version:v1.0.1`.
* **Run Experiments:** Tag traces with `experiment:prompt_a` vs. `experiment:prompt_b`.

Labels make it easy to zoom in on specific features or A/B test different approaches right within the LangWatch dashboard.

---

# FILE: ./integration/overview.mdx

---
title: Overview
description: Easily integrate LangWatch with your Python, TypeScript, or REST API projects.
---

Integrating LangWatch into your projects is designed to be a straightforward process. Regardless of the language or LLM model you are using, you can set up LangWatch with minimal configuration and start gathering valuable insights into your LLM's performance and user interactions.

<CardGroup cols={2}>
  <Card title="Python Integration Guide" icon="link" href="./python/guide" />
  <Card title="TypeScript Integration Guide" icon="link" href="./typescript/guide" />
  <Card title="REST API" icon="link" href="./rest-api" />
</CardGroup>

---

# FILE: ./integration/python/reference.mdx

---
title: Python SDK API Reference
sidebarTitle: Python
description: LangWatch Python SDK API reference
---

## Setup

### `langwatch.setup()`

Initializes the LangWatch client, enabling data collection and tracing for your LLM application. This is typically the first function you'll call when integrating LangWatch.

<CodeGroup>
```python Basic Setup
import langwatch
import os

client = langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    endpoint_url=os.getenv("LANGWATCH_ENDPOINT_URL") # Optional, defaults to LangWatch Cloud
)
```

```python Advanced Setup
import langwatch
import os
from opentelemetry.sdk.trace import TracerProvider
from langwatch.domain import SpanProcessingExcludeRule
from langwatch.types import BaseAttributes

# Example: Configure with a custom TracerProvider and exclude rules
my_provider = TracerProvider()
exclude_rules = [
    SpanProcessingExcludeRule(
        field_name="span_name",
        match_value="InternalHealthCheck",
        match_operation="exact_match"
    )
]
custom_attributes = BaseAttributes(
    tags=["my-app", "production"],
    props={"service.version": "1.2.3"}
)

client = langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    tracer_provider=my_provider,
    span_exclude_rules=exclude_rules,
    base_attributes=custom_attributes,
    debug=True
)
```
</CodeGroup>

<ParamField path="api_key" type="Optional[str]" default="None">
  Your LangWatch API key. It's recommended to set this via an environment variable (e.g., `LANGWATCH_API_KEY`) and retrieve it using `os.getenv("LANGWATCH_API_KEY")`.
</ParamField>

<ParamField path="endpoint_url" type="Optional[str]" default="LangWatch Cloud URL">
  The URL of the LangWatch backend where traces will be sent. Defaults to the LangWatch Cloud service. For self-hosted instances, you'll need to provide this.
</ParamField>

<ParamField path="base_attributes" type="Optional[BaseAttributes]" default="None">
  A `BaseAttributes` object allowing you to set default tags and properties that will be attached to all traces and spans. See `BaseAttributes` type for more details.
</ParamField>

<ParamField path="tracer_provider" type="Optional[TracerProvider]" default="None">
  An OpenTelemetry `TracerProvider` instance. If you have an existing OpenTelemetry setup, you can pass your `TracerProvider` here. LangWatch will add its exporter to this provider. If not provided, LangWatch will configure its own.
</ParamField>

<ParamField path="instrumentors" type="Optional[Sequence[Instrumentor]]" default="None">
  A sequence of OpenTelemetry instrumentor instances. LangWatch can automatically apply these instrumentors. (Note: Specific instrumentor types might need to be defined or linked here).
</ParamField>

<ParamField path="span_exclude_rules" type="Optional[List[SpanProcessingExcludeRule]]" default="[]">
  A list of `SpanProcessingExcludeRule` objects. These rules allow you to filter out specific spans from being exported to LangWatch, based on span name or attributes. See `SpanProcessingExcludeRule` for details.
</ParamField>

<ParamField path="debug" type="bool" default="False">
  If `True`, enables debug logging for the LangWatch client, providing more verbose output about its operations.
</ParamField>

**Returns**

<ResponseField name="client" type="Client">
  An instance of the LangWatch `Client`.
</ResponseField>

## Tracing

### `@langwatch.trace()` / `langwatch.trace()`

This is the primary way to define the boundaries of a request or a significant operation in your application. It can be used as a decorator around a function or as a context manager.

When used, it creates a new trace and a root span for that trace. Any `@langwatch.span()` or other instrumented calls made within the decorated function or context manager will be nested under this root span.

<CodeGroup>
```python Decorator Usage
import langwatch

langwatch.setup()

@langwatch.trace(name="MyRequestHandler", metadata={"user_id": "123"})
def handle_request(query: str):
    # ... your logic ...
    # langwatch.get_current_span().update(output=response)
    return f"Processed: {query}"

handle_request("Hello LangWatch!")
```

```python Context Manager Usage
import langwatch

langwatch.setup()

def process_data(data: str):
    with langwatch.trace(name="DataProcessingFlow", input={"data_input": data}) as current_trace:
        # ... your logic ...
        # langwatch.get_current_span().update(output={"status": "success"})
        result = f"Processed data: {data}"
        if langwatch.get_current_span(): # Check if span exists
            langwatch.get_current_span().update(output=result)
        return result

process_data("Sample Data")
```
</CodeGroup>


<ParamField path="name" type="Optional[str]" default="Name of the decorated function or a default name">
  The name for the root span of this trace. If used as a decorator and not provided, it defaults to the name of the decorated function. For context manager usage, a name like "LangWatch Trace" might be used if not specified.
</ParamField>

<ParamField path="trace_id" type="Optional[Union[str, UUID]]" default="Auto-generated">
  (Deprecated) A specific ID to assign to this trace. If not provided, a new UUID will be generated. It's generally recommended to let LangWatch auto-generate trace IDs. This will be mapped to `deprecated.trace_id` in metadata.
</ParamField>

<ParamField path="metadata" type="Optional[TraceMetadata]" default="None">
  A dictionary of metadata to attach to the entire trace. This can include things like user IDs, session IDs, or any other contextual information relevant to the whole operation. `TraceMetadata` is typically `Dict[str, Any]`.
</ParamField>

<ParamField path="expected_output" type="Optional[str]" default="None">
  If you have a known expected output for this trace (e.g., for testing or evaluation), you can provide it here.
</ParamField>

<ParamField path="api_key" type="Optional[str]" default="Global or None">
  Overrides the global API key for this specific trace. Useful if you need to direct traces to different projects or accounts dynamically.
</ParamField>

<ParamField path="disable_sending" type="bool" default="False">
  If `True`, this trace (and its spans) will be processed but not sent to the LangWatch backend. This can be useful for local debugging or conditional tracing.
</ParamField>

<ParamField path="max_string_length" type="Optional[int]" default="5000">
  The maximum length for string values captured in inputs, outputs, and metadata. Longer strings will be truncated.
</ParamField>

<ParamField path="skip_root_span" type="bool" default="False">
  If `True`, a root span will not be automatically created for this trace. This is an advanced option, typically used if you intend to manage the root span's lifecycle manually or if the trace is purely a logical grouping without its own primary operation.
</ParamField>

<Expandable title="Root Span Parameters">
The following parameters are used to configure the root span that is automatically created when the trace starts. Refer to `@langwatch.span()` documentation for more details on these, as they behave similarly.

<ParamField path="span_id" type="Optional[str]">
  A specific ID for the root span.
</ParamField>

<ParamField path="capture_input" type="bool" default="True">
  Whether to capture the input of the decorated function/context block for the root span.
</ParamField>

<ParamField path="capture_output" type="bool" default="True">
  Whether to capture the output/result for the root span.
</ParamField>

<ParamField path="type" type="SpanTypes" default="'span'">
  The type of the root span (e.g., 'llm', 'rag', 'agent', 'tool', 'span').
</ParamField>

<ParamField path="input" type="Optional[Union[SpanInputOutput, str, List[ChatMessage]]]">
  Explicitly sets the input for the root span. If not provided and `capture_input` is `True`, it may be inferred from function arguments.
</ParamField>

<ParamField path="output" type="Optional[Union[SpanInputOutput, str, List[ChatMessage]]]">
  Explicitly sets the output for the root span. If not provided and `capture_output` is `True`, it may be inferred from the function's return value.
</ParamField>

<ParamField path="error" type="Optional[Exception]">
  Records an error for the root span.
</ParamField>

<ParamField path="timestamps" type="Optional[SpanTimestamps]">
  Custom start and end timestamps for the root span.
</ParamField>

<ParamField path="contexts" type="Optional[Union[List[RAGChunk], List[str]]]">
  RAG chunks or string contexts relevant to the root span.
</ParamField>

<ParamField path="model" type="Optional[str]">
  The model used in the operation represented by the root span (e.g., an LLM model name).
</ParamField>

<ParamField path="params" type="Optional[SpanParams]">
  Parameters associated with the operation of the root span (e.g., LLM call parameters).
</ParamField>

<ParamField path="metrics" type="Optional[SpanMetrics]">
  Metrics for the root span (e.g., token counts, cost).
</ParamField>

<ParamField path="evaluations" type="Optional[List[Evaluation]]">
  A list of `Evaluation` objects to associate directly with the root span.
</ParamField>
</Expandable>


**Context Manager Return**

When used as a context manager, `langwatch.trace()` returns a `LangWatchTrace` object.

<ResponseField name="current_trace" type="LangWatchTrace">
  The `LangWatchTrace` instance. You can use this object to call methods like `current_trace.add_evaluation()`.
</ResponseField>

#### `LangWatchTrace` Object Methods

When `langwatch.trace()` is used as a context manager, it yields a `LangWatchTrace` object. This object has several methods to interact with the current trace:

<ParamField path="update" type="Callable">
  Updates attributes of the trace or its root span.
  This method can take many of the same parameters as the `langwatch.trace()` decorator/context manager itself, such as `metadata`, `expected_output`, or any of the root span parameters like `name`, `input`, `output`, `metrics`, etc.

  ```python
  with langwatch.trace(name="Initial Trace") as current_trace:
      # ... some operations ...
      current_trace.update(metadata={"step": "one_complete"})
      # ... more operations ...
      root_span_output = "Final output of root operation"
      current_trace.update(output=root_span_output, metrics={"total_items": 10})
  ```
</ParamField>

<ParamField path="add_evaluation" type="Callable">
  Adds an `Evaluation` object directly to the trace (or a specified span within it).

  ```python
  from langwatch.domain import Evaluation, EvaluationTimestamps

  with langwatch.trace(name="MyEvaluatedProcess") as current_trace:
      # ... operation ...
      eval_result = Evaluation(
          name="Accuracy Check",
          status="processed",
          passed=True,
          score=0.95,
          details="All checks passed.",
          timestamps=EvaluationTimestamps(started_at=..., finished_at=...)
      )
      current_trace.add_evaluation(**eval_result) # Pass fields as keyword arguments
  ```
  (Refer to `Evaluation` type in Core Data Types and `langwatch.evaluations` module for more details on parameters.)
</ParamField>

<ParamField path="evaluate" type="Callable">
  Triggers a remote evaluation for this trace using a pre-configured evaluator slug on the LangWatch platform.

  ```python
  with langwatch.trace(name="ProcessWithRemoteEval") as current_trace:
      output_to_evaluate = "Some generated text."
      current_trace.evaluate(
          slug="sentiment-analyzer",
          output=output_to_evaluate,
          as_guardrail=False
      )
  ```
  Parameters include `slug`, `name`, `input`, `output`, `expected_output`, `contexts`, `conversation`, `settings`, `as_guardrail`, `data`.
  Returns: Result of the evaluation call.
</ParamField>

<ParamField path="async_evaluate" type="Callable">
  An asynchronous version of `evaluate`.
</ParamField>

<ParamField path="autotrack_openai_calls" type="Callable">
  Instruments an OpenAI client instance (e.g., `openai.OpenAI()`) to automatically create spans for any OpenAI API calls made using that client within the current trace.

  ```python
  import openai
  # client = openai.OpenAI() # or AzureOpenAI, etc.

  with langwatch.trace(name="OpenAICallsDemo") as current_trace:
      # current_trace.autotrack_openai_calls(client)
      # response = client.chat.completions.create(...)
      # The above call will now be automatically traced as a span.
      pass
  ```
  Takes the OpenAI client instance as an argument.
</ParamField>

<ParamField path="autotrack_dspy" type="Callable">
  Enables automatic tracing for DSPy operations within the current trace. Requires DSPy to be installed and properly configured.

  ```python
  with langwatch.trace(name="DSPyPipeline") as current_trace:
      # current_trace.autotrack_dspy()
      # ... your DSPy calls ...
      pass
  ```
</ParamField>

<ParamField path="get_langchain_callback" type="Callable">
  Returns a LangChain callback handler (`LangChainTracer`) associated with the current trace. This handler can be passed to LangChain runs to automatically trace LangChain operations.

  ```python
  # from langchain_core.llms import FakeLLM (or any LangChain LLM)
  # from langchain.chains import LLMChain

  with langwatch.trace(name="LangChainFlow") as current_trace:
      # handler = current_trace.get_langchain_callback()
      # llm = FakeLLM()
      # chain = LLMChain(llm=llm, prompt=...)
      # chain.run("some input", callbacks=[handler])
      pass
  ```
  Returns: `LangChainTracer` instance.
</ParamField>

<ParamField path="share" type="Callable">
  (Potentially) Generates a shareable link or identifier for this trace. The exact behavior might depend on backend support and configuration.
  Returns: A string, possibly a URL or an ID.
</ParamField>

<ParamField path="unshare" type="Callable">
  (Potentially) Revokes sharing for this trace if it was previously shared.
</ParamField>

### `@langwatch.span()` / `langwatch.span()`

Use this to instrument specific operations or blocks of code within a trace. Spans can be nested to create a hierarchical view of your application's execution flow.

It can be used as a decorator around a function or as a context manager.

<CodeGroup>
```python Decorator Usage
import langwatch

langwatch.setup()

@langwatch.trace(name="MainProcess")
def main_process():
    do_first_step("data for step 1")
    do_second_step("data for step 2")

@langwatch.span(name="StepOne", type="tool")
def do_first_step(data: str):
    # langwatch.get_current_span().set_attributes({"custom_info": "info1"})
    return f"Step 1 processed: {data}"

@langwatch.span() # Name will be 'do_second_step', type will be 'span'
def do_second_step(data: str):
    # langwatch.get_current_span().update(metrics={"items_processed": 10})
    return f"Step 2 processed: {data}"

main_process()
```

```python Context Manager Usage
import langwatch

langwatch.setup()

@langwatch.trace(name="ComplexOperation")
def complex_op():
    # ... some initial work ...
    with langwatch.span(name="DataRetrieval", type="rag") as rag_span:
        # rag_span.update(contexts=[{"document_id": "doc1", "content": "..."}])
        retrieved_data = "some data from RAG"
        rag_span.update(output=retrieved_data)

    # ... more work ...
    with langwatch.span(name="LLMCall", type="llm", input={"prompt": "Summarize this"}, model="gpt-5") as llm_span:
        summary = "summary from LLM"
        llm_span.update(output=summary, metrics={"input_tokens": 50, "output_tokens": 15})
    return summary

complex_op()
```
</CodeGroup>


<ParamField path="name" type="Optional[str]" default="Name of the decorated function or a default name">
  The name for the span. If used as a decorator and not provided, it defaults to the name of the decorated function. For context manager usage, a default name like "LangWatch Span" might be used if not specified.
</ParamField>

<ParamField path="type" type="Optional[SpanType]" default="'span'">
  The semantic type of the span, which helps categorize the operation. Common types include `'llm'`, `'rag'`, `'agent'`, `'tool'`, `'embedding'`, or a generic `'span'`. `SpanType` is typically a string literal from `langwatch.domain.SpanTypes`.
</ParamField>

<ParamField path="span_id" type="Optional[Union[str, UUID]]" default="Auto-generated">
  (Deprecated) A specific ID to assign to this span. It's generally recommended to let LangWatch auto-generate span IDs. This will be mapped to `deprecated.span_id` in the span's attributes.
</ParamField>

<ParamField path="parent" type="Optional[Union[OtelSpan, LangWatchSpan]]" default="Current span in context">
  Explicitly sets the parent for this span. If not provided, the span will be nested under the currently active `LangWatchSpan` or OpenTelemetry span.
</ParamField>

<ParamField path="capture_input" type="bool" default="True">
  If `True` (and used as a decorator), automatically captures the arguments of the decorated function as the span's input.
</ParamField>

<ParamField path="capture_output" type="bool" default="True">
  If `True` (and used as a decorator), automatically captures the return value of the decorated function as the span's output.
</ParamField>

<ParamField path="input" type="SpanInputType" default="None">
  Explicitly sets the input for this span. `SpanInputType` can be a dictionary, a string, or a list of `ChatMessage` objects. This overrides automatic input capture.
</ParamField>

<ParamField path="output" type="SpanInputType" default="None">
  Explicitly sets the output for this span. `SpanInputType` has the same flexibility as for `input`. This overrides automatic output capture.
</ParamField>

<ParamField path="error" type="Optional[Exception]" default="None">
  Records an error for this span. If an exception occurs within a decorated function or context manager, it's often automatically recorded.
</ParamField>

<ParamField path="timestamps" type="Optional[SpanTimestamps]" default="None">
  A `SpanTimestamps` object to explicitly set the `start_time` and `end_time` for the span. Useful for instrumenting operations where the duration is known or externally managed.
</ParamField>

<ParamField path="contexts" type="ContextsType" default="None">
  Relevant contextual information for this span, especially for RAG operations. `ContextsType` can be a list of `RAGChunk` objects or a list of strings.
</ParamField>

<ParamField path="model" type="Optional[str]" default="None">
  The name or identifier of the model used in this operation (e.g., `'gpt-5'`, `'text-embedding-ada-002'`).
</ParamField>

<ParamField path="params" type="Optional[Union[SpanParams, Dict[str, Any]]]" default="None">
  A dictionary or `SpanParams` object containing parameters relevant to the operation (e.g., temperature for an LLM call, k for a vector search).
</ParamField>

<ParamField path="metrics" type="Optional[SpanMetrics]" default="None">
  A `SpanMetrics` object or dictionary to record quantitative measurements for this span, such as token counts (`input_tokens`, `output_tokens`), cost, or other custom metrics.
</ParamField>

<ParamField path="evaluations" type="Optional[List[Any]]" default="None">
  A list of `Evaluation` objects to directly associate with this span.
</ParamField>

<ParamField path="ignore_missing_trace_warning" type="bool" default="False">
  If `True`, suppresses the warning that is normally emitted if a span is created without an active parent trace.
</ParamField>

**OpenTelemetry Parameters:**
These parameters are passed directly to the underlying OpenTelemetry span creation. Refer to OpenTelemetry documentation for more details.

<ParamField path="kind" type="SpanKind" default="SpanKind.INTERNAL">
  The OpenTelemetry `SpanKind` (e.g., `INTERNAL`, `CLIENT`, `SERVER`, `PRODUCER`, `CONSUMER`).
</ParamField>

<ParamField path="span_context" type="Optional[Context]" default="None">
  An OpenTelemetry `Context` object to use for creating the span.
</ParamField>

<ParamField path="attributes" type="Optional[Dict[str, Any]]" default="None">
  Additional custom attributes (key-value pairs) to attach to the span.
</ParamField>

<ParamField path="links" type="Optional[List[Link]]" default="None">
  A list of OpenTelemetry `Link` objects to associate with this span.
</ParamField>

<ParamField path="start_time" type="Optional[int]" default="None">
  An explicit start time for the span (in nanoseconds since epoch).
</ParamField>

<ParamField path="record_exception" type="bool" default="True">
  Whether OpenTelemetry should automatically record exceptions for this span.
</ParamField>

<ParamField path="set_status_on_exception" type="bool" default="True">
  Whether OpenTelemetry should automatically set the span status to error when an exception is recorded.
</ParamField>


**Context Manager Return**

When used as a context manager, `langwatch.span()` returns a `LangWatchSpan` object.

<ResponseField name="current_span" type="LangWatchSpan">
  The `LangWatchSpan` instance. You can use this object to call methods like `current_span.update()` or `current_span.add_evaluation()`.
</ResponseField>

#### `LangWatchSpan` Object Methods

When `langwatch.span()` is used as a context manager, it yields a `LangWatchSpan` object. This object provides methods to interact with the current span:

<ParamField path="update" type="Callable">
  Updates attributes of the span. This is the primary method for adding or changing information on an active span.
  It accepts most of the same parameters as the `@langwatch.span()` decorator itself, such as `name`, `type`, `input`, `output`, `error`, `timestamps`, `contexts`, `model`, `params`, `metrics`, and arbitrary key-value pairs for custom attributes.

  ```python
  with langwatch.span(name="MySpan", type="tool") as current_span:
      # ... some operation ...
      current_span.update(output={"result": "success"}, metrics={"items_processed": 5})
      # Add a custom attribute
      current_span.update(custom_tool_version="1.2.3")
  ```
</ParamField>

<ParamField path="add_evaluation" type="Callable">
  Adds an `Evaluation` object directly to this span.

  ```python
  from langwatch.domain import Evaluation

  with langwatch.span(name="SubOperation") as current_span:
      # ... operation ...
      eval_data = {
          "name": "Correctness Check",
          "status": "processed",
          "passed": False,
          "score": 0.2,
          "label": "Incorrect"
      } # Example, more fields available
      current_span.add_evaluation(**eval_data)
  ```
  (Refer to `Evaluation` type in Core Data Types and `langwatch.evaluations` module for more details on parameters.)
</ParamField>

<ParamField path="evaluate" type="Callable">
  Triggers a remote evaluation for this span using a pre-configured evaluator slug on the LangWatch platform.

  ```python
  with langwatch.span(name="LLM Generation", type="llm") as llm_span:
      llm_output = "Some text generated by an LLM."
      llm_span.update(output=llm_output)
      llm_span.evaluate(slug="toxicity-check", output=llm_output, as_guardrail=True)
  ```
  Parameters include `slug`, `name`, `input`, `output`, `expected_output`, `contexts`, `conversation`, `settings`, `as_guardrail`, `data`.
  Returns: Result of the evaluation call.
</ParamField>

<ParamField path="async_evaluate" type="Callable">
  An asynchronous version of `evaluate` for spans.
</ParamField>

<ParamField path="end" type="Callable">
  Explicitly ends the span. If you provide arguments (like `output`, `metrics`, etc.), it will call `update()` with those arguments before ending.
  Usually not needed when using the span as a context manager, as `__exit__` handles this.

  ```python
  # Manual span management example
  # current_span = langwatch.span(name="ManualSpan").__enter__() # Start span manually
  # try:
  #     # ... operations ...
  #     result = "some result"
  #     current_span.end(output=result) # Update and end
  # except Exception as e:
  #     current_span.end(error=e) # Record error and end
  #     raise
  ```
</ParamField>

**OpenTelemetry Span Methods:**

The `LangWatchSpan` object also directly exposes standard OpenTelemetry `trace.Span` API methods for more advanced use cases or direct OTel interop. These include:
- `record_error(exception)`: Records an exception against the span.
- `add_event(name, attributes)`: Adds a timed event to the span.
- `set_status(status_code, description)`: Sets the OTel status of the span (e.g., `StatusCode.ERROR`).
- `set_attributes(attributes_dict)`: Sets multiple OTel attributes at once.
- `update_name(new_name)`: Changes the name of the span.
- `is_recording()`: Returns `True` if the span is currently recording information.
- `get_span_context()`: Returns the `SpanContext` of the underlying OTel span.

Refer to the OpenTelemetry Python documentation for details on these methods.

```python
from opentelemetry.trace import Status, StatusCode

with langwatch.span(name="MyDetailedSpan") as current_span:
    current_span.add_event("Checkpoint 1", {"detail": "Reached stage A"})
    # ... some work ...
    try:
        # risky_operation()
        pass
    except Exception as e:
        current_span.record_error(e)
        current_span.set_status(Status(StatusCode.ERROR, description="Risky op failed"))
    current_span.set_attributes({"otel_attribute": "value"})
```

## Context Accessors

These functions allow you to access the currently active LangWatch trace or span from anywhere in your code, provided that a trace/span has been started (e.g., via `@langwatch.trace` or `@langwatch.span`).

### `langwatch.get_current_trace()`

Retrieves the currently active `LangWatchTrace` object.

This is useful if you need to interact with the trace object directly, for example, to add trace-level metadata or evaluations from a helper function called within an active trace.

```python
import langwatch

langwatch.setup()

def some_utility_function(detail: str):
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(metadata={"utility_info": detail})

@langwatch.trace(name="MainOperation")
def main_operation():
    # ... some work ...
    some_utility_function("Processed step A")
    # ... more work ...

main_operation()
```

<ParamField path="suppress_warning" type="bool" default="False">
  If `True`, suppresses the warning that is normally emitted if this function is called when no LangWatch trace is currently in context.
</ParamField>

<ResponseField name="trace" type="LangWatchTrace">
  The current `LangWatchTrace` object. If no trace is active and `suppress_warning` is `False`, a warning is issued and a new (detached) `LangWatchTrace` instance might be returned.
</ResponseField>

### `langwatch.get_current_span()`

Retrieves the currently active `LangWatchSpan` object.

This allows you to get a reference to the current span to update its attributes, add events, or record information specific to that span from nested function calls.
If no LangWatch-specific span is in context, it will attempt to wrap the current OpenTelemetry span.

```python
import langwatch

langwatch.setup()

def enrich_span_data(key: str, value: any):
    current_span = langwatch.get_current_span()
    if current_span:
        current_span.update(**{key: value})
        # Or using the older set_attributes for OpenTelemetry compatible attributes
        # current_span.set_attributes({key: value})

@langwatch.trace(name="UserFlow")
def user_flow():
    with langwatch.span(name="Step1") as span1:
        # ... step 1 logic ...
        enrich_span_data("step1_result", "success")

    with langwatch.span(name="Step2") as span2:
        # ... step 2 logic ...
        enrich_span_data("step2_details", {"info": "more data"})

user_flow()
```

<ResponseField name="span" type="LangWatchSpan">
  The current `LangWatchSpan` object. This could be a span initiated by `@langwatch.span`, the root span of a `@langwatch.trace`, or a `LangWatchSpan` wrapping an existing OpenTelemetry span if no LangWatch span is directly in context.
</ResponseField>

## Core Data Types

This section describes common data structures used throughout the LangWatch SDK, particularly as parameters to functions like `langwatch.setup()`, `@langwatch.trace()`, and `@langwatch.span()`, or as part of the data captured.

### `SpanProcessingExcludeRule`

Defines a rule to filter out spans before they are exported to LangWatch. Used in the `span_exclude_rules` parameter of `langwatch.setup()`.

<ResponseField name="field_name" type="Literal['span_name']" required>
  The field of the span to match against. Currently, only `"span_name"` is supported.
</ResponseField>
<ResponseField name="match_value" type="str" required>
  The value to match for the specified `field_name`.
</ResponseField>
<ResponseField name="match_operation" type="Literal['includes', 'exact_match', 'starts_with', 'ends_with']" required>
  The operation to use for matching (e.g., `"exact_match"`, `"starts_with"`).
</ResponseField>

```python Example
from langwatch.domain import SpanProcessingExcludeRule

exclude_health_checks = SpanProcessingExcludeRule(
    field_name="span_name",
    match_value="/health_check",
    match_operation="exact_match"
)

exclude_internal_utils = SpanProcessingExcludeRule(
    field_name="span_name",
    match_value="utils.",
    match_operation="starts_with"
)
```

### `ChatMessage`

Represents a single message in a chat conversation, typically used for `input` or `output` of LLM spans.

<ResponseField name="role" type="ChatRole" required>
  The role of the message sender. `ChatRole` is a Literal: `"system"`, `"user"`, `"assistant"`, `"function"`, `"tool"`, `"guardrail"`, `"evaluation"`, `"unknown"`.
</ResponseField>
<ResponseField name="content" type="Optional[str]">
  The textual content of the message.
</ResponseField>
<ResponseField name="function_call" type="Optional[FunctionCall]">
  For assistant messages that involve a function call (legacy OpenAI format).
  <Expandable title="FunctionCall Properties">
    <ResponseField name="name" type="str">Name of the function to call.</ResponseField>
    <ResponseField name="arguments" type="str">JSON string of arguments for the function.</ResponseField>
  </Expandable>
</ResponseField>
<ResponseField name="tool_calls" type="Optional[List[ToolCall]]">
  For assistant messages that involve tool calls (current OpenAI format).
  <Expandable title="ToolCall Properties">
    <ResponseField name="id" type="str">ID of the tool call.</ResponseField>
    <ResponseField name="type" type="str">Type of the tool (usually `"function"`).</ResponseField>
    <ResponseField name="function" type="FunctionCall">Details of the function to be called (see `FunctionCall` above).</ResponseField>
  </Expandable>
</ResponseField>
<ResponseField name="tool_call_id" type="Optional[str]">
  For messages that are responses from a tool, this is the ID of the tool call that this message is a response to.
</ResponseField>
<ResponseField name="name" type="Optional[str]">
  The name of the function whose result is in the `content` (if role is `"function"`), or the name of the tool/participant (if role is `"tool"`).
</ResponseField>

```python Example
from langwatch.domain import ChatMessage

user_message = ChatMessage(role="user", content="Hello, world!")
assistant_response = ChatMessage(role="assistant", content="Hi there!")
```

### `RAGChunk`

Represents a chunk of retrieved context, typically used with RAG (Retrieval Augmented Generation) operations in the `contexts` field of a span.

<ResponseField name="document_id" type="Optional[str]">
  An identifier for the source document of this chunk.
</ResponseField>
<ResponseField name="chunk_id" type="Optional[str]">
  An identifier for this specific chunk within the document.
</ResponseField>
<ResponseField name="content" type="Union[str, Dict[str, Any], List[Any]]" required>
  The actual content of the RAG chunk. Can be a simple string or a more complex structured object.
</ResponseField>

```python Example
from langwatch.domain import RAGChunk

retrieved_chunk = RAGChunk(
    document_id="doc_123",
    chunk_id="chunk_005",
    content="LangWatch helps monitor LLM applications."
)
```

### `SpanInputOutput`

This is a flexible type used for the `input` and `output` fields of spans. It's a Union that can take several forms to represent different kinds of data. LangWatch will store it as a typed value.
Common forms include:
- `TypedValueText`: For simple string inputs/outputs. `{"type": "text", "value": "your string"}`
- `TypedValueChatMessages`: For conversational inputs/outputs. `{"type": "chat_messages", "value": [ChatMessage, ...]}`
- `TypedValueJson`: For arbitrary JSON-serializable data. `{"type": "json", "value": {"key": "value"}}`
- `TypedValueRaw`: For data that should be stored as a raw string, escaping any special interpretation. `{"type": "raw", "value": "<xml>data</xml>"}`
- `TypedValueList`: For a list of `SpanInputOutput` objects. `{"type": "list", "value": [SpanInputOutput, ...]}`

When providing `input` or `output` to `@langwatch.span()` or `span.update()`, you can often provide the raw Python object (e.g., a string, a list of `ChatMessage` dicts, a dictionary for JSON), and the SDK will attempt to serialize it correctly. For more control, you can construct the `TypedValue` dictionaries yourself.

### `SpanTimestamps`

A dictionary defining custom start and end times for a span, in nanoseconds since epoch.

<ResponseField name="started_at" type="int" required>
  Timestamp when the span started (nanoseconds since epoch).
</ResponseField>
<ResponseField name="first_token_at" type="Optional[int]">
  For LLM operations, timestamp when the first token was received (nanoseconds since epoch).
</ResponseField>
<ResponseField name="finished_at" type="int" required>
  Timestamp when the span finished (nanoseconds since epoch).
</ResponseField>

### `SpanTypes` (Semantic Span Types)

A string literal defining the semantic type of a span. This helps categorize spans in the LangWatch UI and for analytics. Possible values include:
- `"span"` (generic span)
- `"llm"` (Language Model operation)
- `"chain"` (a sequence of operations, e.g., LangChain chain)
- `"tool"` (execution of a tool or function call)
- `"agent"` (an autonomous agent's operation)
- `"guardrail"` (a guardrail or safety check)
- `"evaluation"` (an evaluation step)
- `"rag"` (Retrieval Augmented Generation operation)
- `"workflow"` (a broader workflow or business process)
- `"component"` (a sub-component within a larger system)
- `"module"` (a logical module of code)
- `"server"` (server-side operation)
- `"client"` (client-side operation)
- `"producer"` (message producer)
- `"consumer"` (message consumer)
- `"task"` (a background task or job)
- `"unknown"`

### `SpanMetrics`

A dictionary for quantitative measurements associated with a span.

<ResponseField name="prompt_tokens" type="Optional[int]">
  Number of tokens in the input/prompt to an LLM.
</ResponseField>
<ResponseField name="completion_tokens" type="Optional[int]">
  Number of tokens in the output/completion from an LLM.
</ResponseField>
<ResponseField name="cost" type="Optional[float]">
  Estimated or actual monetary cost of the operation (e.g., LLM API call cost).
</ResponseField>

### `SpanParams`

A dictionary for parameters related to an operation, especially LLM calls.
Examples include:
<ResponseField name="frequency_penalty" type="Optional[float]">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</ResponseField>
<ResponseField name="logit_bias" type="Optional[Dict[str, float]]">
  Modify the likelihood of specified tokens appearing in the completion. Accepts a dictionary mapping token IDs (or tokens, depending on the model) to a bias value from -100 to 100.
</ResponseField>
<ResponseField name="logprobs" type="Optional[bool]">
  Whether to return log probabilities of the output tokens.
</ResponseField>
<ResponseField name="top_logprobs" type="Optional[int]">
  An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with log probability. `logprobs` must be `True` if this is used.
</ResponseField>
<ResponseField name="max_tokens" type="Optional[int]">
  The maximum number of tokens to generate in the completion.
</ResponseField>
<ResponseField name="n" type="Optional[int]">
  How many completions to generate for each prompt.
</ResponseField>
<ResponseField name="presence_penalty" type="Optional[float]">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
</ResponseField>
<ResponseField name="seed" type="Optional[int]">
  If specified, the system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
</ResponseField>
<ResponseField name="stop" type="Optional[Union[str, List[str]]]">
  Up to 4 sequences where the API will stop generating further tokens.
</ResponseField>
<ResponseField name="stream" type="Optional[bool]">
  If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available.
</ResponseField>
<ResponseField name="temperature" type="Optional[float]">
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
</ResponseField>
<ResponseField name="top_p" type="Optional[float]">
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass.
</ResponseField>
<ResponseField name="tools" type="Optional[List[Dict[str, Any]]]">
  A list of tools the model may call. Currently, only functions are supported as a tool.
</ResponseField>
<ResponseField name="tool_choice" type="Optional[str]">
  Controls which (if any) tool is called by the model. `none` means the model will not call any tool. `auto` means the model can pick between generating a message or calling a tool.
</ResponseField>
<ResponseField name="user" type="Optional[str]">
  A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
</ResponseField>

### `TraceMetadata`

A dictionary (`

---

# FILE: ./integration/python/guide.mdx

---
title: Python Integration Guide
sidebarTitle: Python
description: LangWatch Python SDK integration guide
keywords: LangWatch, Python, SDK, integration, guide, setup, tracing, spans, traces, OpenTelemetry, OpenAI, Celery, HTTP clients, databases, ORMs
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
    <a href="https://github.com/langwatch/langwatch/tree/main/python-sdk" target="_blank">
      <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch Python Repo" />
    </a>
  </div>

  <div>
    <a href="https://pypi.org/project/langwatch/" target="_blank">
      <img src="https://img.shields.io/pypi/v/langwatch?color=007EC6" noZoom alt="LangWatch Python SDK version" />
    </a>
  </div>
</div>

Integrate LangWatch into your Python application to start observing your LLM interactions. This guide covers the setup and basic usage of the LangWatch Python SDK.

## Get your LangWatch API Key

First, you need a LangWatch API key. Sign up at [app.langwatch.ai](https://app.langwatch.ai) and find your API key in your project settings. The SDK will automatically use the `LANGWATCH_API_KEY` environment variable if it is set.

## Start Instrumenting

First, ensure you have the SDK installed:

```bash
pip install langwatch
```

Initialize LangWatch early in your application, typically where you configure services:

```python
import langwatch
import os

langwatch.setup()

# Your application code...
```

<Note>
  If you have an existing OpenTelemetry setup in your application, please see
  the [Already using OpenTelemetry?](#already-using-opentelemetry) section
  below.
</Note>

## Capturing Messages

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces-one-task-end-to-end).
- A [Trace](/concepts#traces-one-task-end-to-end) contains multiple [Spans](/concepts#spans-the-building-blocks), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans-the-building-blocks) capture different parameters.
  - [Spans](/concepts#spans-the-building-blocks) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces-one-task-end-to-end) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads-the-whole-conversation) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.

### Creating a Trace

To capture an end-to-end operation, like processing a user message, you can wrap the main function or entry point with the `@langwatch.trace()` decorator. This automatically creates a root span for the entire operation.

```python
import langwatch
from openai import OpenAI

client = OpenAI()

@langwatch.trace()
async def handle_message():
    # This whole function execution is now a single trace
    langwatch.get_current_trace().autotrack_openai_calls(client) # Automatically capture OpenAI calls

    # ... rest of your message handling logic ...
    pass

```

You can customize the trace name and add initial metadata if needed:

```python
@langwatch.trace(name="My Custom Trace Name", metadata={"foo": "bar"})
async def handle_message():
    # ...
    pass
```

Within a traced function, you can access the current trace context using `langwatch.get_current_trace()`.

### Capturing a Span

To instrument specific parts of your pipeline within a trace (like an llm operation, rag retrieval, or external api call), use the `@langwatch.span()` decorator.

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.span(type="rag", name="RAG Document Retrieval") # Add type and custom name
def rag_retrieval(query: str):
    # ... logic to retrieve documents ...
    search_results = [
        {"id": "doc-1", "content": "..." },
        {"id": "doc-2", "content": "..." }
    ]

    # Add specific context data to the span
    langwatch.get_current_span().update(
        contexts=[
            RAGChunk(document_id=doc["id"], content=doc["content"])
            for doc in search_results
        ],
        retrieval_strategy="vector_search",
    )

    return search_results

@langwatch.trace()
async def handle_message(message: cl.Message):
    # ...
    retrieved_docs = rag_retrieval(message.content) # This call creates a nested span
    # ...
```

<Note>
  The `@langwatch.span()` decorator automatically captures the decorated
  function's arguments as the span's `input` and its return value as the
  `output`. This behavior can be controlled via the `capture_input` and
  `capture_output` arguments (both default to `True`).
</Note>

Spans created within a function **decorated** with `@langwatch.trace()` will automatically be nested under the main trace span. You can add additional `type`, `name`, `metadata`, and `events`, or override the automatic input/output using decorator arguments or the `update()` method on the span object obtained via `langwatch.get_current_span()`.

For detailed guidance on manually creating traces and spans using context managers or direct start/end calls, see the [Manual Instrumentation Tutorial](/integration/python/tutorials/manual-instrumentation).

## Full Setup

```python
import os

import langwatch
from langwatch.attributes import AttributeKey
from langwatch.domain import SpanProcessingExcludeRule

from community.instrumentors import OpenAIInstrumentor # Example instrumentor

from opentelemetry.sdk.trace import TracerProvider

# Example: Providing an existing TracerProvider
# existing_provider = TracerProvider()

# Example: Defining exclude rules
exclude_rules = [
    SpanProcessingExcludeRule(
      field_name=["span_name"],
      match_value="GET /health_check",
      match_operation="exact_match"
    ),
]

langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    endpoint_url="https://your-langwatch-instance.com", # Optional: Defaults to env var or cloud
    base_attributes={
      AttributeKey.ServiceName: "my-awesome-service",
      AttributeKey.ServiceVersion: "1.2.3",
      # Add other custom attributes here
    },
    instrumentors=[OpenAIInstrumentor()], # Optional: List of instrumentors that conform to the `Instrumentor` protocol
    # tracer_provider=existing_provider, # Optional: Provide your own TracerProvider
    debug=True, # Optional: Enable debug logging
    disable_sending=False, # Optional: Disable sending traces
    flush_on_exit=True, # Optional: Flush traces on exit (default: True)
    span_exclude_rules=exclude_rules, # Optional: Rules to exclude spans
    ignore_global_tracer_provider_override_warning=False # Optional: Silence warning if global provider exists
)

# Your application code...
```

### Options

<ParamField path="api_key" type="str | None">
  Your LangWatch API key. If not provided, it uses the `LANGWATCH_API_KEY`
  environment variable.
</ParamField>

<ParamField path="endpoint_url" type="str | None">
  The LangWatch endpoint URL. Defaults to the `LANGWATCH_ENDPOINT` environment
  variable or `https://app.langwatch.ai`.
</ParamField>

<ParamField path="base_attributes" type="dict[str, Any] | None">
  A dictionary of attributes to add to all spans (e.g., service name, version).
  Automatically includes SDK name, version, and language.
</ParamField>

<ParamField path="instrumentors" type="Sequence[Instrumentor] | None">
  A list of automatic instrumentors (e.g., `OpenAIInstrumentor`,
  `LangChainInstrumentor`) to capture data from supported libraries.
</ParamField>

<ParamField path="tracer_provider" type="TracerProvider | None">
  An existing OpenTelemetry `TracerProvider`. If provided, LangWatch will use it
  (adding its exporter) instead of creating a new one. If not provided,
  LangWatch checks the global provider or creates a new one.
</ParamField>

<ParamField path="debug" type="bool" default="False">
  Enable debug logging for LangWatch. Defaults to `False` or checks if the
  `LANGWATCH_DEBUG` environment variable is set to `"true"`.
</ParamField>

<ParamField path="disable_sending" type="bool" default="False">
  If `True`, disables sending traces to the LangWatch server. Useful for testing
  or development.
</ParamField>

<ParamField path="flush_on_exit" type="bool" default="True">
  If `True` (the default), the tracer provider will attempt to flush all pending
  spans when the program exits via `atexit`.
</ParamField>

<ParamField
  path="span_exclude_rules"
  type="List[SpanProcessingExcludeRule] | None"
>
  If provided, the SDK will exclude spans from being exported to LangWatch based
  on the rules defined in the list (e.g., matching span names).
</ParamField>

<ParamField
  path="ignore_global_tracer_provider_override_warning"
  type="bool"
  default="False"
>
  If `True`, suppresses the warning message logged when an existing global
  `TracerProvider` is detected and LangWatch attaches its exporter to it instead
  of overriding it.
</ParamField>

## Integrations

LangWatch offers seamless integrations with a variety of popular Python libraries and frameworks. These integrations provide automatic instrumentation, capturing relevant data from your LLM applications with minimal setup.

Below is a list of currently supported integrations. Click on each to learn more about specific setup instructions and available features:

- [Agno](/integration/python/integrations/agno)
- [AWS Bedrock](/integration/python/integrations/aws-bedrock)
- [Azure AI](/integration/python/integrations/azure-ai)
- [Crew AI](/integration/python/integrations/crew-ai)
- [DSPy](/integration/python/integrations/dspy)
- [Haystack](/integration/python/integrations/haystack)
- [Langchain](/integration/python/integrations/langchain)
- [LangGraph](/integration/python/integrations/langgraph)
- [LiteLLM](/integration/python/integrations/lite-llm)
- [OpenAI](/integration/python/integrations/open-ai)
- [OpenAI Agents](/integration/python/integrations/open-ai-agents)
- [OpenAI Azure](/integration/python/integrations/open-ai-azure)
- [Pydantic AI](/integration/python/integrations/pydantic-ai)
- [Other Frameworks](/integration/python/integrations/other)

If you are using a library that is not listed here, you can still instrument your application manually. See the [Manual Instrumentation Tutorial](/integration/python/tutorials/manual-instrumentation) for more details. Since LangWatch is built on OpenTelemetry, it also supports any library or framework that integrates with OpenTelemetry. We are also continuously working on adding support for more integrations.

## FAQ: Frequently Asked Questions

<AccordionGroup>
  <Accordion title="How do I track LLM costs and token usage?">
    LangWatch automatically captures cost and token data for most LLM providers. If you're missing costs or token counts, our [cost tracking tutorial](/integration/python/tutorials/tracking-llm-costs) covers troubleshooting steps, model cost configuration, and manual token tracking setup.
  </Accordion>

  <Accordion title="How do I capture RAG (Retrieval Augmented Generation) contexts?">
    To monitor your RAG pipelines and track retrieved documents, see our [RAG capturing guide](/integration/python/tutorials/capturing-rag). This enables specialized RAG evaluators and analytics on document usage patterns.
  </Accordion>

  <Accordion title="How can I make input and output of the trace more human readable to better read the conversation?">
    Our [input/output mapping guide](/integration/python/tutorials/capturing-mapping-input-output) shows how to properly structure chat messages, handle different data formats, and ensure your LLM conversations are captured correctly for analysis.
  </Accordion>

  <Accordion title="How do I add custom metadata and user information to traces?">
    Learn how to enrich your traces with user IDs, session data, and custom attributes in our [metadata and attributes tutorial](/integration/python/tutorials/capturing-metadata). This is essential for user analytics and filtering traces by custom criteria.
  </Accordion>

  <Accordion title="How can I capture a whole conversation?">
    To connect multiple traces into a conversation, you can use the `thread_id` metadata. See the [metadata and attributes tutorial](/integration/python/tutorials/capturing-metadata) for more details.
  </Accordion>

  <Accordion title="How do I capture evaluations and guardrails tracing data?">
    Implement automated quality checks and safety measures with our [evaluations and guardrails tutorial](/integration/python/tutorials/capturing-evaluations-guardrails). Learn to create custom evaluators and integrate safety guardrails into your LLM workflows.
  </Accordion>

  <Accordion title="How can I manually instrument my application for more fine-grained control?">
    For custom frameworks or fine-grained control, our [manual instrumentation guide](/integration/python/tutorials/manual-instrumentation) covers creating traces and spans programmatically using context managers and direct API calls.
  </Accordion>

  <Accordion title="How do I integrate with existing OpenTelemetry setups?">
    LangWatch is OpenTelemetry-based, so it can be integrated seamlessly with any OpenTelemetry-compatible application. If you already use OpenTelemetry in your application, our [OpenTelemetry integration tutorial](/integration/python/tutorials/open-telemetry) explains how to configure LangWatch alongside existing telemetry infrastructure, including custom collectors and exporters.
  </Accordion>
</AccordionGroup>

---

# FILE: ./integration/python/integrations/promptflow.mdx

---
title: PromptFlow Instrumentation
sidebarTitle: PromptFlow
description: Learn how to instrument PromptFlow applications with LangWatch.
keywords: promptflow, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

PromptFlow is a development tool designed to streamline the entire development cycle of AI applications, from ideation, prototyping, testing, evaluation to production deployment and monitoring. For more details on PromptFlow, refer to the [official PromptFlow documentation](https://microsoft.github.io/promptflow/).

LangWatch can capture traces generated by PromptFlow by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install PromptFlow and OpenInference instrumentor**:
    ```bash
    pip install promptflow openinference-instrumentation-promptflow
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for PromptFlow using the [OpenInference PromptFlow instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-promptflow). This approach automatically captures traces from your PromptFlow flows and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from promptflow import PFClient
from openinference.instrumentation.promptflow import PromptFlowInstrumentor
import os

# Initialize LangWatch with the PromptFlow instrumentor
langwatch.setup(
    instrumentors=[PromptFlowInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Initialize PromptFlow client
pf = PFClient()

# Use PromptFlow as usual—traces will be sent to LangWatch automatically
def run_promptflow_flow(flow_path: str, inputs: dict):
    # Run a flow
    result = pf.run(
        flow=flow_path,
        inputs=inputs
    )
    return result

# Example usage
if __name__ == "__main__":
    # Example flow path and inputs
    flow_path = "./my_flow"
    inputs = {
        "question": "What is the capital of France?",
        "context": "Geography information"
    }

    result = run_promptflow_flow(flow_path, inputs)
    print(f"Flow result: {result}")
```

**That's it!** All PromptFlow operations will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from promptflow import PFClient
from openinference.instrumentation.promptflow import PromptFlowInstrumentor
import os

langwatch.setup(
    instrumentors=[PromptFlowInstrumentor()]
)

# ... client setup code ...

@langwatch.trace(name="PromptFlow Flow Execution")
def run_promptflow_flow(flow_path: str, inputs: dict):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "flow_path": flow_path,
                "input_count": len(inputs)
            }
        )

    result = pf.run(
        flow=flow_path,
        inputs=inputs
    )
    return result
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `PromptFlowInstrumentor()`: The OpenInference instrumentor automatically patches PromptFlow components to create OpenTelemetry spans for their operations, including:
    - Flow execution
    - Node execution
    - LLM calls
    - Tool executions
    - Data processing
    - Input/output handling

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all flow executions, node operations, model calls, and data processing will be automatically traced and sent to LangWatch, providing comprehensive visibility into your PromptFlow-powered applications.

## Environment Variables

Make sure to set the following environment variables:

```bash
# For OpenAI
export OPENAI_API_KEY=your-openai-api-key

# For Anthropic
export ANTHROPIC_API_KEY=your-anthropic-api-key

# LangWatch API key
export LANGWATCH_API_KEY=your-langwatch-api-key
```

## Supported Models

PromptFlow supports various LLM providers including:

- OpenAI (GPT-5, GPT-4o, etc.)
- Anthropic (Claude models)
- Local models (via Ollama, etc.)
- Other providers supported by PromptFlow

All model interactions and flow executions will be automatically traced and captured by LangWatch.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine PromptFlow instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all PromptFlow activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your PromptFlow code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from promptflow import PFClient
from openinference.instrumentation.promptflow import PromptFlowInstrumentor

langwatch.setup(
    instrumentors=[PromptFlowInstrumentor()]
)

@langwatch.trace(name="Custom PromptFlow Application")
def my_custom_promptflow_app(flow_path: str, inputs: dict):
    # Your PromptFlow code here
    pf = PFClient()

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "flow_path": flow_path,
                "input_count": len(inputs)
            }
        )

    # Run your flow
    result = pf.run(
        flow=flow_path,
        inputs=inputs
    )

    return result
```

This approach allows you to combine the automatic tracing capabilities of PromptFlow with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/langchain.mdx

---
title: LangChain Instrumentation
sidebarTitle: Python
description: Learn how to instrument Langchain applications with the LangWatch Python SDK.
keywords: langchain, instrumentation, callback, opentelemetry, langwatch, python, tracing, openinference, openllmetry
---

Langchain is a powerful framework for building LLM applications. LangWatch integrates with Langchain to provide detailed observability into your chains, agents, LLM calls, and tool usage.

This guide covers the primary approaches to instrumenting Langchain with LangWatch:

1.  **Using LangWatch's Langchain Callback Handler (Recommended)**: The most direct method, using a specific callback provided by LangWatch to capture rich Langchain-specific trace data.
2.  **Using Community OpenTelemetry Instrumentors**: Leveraging dedicated Langchain instrumentors like those from OpenInference or OpenLLMetry.
3.  **Utilizing Langchain's Native OpenTelemetry Export (Advanced)**: Configuring Langchain to send its own OpenTelemetry traces to an endpoint where LangWatch can collect them.

## 1. Using LangWatch's Langchain Callback Handler (Recommended)

This is the preferred and most comprehensive method for instrumenting Langchain with LangWatch. The LangWatch SDK provides a `LangchainCallbackHandler` that deeply integrates with Langchain's event system.

```python
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
import os
import asyncio

# Initialize LangWatch
langwatch.setup()

# Standard Langchain setup
model = ChatOpenAI(streaming=True)
prompt = ChatPromptTemplate.from_messages(
    [("system", "You are a concise assistant."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - QA with Callback")
async def handle_message_with_callback(user_question: str):
    current_trace = langwatch.get_current_trace()
    current_trace.update(metadata={"user_id": "callback-user"})

    langwatch_callback = current_trace.get_langchain_callback()

    final_response = ""
    async for chunk in runnable.astream(
        {"question": user_question},
        config=RunnableConfig(callbacks=[langwatch_callback])
    ):
        final_response += chunk
    return final_response

async def main_callback():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping Langchain callback example.")
        return
    response = await handle_message_with_callback("What is Langchain? Explain briefly.")
    print(f"AI (Callback): {response}")

if __name__ == "__main__":
    asyncio.run(main_callback())
```

**How it Works:**
-   `@langwatch.trace()`: Creates a parent LangWatch trace.
-   `current_trace.get_langchain_callback()`: Retrieves a LangWatch-specific callback handler linked to the current trace.
-   `RunnableConfig(callbacks=[langwatch_callback])`: Injects the handler into Langchain's execution. Langchain emits events (on_llm_start, on_chain_end, etc.), which the handler converts into detailed LangWatch spans, correctly parented under the main trace.

**Key points:**
-   Provides the most detailed Langchain-specific structural information (chains, agents, tools, LLMs as distinct steps).
-   Works for all Langchain execution methods (`astream`, `stream`, `invoke`, `ainvoke`).

## 2. Using Community OpenTelemetry Instrumentors

Dedicated Langchain instrumentors from libraries like OpenInference and OpenLLMetry can also be used to capture Langchain operations as OpenTelemetry traces, which LangWatch can then ingest.

### Instrumenting Langchain with Dedicated Instrumentors

#### i. Via `langwatch.setup()`

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangchainInstrumentor
import os
import asyncio

langwatch.setup(
    instrumentors=[LangchainInstrumentor()] # Add OpenInference LangchainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference via Setup")
async def handle_message_oinference_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (setup) example.")
        return
    response = await handle_message_oinference_setup("Explain Langchain instrumentation with OpenInference.")
    print(f"AI (OInference Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_setup())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangchainInstrumentor # Corrected import
import os
import asyncio

langwatch.setup(
    instrumentors=[LangchainInstrumentor()] # Add OpenLLMetry LangchainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry via Setup")
async def handle_message_openllmetry_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (setup) example.")
        return
    response = await handle_message_openllmetry_setup("Explain Langchain instrumentation with OpenLLMetry.")
    print(f"AI (OpenLLMetry Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_setup())
```
</CodeGroup>

#### ii. Direct Instrumentation

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangchainInstrumentor
import os
import asyncio

langwatch.setup()
LangchainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference Direct")
async def handle_message_oinference_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (direct) example.")
        return
    response = await handle_message_oinference_direct("How does direct Langchain instrumentation work?")
    print(f"AI (OInference Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_direct())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangchainInstrumentor # Corrected import
import os
import asyncio

langwatch.setup()
LangchainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry Direct")
async def handle_message_openllmetry_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (direct) example.")
        return
    response = await handle_message_openllmetry_direct("How does direct Langchain instrumentation work with OpenLLMetry?")
    print(f"AI (OpenLLMetry Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_direct())
```
</CodeGroup>

**Key points for dedicated Langchain instrumentors:**
- Directly instrument Langchain operations, providing traces from Langchain's perspective.
- Requires installing the respective instrumentation package (e.g., `openinference-instrumentation-langchain` or `opentelemetry-instrumentation-langchain` for OpenLLMetry).

## 3. Utilizing Langchain's Native OpenTelemetry Export (Advanced)

Langchain itself can be configured to export OpenTelemetry traces. If you set this up and configure Langchain to send traces to an OpenTelemetry collector endpoint that LangWatch is also configured to receive from (or if LangWatch *is* your OTLP endpoint), then LangWatch can ingest these natively generated Langchain traces.

**Setup (Conceptual):**
1.  Configure Langchain for OpenTelemetry export. This usually involves setting environment variables:
    ```bash
    export LANGCHAIN_TRACING_V2=true
    export LANGCHAIN_ENDPOINT= # Your OTLP gRPC endpoint (e.g., http://localhost:4317)
    # Potentially other OTEL_EXPORTER_OTLP_* variables for more granular control
    ```
2.  Initialize LangWatch: `langwatch.setup()`.

```python
# This example assumes Langchain is configured via environment variables
# to send OTel traces to an endpoint LangWatch can access.
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import os
import asyncio

langwatch.setup() # LangWatch is ready to receive OTel traces

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - Native OTel Export") # Optional: group Langchain traces
async def handle_message_native_otel(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_native_otel():
    if not os.getenv("OPENAI_API_KEY") or not os.getenv("LANGCHAIN_TRACING_V2") == "true":
        print("Required env vars (OPENAI_API_KEY, LANGCHAIN_TRACING_V2='true') not set. Skipping native OTel.")
        return
    response = await handle_message_native_otel("Tell me about Langchain OTel export itself.")
    print(f"AI (Native OTel): {response}")

if __name__ == "__main__":
    asyncio.run(main_native_otel())
```

**Key points for Langchain's native OTel export:**
-   LangWatch acts as a backend/collector for OpenTelemetry traces generated directly by Langchain.
-   Requires careful configuration of Langchain's environment variables.
-   The level of detail depends on Langchain's native OpenTelemetry instrumentation quality.

<Note>
### Which Approach to Choose?

-   **LangWatch's Langchain Callback Handler (Recommended)**: Provides the richest, most Langchain-aware traces directly integrated with LangWatch's tracing context. Ideal for most users.
-   **Dedicated Langchain Instrumentors (OpenInference, OpenLLMetry)**: Good alternatives if you prefer an explicit instrumentor pattern for Langchain itself or are standardizing on these specific OpenTelemetry ecosystems.
-   **Langchain's Native OTel Export (Advanced)**: Suitable if you have an existing OpenTelemetry collection infrastructure and want Langchain to be another OTel-compliant source.

For the best Langchain-specific observability within LangWatch, the **Langchain Callback Handler** is the generally recommended approach, with dedicated **Langchain Instrumentors** as strong alternatives for instrumentor-based setups.
</Note>

---

# FILE: ./integration/python/integrations/agno.mdx

---
title: Agno Instrumentation
sidebarTitle: Agno
description: Learn how to instrument Agno agents and send traces to LangWatch using the Python SDK.
keywords: agno, openinference, openlit, opentelemetry, LangWatch, Python, tracing, observability
---

LangWatch supports seamless observability for [Agno](https://github.com/agno-agi/agno) agents. You can instrument your Agno-based applications and send traces directly to LangWatch for monitoring and analysis.

This guide shows how to set up tracing for Agno agents using the LangWatch Python SDK. Unlike the default OpenTelemetry/OTLP setup, you only need to call `langwatch.setup()`—no manual exporter or environment variable configuration is required.

## Prerequisites

- Install the required packages:
  ```bash
  pip install agno openai langwatch openinference-instrumentation-agno
  ```
- Get your LangWatch API key from your [project settings](https://app.langwatch.ai/) and set it as an environment variable:
  ```bash
  export LANGWATCH_API_KEY=your-langwatch-api-key
  ```

## Instrumenting Agno with LangWatch

You can use the [OpenInference Agno instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-agno) to automatically capture traces from your Agno agents. Just pass the instrumentor to `langwatch.setup()`.

```python
import langwatch
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor

# Initialize LangWatch and instrument Agno
langwatch.setup(
    instrumentors=[AgnoInstrumentor()]
)

# Create and configure your Agno agent
agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-5"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

# Use the agent as usual—traces will be sent to LangWatch
agent.print_response("What is the current price of Tesla?")
```

**That's it!** All Agno agent activity will now be traced and sent to your LangWatch dashboard.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Agno instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.

For more details, see the [LangWatch Python SDK reference](/integration/python/reference) and the [Agno documentation](https://docs.agno.com/observability/langfuse).

---

# FILE: ./integration/python/integrations/haystack.mdx

---
title: Haystack Instrumentation
sidebarTitle: Haystack
description: Learn how to instrument Haystack pipelines with LangWatch using community OpenTelemetry instrumentors.
keywords: haystack, deepset, instrumentation, openinference, openllmetry, LangWatch, Python
---

LangWatch does not have a built-in auto-tracking integration for Haystack from deepset.ai. However, you can leverage community-provided OpenTelemetry instrumentors to integrate your Haystack pipelines with LangWatch.

## Integrating Community Instrumentors with LangWatch

Community-provided OpenTelemetry instrumentors for Haystack allow you to automatically capture detailed trace data from your Haystack pipeline components (Nodes, Pipelines, etc.). LangWatch can seamlessly integrate with these instrumentors.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the Haystack instrumentor to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

<CodeGroup>

```python openinference_setup.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from openinference.instrumentation.haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup(
    instrumentors=[HaystackInstrumentor()]
)

# Initialize Haystack components
document_store = InMemoryDocumentStore(use_bm25=True)
# You can add documents to the store here if needed

prompt_template = PromptTemplate(
    prompt="Answer the question based on the context: {query}",
    output_parser=None # Or specify an output parser
)
prompt_node = PromptNode(
    model_name_or_path="gpt-5", # Replace with your desired model
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)

pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="PromptNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenInference (Setup)")
def run_haystack_pipeline_oi_setup(query: str):
    # The OpenInference instrumentor, when configured via langwatch.setup(),
    # should automatically capture Haystack operations.
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "What is the capital of France?"
        print(f"Running Haystack pipeline with OpenInference (setup) for query: {user_query}")
        output = run_haystack_pipeline_oi_setup(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

```python openllmetry_setup.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from opentelemetry_instrumentation_haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup(
    instrumentors=[HaystackInstrumentor()]
)

# Initialize Haystack components
document_store = InMemoryDocumentStore(use_bm25=True)
prompt_template = PromptTemplate(prompt="Answer the question: {query}")
prompt_node = PromptNode(
    model_name_or_path="gpt-5",
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)
pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="MyPromptNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenLLMetry (Setup)")
def run_haystack_pipeline_ollm_setup(query: str):
    # The OpenLLMetry instrumentor should automatically capture Haystack operations.
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "What is deep learning?"
        print(f"Running Haystack pipeline with OpenLLMetry (setup) for query: {user_query}")
        output = run_haystack_pipeline_ollm_setup(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

</CodeGroup>

<Note>
  Ensure you have the respective community instrumentation library installed:
  - For OpenInference: `pip install openinference-instrumentation-haystack`
  - For OpenLLMetry: `pip install opentelemetry-instrumentation-haystack`
  You'll also need Haystack: `pip install farm-haystack[openai]` (if using OpenAI models).
  Consult the specific library's documentation for the exact package name and instrumentor class if the above assumptions are incorrect.
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

<CodeGroup>

```python openinference_direct.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from openinference.instrumentation.haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup()

# Instrument Haystack directly using OpenInference
HaystackInstrumentor().instrument()

document_store = InMemoryDocumentStore(use_bm25=True)
prompt_template = PromptTemplate(prompt="Summarize this for a 5-year old: {query}")
prompt_node = PromptNode(
    model_name_or_path="gpt-5",
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)
pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="SummarizerNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenInference (Direct)")
def run_haystack_pipeline_oi_direct(query: str):
    # Spans from Haystack operations should be captured by the directly configured instrumentor.
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "The quick brown fox jumps over the lazy dog."
        print(f"Running Haystack pipeline with OpenInference (direct) for query: {user_query}")
        output = run_haystack_pipeline_oi_direct(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

```python openllmetry_direct.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from opentelemetry_instrumentation_haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup()

# Instrument Haystack directly using OpenLLMetry
HaystackInstrumentor().instrument()

document_store = InMemoryDocumentStore(use_bm25=True)
prompt_template = PromptTemplate(prompt="Translate to French: {query}")
prompt_node = PromptNode(
    model_name_or_path="gpt-5",
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)
pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="TranslatorNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenLLMetry (Direct)")
def run_haystack_pipeline_ollm_direct(query: str):
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "Hello, how are you?"
        print(f"Running Haystack pipeline with OpenLLMetry (direct) for query: {user_query}")
        output = run_haystack_pipeline_ollm_direct(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

</CodeGroup>

### Key points for community instrumentors:
-   These instrumentors typically patch Haystack at a global level or integrate deeply with its execution flow (e.g., via `BaseComponent` or `Pipeline` hooks), meaning most Haystack operations should be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup and lifecycle of the instrumentor.
-   If instrumenting directly (e.g., `HaystackInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.
-   Always refer to the specific documentation of the community instrumentor (OpenInference or OpenLLMetry) for the most accurate and up-to-date installation and usage instructions, including the correct class names for instrumentors and any specific setup requirements for different Haystack versions or components.
-   The examples use a `PromptNode` with an OpenAI model for simplicity. Ensure you have the necessary API keys (e.g., `OPENAI_API_KEY`) set in your environment if you run these examples.

---

# FILE: ./integration/python/integrations/open-ai-azure.mdx

---
title: Azure OpenAI Instrumentation
sidebarTitle: Azure OpenAI
description: Learn how to instrument Azure OpenAI API calls with the LangWatch Python SDK
keywords: azure openai, openai, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Azure OpenAI, allowing you to capture detailed information about your LLM calls automatically. There are two primary approaches to instrumenting your Azure OpenAI interactions:

1.  **Using `autotrack_openai_calls()`**: This method, part of the LangWatch SDK, dynamically patches your `AzureOpenAI` client instance to capture calls made through it within a specific trace.
2.  **Using Community OpenTelemetry Instrumentors**: Leverage existing OpenTelemetry instrumentation libraries like those from OpenInference or OpenLLMetry. These can be integrated with LangWatch by either passing them to the `langwatch.setup()` function or by using their native `instrument()` methods if you're managing your OpenTelemetry setup more directly.

This guide will walk you through both methods.

## Using `autotrack_openai_calls()`

The `autotrack_openai_calls()` function provides a straightforward way to capture all Azure OpenAI calls made with a specific client instance for the duration of the current trace.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`.

```python
import langwatch
from openai import AzureOpenAI
import os

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup()

# Initialize your AzureOpenAI client
# Ensure your Azure environment variables are set:
# AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME
client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2023-05-15"  # Or your preferred API version
)


@langwatch.trace(name="Azure OpenAI Chat Completion")
async def get_azure_openai_chat_response(user_prompt: str):
    # Get the current trace and enable autotracking for the 'client' instance
    langwatch.get_current_trace().autotrack_openai_calls(client)

    # All calls made with 'client' will now be automatically captured as spans
    response = client.chat.completions.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"), # Use your Azure deployment name
        messages=[{"role": "user", "content": user_prompt}],
    )
    completion = response.choices[0].message.content
    return completion

async def main():
    user_query = "Tell me a fact about the Azure cloud."
    response = await get_azure_openai_chat_response(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

Key points for `autotrack_openai_calls()` with Azure OpenAI:
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments a *specific instance* of the `AzureOpenAI` client. If you have multiple clients, you'll need to call it for each one you want to track.
-   Ensure your `AzureOpenAI` client is correctly configured with `azure_endpoint`, `api_key`, `api_version`, and you use the deployment name for the `model` parameter.

## Using Community OpenTelemetry Instrumentors

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from the `openai` library, which `AzureOpenAI` is part of.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `OpenAIInstrumentor` from OpenInference) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from openai import AzureOpenAI
import os

from openinference.instrumentation.openai import OpenAIInstrumentor

# Initialize LangWatch with the OpenAIInstrumentor
langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

# Initialize your AzureOpenAI client
client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2023-05-15"
)

@langwatch.trace(name="Azure OpenAI Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles OpenAI calls globally.
    response = client.chat.completions.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "your-deployment-name"), # Use your Azure deployment name
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    user_query = "Explain Azure Machine Learning in simple terms."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```
<Note>
  Ensure you have the respective community instrumentation library installed (e.g., `pip install openllmetry-instrumentation-openai` or `pip install openinference-instrumentation-openai`). The instrumentor works with `AzureOpenAI` as it's part of the same `openai` Python package.
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from openai import AzureOpenAI
import os

from openinference.instrumentation.openai import OpenAIInstrumentor

langwatch.setup() # LangWatch sets up or uses the global OpenTelemetry provider

# Initialize your AzureOpenAI client
client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2023-05-15"
)

# Instrument OpenAI directly using the community library
# This will patch the openai library, affecting AzureOpenAI instances too.
OpenAIInstrumentor().instrument()

@langwatch.trace(name="Azure OpenAI Call with Direct Community Instrumentation")
def get_creative_idea(topic: str):
    response = client.chat.completions.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "your-deployment-name"), # Use your Azure deployment name
        messages=[
            {"role": "system", "content": "You are an idea generation bot."},
            {"role": "user", "content": f"Generate a creative idea about {topic}."}
        ]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    subject = "sustainable energy"
    idea = get_creative_idea(subject)
    print(f"Topic: {subject}")
    print(f"AI's Idea: {idea}")
```

### Key points for community instrumentors with Azure OpenAI:
-   These instrumentors often patch the `openai` library at a global level, meaning all calls from any `OpenAI` or `AzureOpenAI` client instance will be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the instrumentor's setup.
-   If instrumenting directly (e.g., `OpenAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This typically happens automatically if LangWatch initializes the global provider or if you configure them to use the same explicit provider.

<Note>
### Which Approach to Choose?

-   **`autotrack_openai_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which `AzureOpenAI` client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
-   **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture Azure OpenAI calls globally across your application, or need to instrument other libraries alongside Azure OpenAI with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send Azure OpenAI call data to LangWatch for monitoring and analysis.
</Note>

---

# FILE: ./integration/python/integrations/google-ai.mdx

---
title: Google Agent Development Kit (ADK) Instrumentation
sidebarTitle: Google ADK
description: Learn how to instrument Google Agent Development Kit (ADK) applications with LangWatch.
keywords: google adk, agent development kit, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

The Google Agent Development Kit (ADK) streamlines building, orchestrating, and tracing generative-AI agents out of the box, letting you move from prototype to production far faster than wiring everything yourself. For more details on ADK, refer to the [official Google ADK documentation](https://ai.google.dev/docs/adk).

LangWatch can capture traces generated by Google ADK by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Google ADK and OpenInference instrumentor**:
    ```bash
    pip install google-adk openinference-instrumentation-google-adk
    ```

3.  **Set up Google Cloud authentication**:
    You'll need to authenticate with Google Cloud. You can either:
    - Set the `GOOGLE_API_KEY` environment variable for Gemini API access
    - Use Application Default Credentials (ADC) if running on Google Cloud
    - Use service account keys for production deployments

## Instrumentation with OpenInference

LangWatch supports seamless observability for Google ADK agents using the [OpenInference Google ADK instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-google-adk). This approach automatically captures traces from your ADK agents and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from google.adk import Agent, Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from openinference.instrumentation.google_adk import GoogleADKInstrumentor
import os

# Initialize LangWatch with the Google ADK instrumentor
langwatch.setup(
    instrumentors=[GoogleADKInstrumentor()]
)

# Set up environment variables
os.environ["GOOGLE_API_KEY"] = "your-gemini-api-key"

# Define your agent tools
def say_hello():
    return {"greeting": "Hello LangWatch 👋"}

def get_weather(location: str):
    return {"location": location, "temperature": "22°C", "condition": "sunny"}

# Create your agent
agent = Agent(
    name="hello_agent",
    model="gemini-2.0-flash",
    instruction="Always greet using the say_hello tool and provide weather information when asked.",
    tools=[say_hello, get_weather],
)

# Set up session service and runner
session_service = InMemorySessionService()
session_service.create_session(
    app_name="hello_app", user_id="demo-user", session_id="demo-session"
)

runner = Runner(agent=agent, app_name="hello_app", session_service=session_service)

# Use the agent as usual—traces will be sent to LangWatch automatically
def run_agent_interaction(user_message: str):
    user_msg = types.Content(role="user", parts=[types.Part(text=user_message)])

    for event in runner.run(user_id="demo-user", session_id="demo-session", new_message=user_msg):
        if event.is_final_response():
            return event.content.parts[0].text

    return "No response generated"

# Example usage
if __name__ == "__main__":
    user_prompt = "hi"
    response = run_agent_interaction(user_prompt)
    print(f"User: {user_prompt}")
    print(f"Agent: {response}")
```

**That's it!** All Google ADK agent activity will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from google.adk import Agent, Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from openinference.instrumentation.google_adk import GoogleADKInstrumentor
import os

langwatch.setup(
    instrumentors=[GoogleADKInstrumentor()]
)

# ... agent setup code ...

@langwatch.trace(name="Google ADK Agent Run")
def run_agent_interaction(user_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "hello_agent",
                "model": "gemini-2.0-flash"
            }
        )

    user_msg = types.Content(role="user", parts=[types.Part(text=user_message)])

    for event in runner.run(user_id="demo-user", session_id="demo-session", new_message=user_msg):
        if event.is_final_response():
            return event.content.parts[0].text

    return "No response generated"
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `GoogleADKInstrumentor()`: The OpenInference instrumentor automatically patches Google ADK components to create OpenTelemetry spans for their operations, including:
    - Agent initialization
    - Tool calls
    - Model completions
    - Session management

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all agent interactions, tool calls, and model completions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your ADK-powered applications.


## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Google ADK instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all ADK activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.
- Ensure you have the correct Google API key set for Gemini access.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from google.adk import Agent, Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from openinference.instrumentation.google_adk import GoogleADKInstrumentor

langwatch.setup(
    instrumentors=[GoogleADKInstrumentor()]
)

@langwatch.trace(name="Custom ADK Agent")
def my_custom_agent(input_message: str):
    # Your ADK agent code here
    agent = Agent(
        name="custom_agent",
        model="gemini-2.0-flash",
        instruction="Your custom instructions",
        tools=[your_custom_tools]
    )

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "custom_agent",
                "model": "gemini-2.0-flash"
            }
        )

    # Run your agent
    # ... agent execution code ...

    return "Agent response"
```

This approach allows you to combine the automatic tracing capabilities of Google ADK with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/dspy.mdx

---
title: DSPy Instrumentation
sidebarTitle: DSPy
description: Learn how to instrument DSPy programs with the LangWatch Python SDK
keywords: dspy, instrumentation, autotrack, LangWatch, Python
---

LangWatch provides seamless integration with DSPy, allowing you to automatically capture detailed information about your DSPy program executions, including module calls and language model interactions.

The primary way to instrument your DSPy programs is by using `autotrack_dspy()` on the current LangWatch trace.

## Using `autotrack_dspy()`

The `autotrack_dspy()` function, when called on an active trace object, dynamically patches the DSPy framework to capture calls made during the execution of your DSPy programs within that trace.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`. This ensures that all DSPy operations within that traced function are monitored.

```python
import langwatch
import dspy
import os

# Ensure LANGWATCH_API_KEY is set, or set it in langwatch.setup()
langwatch.setup() # If not done elsewhere or API key not in env

# Initialize your DSPy LM (Language Model)
# This example uses OpenAI, ensure OPENAI_API_KEY is set
lm = dspy.LM("openai/gpt-5", api_key=os.environ.get("OPENAI_API_KEY"))
dspy.settings.configure(lm=lm)

class BasicRAG(dspy.Signature):
    """Answer questions with short factoid answers."""
    question = dspy.InputField()
    answer = dspy.OutputField(desc="often between 1 and 5 words")

class SimpleModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_answer = dspy.Predict(BasicRAG)

    def forward(self, question):
        prediction = self.generate_answer(question=question)
        return dspy.Prediction(answer=prediction.answer)

@langwatch.trace(name="DSPy RAG Execution")
def run_dspy_program(user_query: str):
    # Get the current trace and enable autotracking for DSPy
    current_trace = langwatch.get_current_trace().autotrack_dspy()

    program = SimpleModule()
    prediction = program(question=user_query)
    return prediction.answer

def main():
    user_question = "What is the capital of France?"
    response = run_dspy_program(user_question)
    print(f"Question: {user_question}")
    print(f"Answer: {response}")

if __name__ == "__main__":
    main()
```

### Key points for `autotrack_dspy()`:
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments DSPy operations specifically for the duration and scope of the current trace.
-   LangWatch will capture interactions with DSPy modules (like `dspy.Predict`, `dspy.ChainOfThought`, `dspy.Retrieve`) and the underlying LM calls.

## Using Community OpenTelemetry Instrumentors

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like OpenInference, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM frameworks, including DSPy.

The [OpenInference community provides an instrumentor for DSPy](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-dspy) which can be used with LangWatch.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `OpenInferenceDSPyInstrumentor` from OpenInference) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
import dspy
import os

from openinference.instrumentation.dspy import DSPyInstrumentor

# Initialize LangWatch with the DSPyInstrumentor
langwatch.setup(
    instrumentors=[DSPyInstrumentor()]
)

# Configure DSPy LM
# This example uses OpenAI, ensure OPENAI_API_KEY is set
lm = dspy.LM("openai/gpt-5", api_key=os.environ.get("OPENAI_API_KEY"))
dspy.settings.configure(lm=lm)

class BasicRAG(dspy.Signature):
    question = dspy.InputField()
    answer = dspy.OutputField()

class SimpleModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_answer = dspy.Predict(BasicRAG)

    def forward(self, question):
        prediction = self.generate_answer(question=question)
        return dspy.Prediction(answer=prediction.answer)

@langwatch.trace(name="DSPy Call with Community Instrumentor")
def generate_with_community_instrumentor(prompt: str):
    # No need to call autotrack_dspy explicitly,
    # the community instrumentor handles DSPy calls globally.
    program = SimpleModule()
    prediction = program(question=prompt)
    return prediction.answer

if __name__ == "__main__":
    # from dotenv import load_dotenv # Make sure to load .env if you have one
    # load_dotenv()
    user_query = "What is DSPy?"
    response = generate_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

```
<Note>
  Ensure you have the respective community instrumentation library installed (e.g., `pip install openinference-instrumentation-dspy`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
import dspy
import os

from openinference.instrumentation.dspy import DSPyInstrumentor

langwatch.setup() # Initialize LangWatch

# Configure DSPy LM
lm = dspy.LM("openai/gpt-5", api_key=os.environ.get("OPENAI_API_KEY"))
dspy.settings.configure(lm=lm)

# Instrument DSPy directly using the community library
DSPyInstrumentor().instrument()

class BasicRAG(dspy.Signature):
    question = dspy.InputField()
    answer = dspy.OutputField()

class SimpleModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_answer = dspy.Predict(BasicRAG)

    def forward(self, question):
        prediction = self.generate_answer(question=question)
        return dspy.Prediction(answer=prediction.answer)

@langwatch.trace(name="DSPy Call with Direct Community Instrumentation")
def ask_dspy_directly_instrumented(original_question: str):
    program = SimpleModule()
    prediction = program(question=original_question)
    return prediction.answer

if __name__ == "__main__":
    query = "Explain the concept of few-shot learning in LLMs."
    response = ask_dspy_directly_instrumented(query)
    print(f"Query: {query}")
    print(f"Response: {response}")
```

### Key points for community instrumentors:
-   These instrumentors often patch DSPy at a global level, meaning all DSPy calls will be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
-   If instrumenting directly (e.g., `DSPyInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

<Note>
### Which Approach to Choose?

-   **`autotrack_dspy()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which DSPy program executions are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
-   **Community Instrumentors** (like OpenInference's `DSPyInstrumentor`) are powerful if you're already using OpenTelemetry, want to capture DSPy calls globally across your application, or need to instrument other libraries alongside DSPy with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send DSPy call data to LangWatch for monitoring and analysis.
</Note>

## Example: Chainlit Bot with DSPy and LangWatch

The following is a more complete example demonstrating `autotrack_dspy()` in a Chainlit application, similar to the `dspy_bot.py` found in the SDK examples.

```python
import os

import chainlit as cl
import langwatch
import dspy

langwatch.setup()

# Configure DSPy LM and RM (Retriever Model)
# Ensure OPENAI_API_KEY is in your environment for the LM
lm = dspy.LM("openai/gpt-5", api_key=os.environ["OPENAI_API_KEY"])
# This example uses a public ColBERTv2 instance for retrieval
colbertv2_wiki17_abstracts = dspy.ColBERTv2(
    url="http://20.102.90.50:2017/wiki17_abstracts"
)
dspy.settings.configure(lm=lm, rm=colbertv2_wiki17_abstracts)


class GenerateAnswer(dspy.Signature):
    """Answer questions with short factoid answers."""

    context = dspy.InputField(desc="may contain relevant facts")
    question = dspy.InputField()
    answer = dspy.OutputField(desc="often between 1 and 5 words")


class RAG(dspy.Module):
    def __init__(self, num_passages=3):
        super().__init__()
        self.retrieve = dspy.Retrieve(k=num_passages)
        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

    def forward(self, question):
        context = self.retrieve(question).passages  # type: ignore
        prediction = self.generate_answer(question=question, context=context)
        return dspy.Prediction(answer=prediction.answer)


@cl.on_message # Decorator for Chainlit message handling
@langwatch.trace() # Decorator to trace this function with LangWatch
async def main_chat_handler(message: cl.Message):
    # Get the current LangWatch trace and enable DSPy autotracking
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.autotrack_dspy()

    msg_ui = cl.Message(content="") # Chainlit UI message

    # Initialize and run the DSPy RAG program
    program = RAG()
    prediction = program(question=message.content)

    # Stream the answer to the Chainlit UI
    # Note: For simplicity, this example doesn't stream token by token
    # from the DSPy prediction if it were a streaming response.
    # DSPy's prediction.answer is typically the full string.
    final_answer = prediction.answer
    if isinstance(final_answer, str):
        await msg_ui.stream_token(final_answer)
    else:
        # Handle cases where answer might not be a direct string (e.g. structured output)
        await msg_ui.stream_token(str(final_answer))

    await msg_ui.update()

# To run this example:
# 1. Ensure you have .env file with OPENAI_API_KEY and LANGWATCH_API_KEY.
# 2. Install dependencies: pip install langwatch dspy-ai openai chainlit python-dotenv
# 3. Run with Chainlit: chainlit run your_script_name.py -w
```

By calling `autotrack_dspy()` within your LangWatch-traced functions, you gain valuable insights into your DSPy program's behavior, including latencies, token counts, and the flow of data through your defined modules and signatures. This is essential for debugging, optimizing, and monitoring your DSPy-powered AI applications.

---

# FILE: ./integration/python/integrations/llamaindex.mdx

---
title: LlamaIndex Instrumentation
sidebarTitle: LlamaIndex
description: Learn how to instrument LlamaIndex applications with LangWatch.
keywords: llamaindex, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

LlamaIndex is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. For more details on LlamaIndex, refer to the [official LlamaIndex documentation](https://docs.llamaindex.ai/).

LangWatch can capture traces generated by LlamaIndex by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install LlamaIndex and OpenInference instrumentor**:
    ```bash
    pip install llama-index openinference-instrumentation-llama-index
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for LlamaIndex using the [OpenInference LlamaIndex instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-llama-index). This approach automatically captures traces from your LlamaIndex applications and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
import os

# Initialize LangWatch with the LlamaIndex instrumentor
langwatch.setup(
    instrumentors=[LlamaIndexInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create documents
documents = SimpleDirectoryReader('data').load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Create query engine
query_engine = index.as_query_engine()

# Use the query engine as usual—traces will be sent to LangWatch automatically
def run_query(user_question: str):
    response = query_engine.query(user_question)
    return response

# Example usage
if __name__ == "__main__":
    user_question = "What is the main topic of the documents?"
    response = run_query(user_question)
    print(f"Question: {user_question}")
    print(f"Answer: {response}")
```

**That's it!** All LlamaIndex activity will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
import os

langwatch.setup(
    instrumentors=[LlamaIndexInstrumentor()]
)

# ... index setup code ...

@langwatch.trace(name="LlamaIndex Query")
def run_query(user_question: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "index_name": "my_documents",
                "model": "gpt-5"
            }
        )

    response = query_engine.query(user_question)
    return response
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `LlamaIndexInstrumentor()`: The OpenInference instrumentor automatically patches LlamaIndex components to create OpenTelemetry spans for their operations, including:
    - Document loading and processing
    - Index creation and updates
    - Query execution
    - LLM calls
    - Retrieval operations

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all document processing, indexing, querying, and LLM interactions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your LlamaIndex-powered applications.


## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine LlamaIndex instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all LlamaIndex activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your LlamaIndex code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor

langwatch.setup(
    instrumentors=[LlamaIndexInstrumentor()]
)

@langwatch.trace(name="Custom LlamaIndex Application")
def my_custom_llamaindex_app(user_question: str):
    # Your LlamaIndex code here
    documents = SimpleDirectoryReader('data').load_data()
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "index_name": "custom_index",
                "model": "gpt-5"
            }
        )

    # Run your query
    response = query_engine.query(user_question)

    return response
```

This approach allows you to combine the automatic tracing capabilities of LlamaIndex with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/aws-bedrock.mdx

---
title: AWS Bedrock Instrumentation
sidebarTitle: AWS Bedrock
description: Learn how to instrument AWS Bedrock calls with the LangWatch Python SDK using OpenInference.
keywords: aws, bedrock, boto3, instrumentation, opentelemetry, openinference, langwatch, python, tracing
---

AWS Bedrock, accessed via the `boto3` library, allows you to leverage powerful foundation models. By using the OpenInference Bedrock instrumentor, you can automatically capture OpenTelemetry traces for your Bedrock API calls. LangWatch, being an OpenTelemetry-compatible observability platform, can seamlessly ingest these traces, providing insights into your LLM interactions.

This guide explains how to configure your Python application to send Bedrock traces to LangWatch.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Bedrock Instrumentation and Dependencies**:
    You'll need `boto3` to interact with AWS Bedrock, and the OpenInference instrumentation library for Bedrock.
    ```bash
    pip install boto3 openinference-instrumentation-bedrock
    ```
    Note: `openinference-instrumentation-bedrock` will install necessary OpenTelemetry packages. Ensure your `boto3` and `botocore` versions are compatible with the Bedrock features you intend to use (e.g., `botocore >= 1.34.116` for the `converse` API).

## Instrumenting AWS Bedrock with LangWatch

The integration involves initializing LangWatch to set up the OpenTelemetry environment and then applying the Bedrock instrumentor.

### Steps:

1.  **Initialize LangWatch**: Call `langwatch.setup()` at the beginning of your application. This configures the global OpenTelemetry SDK to export traces to LangWatch.
2.  **Instrument Bedrock**: Import `BedrockInstrumentor` and call its `instrument()` method. This will patch `boto3` to automatically create spans for Bedrock client calls.

```python
import langwatch
import boto3
import json
import os
import asyncio

# 1. Initialize LangWatch for OpenTelemetry trace export
langwatch.setup()

# 2. Instrument Boto3 for Bedrock
from openinference.instrumentation.bedrock import BedrockInstrumentor
BedrockInstrumentor().instrument()

# Global Bedrock client (initialize after instrumentation)
bedrock_runtime = None
try:
    aws_session = boto3.session.Session(
        region_name=os.environ.get("AWS_REGION_NAME") # Ensure region is set
    )
    bedrock_runtime = aws_session.client("bedrock-runtime")
except Exception as e:
    print(f"Error creating Bedrock client: {e}. Ensure AWS credentials and region are configured.")

@langwatch.span(name="Bedrock - Invoke Claude")
async def invoke_claude(prompt_text: str):
    if not bedrock_runtime:
        print("Bedrock client not initialized. Skipping API call.")
        return None

    current_span = langwatch.get_current_span()
    current_span.update(model_id="anthropic.claude-v2", action="invoke_model")

    try:
        body = json.dumps({
            "prompt": f"Human: {prompt_text} Assistant:",
            "max_tokens_to_sample": 200
        })
        response = bedrock_runtime.invoke_model(modelId="anthropic.claude-v2", body=body)
        response_body = json.loads(response.get("body").read())
        completion = response_body.get("completion")
        current_span.update(outputs={"completion_preview": completion[:50] + "..." if completion else "N/A"})
        return completion
    except Exception as e:
        print(f"Error invoking model: {e}")
        if current_span:
            current_span.record_exception(e)
            current_span.set_status("error", str(e))
        raise

@langwatch.trace(name="Bedrock - Example Usage")
async def main():
    try:
        prompt = "Explain the concept of OpenTelemetry in one sentence."
        print(f"Invoking model with prompt: '{prompt}'")
        response = await invoke_claude(prompt)
        if response:
            print(f"Response from Claude: {response}")
    except Exception as e:
        print(f"An error occurred in main: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

**Key points for this approach:**
-   `langwatch.setup()`: Initializes the global OpenTelemetry environment configured for LangWatch. This must be called before any instrumented code is run.
-   `BedrockInstrumentor().instrument()`: This call patches the `boto3` library. Any subsequent Bedrock calls made using a `boto3.client("bedrock-runtime")` will automatically generate OpenTelemetry spans.
-   `@langwatch.trace()`: Creates a parent trace in LangWatch. The automated Bedrock spans generated by OpenInference will be nested under this parent trace if the Bedrock calls are made within the decorated function. This provides a clear hierarchy for your operations.
-   **API Versions**: The example shows both `invoke_model` and `converse` APIs. The `converse` API requires `botocore` version `1.34.116` or newer.

By following these steps, your application's interactions with AWS Bedrock will be traced, and the data will be sent to LangWatch for monitoring and analysis. This allows you to observe latencies, errors, and other metadata associated with your foundation model calls.
For more details on the specific attributes captured by the OpenInference Bedrock instrumentor, please refer to the [OpenInference Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/ai/openinference/). (Note: Link to general OTel AI/OpenInference conventions, specific Bedrock attributes might be detailed in OpenInference's own docs).

Remember to replace placeholder values for AWS credentials and adapt the model IDs and prompts to your specific use case.

---

# FILE: ./integration/python/integrations/instructor.mdx

---
title: Instructor AI Instrumentation
sidebarTitle: Instructor AI
description: Learn how to instrument Instructor AI applications with LangWatch using OpenInference.
keywords: instructor, python, sdk, instrumentation, opentelemetry, langwatch, tracing, openinference, structured output
---

Instructor AI is a library that provides structured output capabilities for LLMs, making it easier to extract structured data from language models. For more details on Instructor AI, refer to the [official Instructor documentation](https://jxnl.github.io/instructor/).

LangWatch can capture traces generated by Instructor AI by leveraging OpenInference's OpenAI instrumentation, since Instructor AI is built on top of OpenAI's client. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Instructor AI and OpenInference instrumentor**:
    ```bash
    pip install instructor openinference-instrumentation-instructor
    ```

3.  **Set up your OpenAI API key**:
    You'll need to configure your OpenAI API key in your environment.

## Instrumentation with OpenInference

LangWatch supports seamless observability for Instructor AI using the [OpenInference Instructor AI instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-instructor). This dedicated instrumentor automatically captures traces from your Instructor AI calls and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
import instructor
from openinference.instrumentation.instructor import InstructorInstrumentor
from openai import OpenAI
import os
from pydantic import BaseModel
from typing import List

# Initialize LangWatch with the Instructor AI instrumentor
langwatch.setup(
    instrumentors=[InstructorInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create an OpenAI client
client = OpenAI()

# Patch the client with Instructor
client = instructor.patch(client)

# Define your Pydantic models for structured output
class User(BaseModel):
    name: str
    age: int
    email: str

class UserList(BaseModel):
    users: List[User]

# Use the client as usual—traces will be sent to LangWatch automatically
def extract_user_info(text: str) -> User:
    return client.chat.completions.create(
        model="gpt-5",
        response_model=User,
        messages=[
            {"role": "user", "content": f"Extract user information from: {text}"}
        ]
    )

def extract_multiple_users(text: str) -> UserList:
    return client.chat.completions.create(
        model="gpt-5",
        response_model=UserList,
        messages=[
            {"role": "user", "content": f"Extract all users from: {text}"}
        ]
    )

# Example usage
if __name__ == "__main__":
    text = "John is 25 years old and his email is john@example.com"
    user = extract_user_info(text)
    print(f"Extracted user: {user}")

    multiple_text = "Alice is 30 (alice@example.com) and Bob is 28 (bob@example.com)"
    users = extract_multiple_users(multiple_text)
    print(f"Extracted users: {users}")
```

**That's it!** All Instructor AI calls will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
import instructor
from openinference.instrumentation.instructor import InstructorInstrumentor
from openai import OpenAI
import os
from pydantic import BaseModel

langwatch.setup(
    instrumentors=[InstructorInstrumentor()]
)

client = OpenAI()
client = instructor.patch(client)

class Product(BaseModel):
    name: str
    price: float
    category: str

@langwatch.trace(name="Product Information Extraction")
def extract_product_info(text: str) -> Product:
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "extraction_type": "product_info",
                "model": "gpt-5",
                "source_text_length": len(text)
            }
        )

    return client.chat.completions.create(
        model="gpt-5",
        response_model=Product,
        messages=[
            {"role": "user", "content": f"Extract product information from: {text}"}
        ]
    )
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `InstructorInstrumentor()`: The OpenInference instrumentor automatically patches Instructor AI operations to create OpenTelemetry spans for their operations, including:
    - Structured output generation
    - Model calls with response models
    - Validation and parsing
    - Error handling

3.  **Instructor AI Integration**: The dedicated Instructor AI instrumentor captures all Instructor AI operations (structured output generation, validation, etc.) as spans.

4.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all Instructor AI operations, including structured output generation, validation, and error handling, will be automatically traced and sent to LangWatch, providing comprehensive visibility into your structured data extraction applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Instructor AI instrumentation with other instrumentors (e.g., LangChain, DSPy) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all Instructor AI activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your Instructor AI code is being executed.
- Ensure you have the correct OpenAI API key set.
- Verify that your Pydantic models are properly defined and compatible with Instructor AI.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
import instructor
from openinference.instrumentation.instructor import InstructorInstrumentor
from openai import OpenAI
import os
from pydantic import BaseModel

langwatch.setup(
    instrumentors=[InstructorInstrumentor()]
)

client = OpenAI()
client = instructor.patch(client)

class Task(BaseModel):
    title: str
    priority: str
    due_date: str

@langwatch.trace(name="Task Extraction Pipeline")
def extract_tasks_from_text(text: str) -> List[Task]:
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "pipeline_type": "task_extraction",
                "model": "gpt-5",
                "input_length": len(text)
            }
        )

    # Your Instructor AI code here
    return client.chat.completions.create(
        model="gpt-5",
        response_model=List[Task],
        messages=[
            {"role": "user", "content": f"Extract tasks from: {text}"}
        ]
    )
```

This approach allows you to combine the automatic tracing capabilities of Instructor AI with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/semantic-kernel.mdx

---
title: Semantic Kernel Instrumentation
sidebarTitle: Semantic Kernel
description: Learn how to instrument Semantic Kernel applications with LangWatch.
keywords: semantic-kernel, python, sdk, instrumentation, opentelemetry, langwatch, tracing, openinference
---

Semantic Kernel is a lightweight SDK that enables you to easily build AI agents that can combine the power of LLMs with external data sources and APIs. For more details on Semantic Kernel, refer to the [official Semantic Kernel documentation](https://learn.microsoft.com/en-us/semantic-kernel/).

LangWatch can capture traces generated by Semantic Kernel using OpenInference's OpenAI instrumentation. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Semantic Kernel and OpenInference instrumentor**:
    ```bash
    pip install semantic-kernel openinference-instrumentation-openai
    ```

3.  **Set up your OpenAI API key**:
    You'll need to configure your OpenAI API key in your environment.

## Instrumentation with OpenInference

LangWatch supports observability for Semantic Kernel using the [OpenInference OpenAI instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-openai). This approach captures traces from your Semantic Kernel calls and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from openinference.instrumentation.openai import OpenAIInstrumentor
import os

# Initialize LangWatch with the OpenAI instrumentor
langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create a kernel
kernel = sk.Kernel()

# Add OpenAI chat completion service
kernel.add_service(
    OpenAIChatCompletion(
        service_id="chat-gpt",
        ai_model_id="gpt-5",
        api_key=os.environ["OPENAI_API_KEY"]
    )
)

# Use the kernel as usual—traces will be sent to LangWatch automatically
async def run_semantic_kernel_example(user_input: str):
    # Create a prompt template
    prompt = """You are a helpful assistant.
    User: {{$input}}
    Assistant: Let me help you with that."""

    # Create a function from the prompt
    kernel.add_function(
        plugin_name="chat_plugin",
        prompt=prompt,
        function_name="chat",
        description="A helpful chat function"
    )

    # Invoke the function
    result = await kernel.invoke(
        function_name="chat",
        plugin_name="chat_plugin",
        input=user_input
    )
    return result

# Example usage
async def main():
    user_query = "What's the weather like in New York?"
    response = await run_semantic_kernel_example(user_query)
    print(f"Response: {response}")

if __name__ == "__main__":
    asyncio.run(main())
```

**That's it!** All Semantic Kernel calls will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from openinference.instrumentation.openai import OpenAIInstrumentor
import os

langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

kernel = sk.Kernel()
kernel.add_service(
    OpenAIChatCompletion(
        service_id="chat-gpt",
        ai_model_id="gpt-5",
        api_key=os.environ["OPENAI_API_KEY"]
    )
)

@langwatch.trace(name="Semantic Kernel Chat Function")
async def chat_with_context(user_input: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "kernel_function": "chat",
                "model": "gpt-5",
                "input_length": len(user_input)
            }
        )

    prompt = """You are a helpful assistant.
    User: {{$input}}
    Assistant: Let me help you with that."""

    kernel.add_function(
        plugin_name="chat_plugin",
        prompt=prompt,
        function_name="chat",
        description="A helpful chat function"
    )

    result = await kernel.invoke(
        function_name="chat",
        plugin_name="chat_plugin",
        input=user_input
    )
    return result
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `OpenAIInstrumentor()`: The OpenInference instrumentor automatically patches OpenAI client operations to create OpenTelemetry spans for their operations, including:
    - Chat completions
    - Model calls
    - Response parsing
    - Error handling

3.  **Semantic Kernel Integration**: The OpenAI instrumentor captures Semantic Kernel operations (function invocations, prompt processing, etc.) as spans.

4.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, Semantic Kernel operations, including function invocations, prompt processing, and model calls, will be traced and sent to LangWatch, providing visibility into your Semantic Kernel-powered applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Semantic Kernel instrumentation with other instrumentors (e.g., LangChain, DSPy) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture Semantic Kernel activity.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your Semantic Kernel code is being executed.
- Ensure you have the correct OpenAI API key set.
- Verify that your Semantic Kernel functions are properly defined and invoked.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from openinference.instrumentation.openai import OpenAIInstrumentor
import os

langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

kernel = sk.Kernel()
kernel.add_service(
    OpenAIChatCompletion(
        service_id="chat-gpt",
        ai_model_id="gpt-5",
        api_key=os.environ["OPENAI_API_KEY"]
    )
)

@langwatch.trace(name="Semantic Kernel Pipeline")
async def run_kernel_pipeline(user_input: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "pipeline_type": "semantic_kernel",
                "model": "gpt-5",
                "input_length": len(user_input)
            }
        )

    # Your Semantic Kernel code here
    prompt = """You are a helpful assistant.
    User: {{$input}}
    Assistant: Let me help you with that."""

    kernel.add_function(
        plugin_name="chat_plugin",
        prompt=prompt,
        function_name="chat",
        description="A helpful chat function"
    )

    result = await kernel.invoke(
        function_name="chat",
        plugin_name="chat_plugin",
        input=user_input
    )
    return result
```

This approach allows you to combine the tracing capabilities of Semantic Kernel with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/smolagents.mdx

---
title: SmolAgents Instrumentation
sidebarTitle: SmolAgents
description: Learn how to instrument SmolAgents applications with LangWatch.
keywords: smolagents, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

SmolAgents is a lightweight framework for building AI agents with minimal boilerplate. For more details on SmolAgents, refer to the [official SmolAgents documentation](https://github.com/smol-ai/smolagents).

LangWatch can capture traces generated by SmolAgents by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install SmolAgents and OpenInference instrumentor**:
    ```bash
    pip install smolagents openinference-instrumentation-smolagents
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for SmolAgents using the [OpenInference SmolAgents instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-smolagents). This approach automatically captures traces from your SmolAgents and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from smolagents import Agent
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
import os

# Initialize LangWatch with the SmolAgents instrumentor
langwatch.setup(
    instrumentors=[SmolagentsInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create your agent
agent = Agent(
    name="hello_agent",
    model="gpt-5",
    instruction="You are a helpful assistant. Always be friendly and concise.",
)

# Use the agent as usual—traces will be sent to LangWatch automatically
def run_agent_interaction(user_message: str):
    response = agent.run(user_message)
    return response

# Example usage
if __name__ == "__main__":
    user_prompt = "Hello! How are you today?"
    response = run_agent_interaction(user_prompt)
    print(f"User: {user_prompt}")
    print(f"Agent: {response}")
```

**That's it!** All SmolAgents activity will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from smolagents import Agent
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
import os

langwatch.setup(
    instrumentors=[SmolagentsInstrumentor()]
)

# ... agent setup code ...

@langwatch.trace(name="SmolAgents Run")
def run_agent_interaction(user_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "hello_agent",
                "model": "gpt-5"
            }
        )

    response = agent.run(user_message)
    return response
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `SmolagentsInstrumentor()`: The OpenInference instrumentor automatically patches SmolAgents components to create OpenTelemetry spans for their operations, including:
    - Agent initialization
    - Model calls
    - Tool executions
    - Response generation

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all agent interactions, model calls, and tool executions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your SmolAgents-powered applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine SmolAgents instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all SmolAgents activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from smolagents import Agent
from openinference.instrumentation.smolagents import SmolagentsInstrumentor

langwatch.setup(
    instrumentors=[SmolagentsInstrumentor()]
)

@langwatch.trace(name="Custom SmolAgents Agent")
def my_custom_agent(input_message: str):
    # Your SmolAgents code here
    agent = Agent(
        name="custom_agent",
        model="gpt-5",
        instruction="Your custom instructions",
    )

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "custom_agent",
                "model": "gpt-5"
            }
        )

    # Run your agent
    response = agent.run(input_message)

    return response
```

This approach allows you to combine the automatic tracing capabilities of SmolAgents with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/crew-ai.mdx

---
title: CrewAI
description: Learn how to instrument the CrewAI Python SDK with LangWatch.
keywords: crewai, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

LangWatch does not have a built-in auto-tracking integration for CrewAI. However, you can use community-provided instrumentors to integrate CrewAI with LangWatch.

## Community Instrumentors

There are two main community instrumentors available for CrewAI:

<CodeGroup>
<CodeGroupItem title="OpenLLMetry">
OpenLLMetry provides an OpenTelemetry-based instrumentation package for CrewAI.

You can find more details and installation instructions on their GitHub repository:
[traceloop/openllmetry/packages/opentelemetry-instrumentation-crewai](https://github.com/traceloop/openllmetry/tree/main/packages/opentelemetry-instrumentation-crewai)
</CodeGroupItem>

<CodeGroupItem title="OpenInference">
OpenInference, by Arize AI, also offers an instrumentation solution for CrewAI, compatible with OpenTelemetry.

For more information and setup guides, please visit their GitHub repository:
[Arize-ai/openinference/python/instrumentation/openinference-instrumentation-crewai](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-crewai)
</CodeGroupItem>
</CodeGroup>

To use these instrumentors with LangWatch, you would typically configure them to export telemetry data via OpenTelemetry, which LangWatch can then ingest.

## Integrating Community Instrumentors with LangWatch

Community-provided OpenTelemetry instrumentors for CrewAI, like those from OpenLLMetry or OpenInference, allow you to automatically capture detailed trace data from your CrewAI agents and tasks. LangWatch can seamlessly integrate with these instrumentors.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the CrewAI instrumentor to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

<CodeGroup>

```python openinference_setup.py
import langwatch
from crewai import Agent, Task, Crew
import os
from openinference.instrumentation.crewai import CrewAIInstrumentor # Assuming this is the correct import

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup(
    instrumentors=[CrewAIInstrumentor()]
)

# Define your CrewAI agents and tasks
researcher = Agent(
  role='Senior Researcher',
  goal='Discover new insights on AI',
  backstory='A seasoned researcher with a knack for uncovering hidden gems.'
)
writer = Agent(
  role='Expert Writer',
  goal='Craft compelling content on AI discoveries',
  backstory='A wordsmith who can make complex AI topics accessible and engaging.'
)

task1 = Task(description='Investigate the latest advancements in LLM prompting techniques.', agent=researcher)
task2 = Task(description='Write a blog post summarizing the findings.', agent=writer)

# Create and run the crew
crew = Crew(
  agents=[researcher, writer],
  tasks=[task1, task2],
  verbose=2
)

@langwatch.trace(name="CrewAI Execution with OpenInference")
def run_crewai_process_oi():
    result = crew.kickoff()
    return result

if __name__ == "__main__":
    print("Running CrewAI process with OpenInference...")
    output = run_crewai_process_oi()
    print("\n\nCrewAI Process Output:")
    print(output)
```

```python openllmetry_setup.py
import langwatch
from crewai import Agent, Task, Crew
import os
from opentelemetry_instrumentation_crewai import CrewAIInstrumentor # Assuming this is the correct import

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup(
    instrumentors=[CrewAIInstrumentor()]
)

# Define your CrewAI agents and tasks
researcher = Agent(
  role='Senior Researcher',
  goal='Discover new insights on AI',
  backstory='A seasoned researcher with a knack for uncovering hidden gems.'
)
writer = Agent(
  role='Expert Writer',
  goal='Craft compelling content on AI discoveries',
  backstory='A wordsmith who can make complex AI topics accessible and engaging.'
)

task1 = Task(description='Investigate the latest advancements in LLM prompting techniques.', agent=researcher)
task2 = Task(description='Write a blog post summarizing the findings.', agent=writer)

# Create and run the crew
crew = Crew(
  agents=[researcher, writer],
  tasks=[task1, task2],
  verbose=2
)

@langwatch.trace(name="CrewAI Execution with OpenLLMetry")
def run_crewai_process_ollm():
    result = crew.kickoff()
    return result

if __name__ == "__main__":
    print("Running CrewAI process with OpenLLMetry...")
    output = run_crewai_process_ollm()
    print("\n\nCrewAI Process Output:")
    print(output)
```

</CodeGroup>

<Note>
  Ensure you have the respective community instrumentation library installed:
  - For OpenLLMetry: `pip install opentelemetry-instrumentation-crewai`
  - For OpenInference: `pip install openinference-instrumentation-crewai`
  Consult the specific library's documentation for the exact package name and instrumentor class if the above assumptions are incorrect.
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

<CodeGroup>

```python openinference_direct.py
import langwatch
from crewai import Agent, Task, Crew
import os
from openinference.instrumentation.crewai import CrewAIInstrumentor # Assuming this is the correct import
# from opentelemetry.sdk.trace import TracerProvider # If managing your own provider
# from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter # If managing your own provider

langwatch.setup()

# Instrument CrewAI directly using OpenInference
CrewAIInstrumentor().instrument()

planner = Agent(
  role='Event Planner',
  goal='Plan an engaging tech conference',
  backstory='An experienced planner with a passion for technology events.'
)
task_planner = Task(description='Outline the agenda for a 3-day AI conference.', agent=planner)
conference_crew = Crew(agents=[planner], tasks=[task_planner])

@langwatch.trace(name="CrewAI Direct Instrumentation with OpenInference")
def plan_conference_oi():
    agenda = conference_crew.kickoff()
    return agenda

if __name__ == "__main__":
    print("Planning conference with OpenInference (direct)...")
    conference_agenda = plan_conference_oi()
    print("\n\nConference Agenda:")
    print(conference_agenda)
```

```python openllmetry_direct.py
import langwatch
from crewai import Agent, Task, Crew
import os
from opentelemetry_instrumentation_crewai import CrewAIInstrumentor # Assuming this is the correct import
# from opentelemetry.sdk.trace import TracerProvider # If managing your own provider
# from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter # If managing your own provider

langwatch.setup()

# Instrument CrewAI directly using OpenLLMetry
CrewAIInstrumentor().instrument()

planner = Agent(
  role='Event Planner',
  goal='Plan an engaging tech conference',
  backstory='An experienced planner with a passion for technology events.'
)
task_planner = Task(description='Outline the agenda for a 3-day AI conference.', agent=planner)
conference_crew = Crew(agents=[planner], tasks=[task_planner])

@langwatch.trace(name="CrewAI Direct Instrumentation with OpenLLMetry")
def plan_conference_ollm():
    agenda = conference_crew.kickoff()
    return agenda

if __name__ == "__main__":
    print("Planning conference with OpenLLMetry (direct)...")
    conference_agenda = plan_conference_ollm()
    print("\n\nConference Agenda:")
    print(conference_agenda)
```

</CodeGroup>

### Key points for community instrumentors:
-   These instrumentors typically patch CrewAI at a global level or integrate deeply with its execution flow, meaning all CrewAI operations (agents, tasks, tools) should be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup and lifecycle of the instrumentor.
-   If instrumenting directly (e.g., `CrewAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.
-   Always refer to the specific documentation of the community instrumentor (OpenLLMetry or OpenInference) for the most accurate and up-to-date installation and usage instructions, including the correct class names for instrumentors and any specific setup requirements.

---

# FILE: ./integration/python/integrations/strand-agents.mdx

---
title: Strands Agents Instrumentation
sidebarTitle: Strands Agents
description: Learn how to instrument Strands Agents applications with LangWatch.
keywords: strands agents, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

Strands Agents is a framework for building AI agents with a focus on simplicity and performance. For more details on Strands Agents, refer to the [official Strands Agents documentation](https://strandsagents.com).

LangWatch can capture traces generated by Strands Agents by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK and Strands Agents**:
    ```bash
    pip install langwatch strands-agents[otel] strands-agents-tools
    ```

2.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, AWS Bedrock, etc.) with the appropriate API keys.

## Instrumentation with OpenTelemetry

LangWatch supports seamless observability for Strands Agents using the built-in OpenTelemetry integration. This approach automatically captures traces from your Strands Agents and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
import os
from strands import Agent
from strands.telemetry import StrandsTelemetry
from strands.models.bedrock import BedrockModel

# Set up environment variables
os.environ["LANGWATCH_API_KEY"] = "your-langwatch-api-key"


# Initialize LangWatch
langwatch.setup()

# Configure the telemetry with LangWatch endpoint
strands_telemetry = StrandsTelemetry().setup_otlp_exporter(
    endpoint="https://api.langwatch.ai/api/otel/v1/traces",
    headers={"Authorization": f"Bearer {os.environ['LANGWATCH_API_KEY']}"}
)

# Configure the Bedrock model
model = BedrockModel(
    model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
)

# Define the system prompt for the agent
system_prompt = """You are "Hotel Concierge", a luxury hotel concierge helping guests with their requests.
You can assist with room service orders, spa appointments, restaurant reservations, transportation arrangements,
tour bookings, and general hotel amenities. You reply always politely and mention your name in the reply (Hotel Concierge).
NEVER skip your name in the start of a new conversation."""

# Create your agent with tracing attributes
agent = Agent(
    model=model,
    system_prompt=system_prompt,

)

# Use the agent as usual—traces will be sent to LangWatch automatically
def run_agent_interaction(user_message: str):
    response = agent(user_message)
    return response

# Example usage
if __name__ == "__main__":
    user_prompt = "Hi, I need help booking a spa appointment and arranging transportation to the airport tomorrow."
    response = run_agent_interaction(user_prompt)
    print(f"User: {user_prompt}")
    print(f"Agent: {response}")
```

**That's it!** All Strands Agents activity will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
import os
from strands import Agent
from strands.telemetry import StrandsTelemetry
from strands.models.bedrock import BedrockModel

# ... setup code ...

@langwatch.trace(name="Strands Agents Run")
def run_agent_interaction(user_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "guest_456",
                "session_id": "hotel_session_789",
                "agent_name": "hotel_concierge",
                "model": "claude-3-7-sonnet"
            },
        )

    response = agent(user_message)
    return response
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter.

2.  `StrandsTelemetry().setup_otlp_exporter()`: Configures the Strands Agents telemetry to send traces to LangWatch via OpenTelemetry protocol.

3.  **Trace Attributes**: The `trace_attributes` parameter in the Agent constructor allows you to add metadata that will be included with every trace, such as session IDs, user IDs, and custom tags.

4.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all agent interactions, model calls, and tool executions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your Strands Agents-powered applications.

## Notes

- The `strands-agents[otel]` package includes OpenTelemetry support out of the box.
- You can use different model providers (Bedrock, OpenAI, Anthropic, etc.) by importing the appropriate model class.
- The `trace_attributes` parameter allows you to add consistent metadata to all traces from a specific agent instance.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the telemetry is properly configured and that your agent code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.
- For AWS Bedrock, make sure your AWS credentials are properly configured.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
import os
from strands import Agent
from strands.telemetry import StrandsTelemetry
from strands.models.bedrock import BedrockModel

# ... setup code ...

@langwatch.trace(name="Custom Strands Agents Application")
def my_custom_strands_agents_app(input_message: str):
    # Your Strands Agents code here
    model = BedrockModel(model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0")

    agent = Agent(
        model=model,
        system_prompt="You are a luxury hotel concierge. Help guests with their requests professionally.",
        trace_attributes={
            "agent_name": "hotel_concierge",
            "model": "claude-3-7-sonnet",
            "service": "hotel_concierge",
            "environment": "production",
    },

    )

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "guest_456",
                "session_id": "hotel_session_789",
                "agent_name": "hotel_concierge",
                "model": "claude-3-7-sonnet"
            }
        )

    # Run your agent
    response = agent(input_message)

    return response
```

This approach allows you to combine the automatic tracing capabilities of Strands Agents with the rich metadata and custom attributes provided by LangWatch.

## Next Steps

Once you have instrumented your code, you can manage, evaluate and debug your application:

- **View traces** in the LangWatch dashboard
- **Add evaluation scores** to your traces
- **Create custom dashboards** for monitoring
- **Set up alerts** for performance issues
- **Export data** for further analysis

## Learn More

For more detailed information, refer to the official documentation and other examples:

- [Strands Agents Documentation](https://strandsagents.com)
- [Strands Agents GitHub Cookbook](https://github.com/strands-agents/samples/blob/main/01-getting-started/10-agent-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb)
- [LangWatch Python Integration Guide](/integration/python/guide)
---

# FILE: ./integration/python/integrations/google-genai.mdx

---
title: Google GenAI Instrumentation
sidebarTitle: Google GenAI
description: Learn how to instrument Google GenAI API calls with the LangWatch Python SDK
keywords: google genai, gemini, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Google GenAI, allowing you to capture detailed information about your Gemini API calls automatically. The recommended approach is to use OpenInference instrumentation, which provides comprehensive tracing for Google GenAI API calls and integrates seamlessly with LangWatch.

## Using OpenInference Instrumentation

The recommended approach for instrumenting Google GenAI calls with LangWatch is to use the OpenInference instrumentation library, which provides comprehensive tracing for Google GenAI API calls.

## Installation and Setup

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM providers, including Google GenAI.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `GoogleGenAIInstrumentor` from OpenInference or OpenLLMetry) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
import google.generativeai as genai
import os

# Example using OpenInference's GoogleGenAIInstrumentor
from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor

# Initialize LangWatch with the GoogleGenAIInstrumentor
langwatch.setup(
    instrumentors=[GoogleGenAIInstrumentor()]
)

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

@langwatch.trace(name="Google GenAI Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles Google GenAI calls globally.
    response = model.generate_content(prompt)
    return response.text

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```

<Note>
Ensure you have the respective community instrumentation library installed (e.g., `pip install openinference-instrumentation-google-genai` or `pip install opentelemetry-instrumentation-google-genai`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
import google.generativeai as genai
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor

langwatch.setup()

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

# Instrument Google GenAI directly using the community library
GoogleGenAIInstrumentor().instrument()

@langwatch.trace(name="Google GenAI Call with Direct Community Instrumentation")
def get_story_ending(beginning: str):
    response = model.generate_content(
        f"You are a creative writer. Complete the story: {beginning}"
    )
    return response.text

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

### Key points for community instrumentors:
- These instrumentors often patch Google GenAI at a global level, meaning all Google GenAI calls from any client instance will be captured once instrumented.
- If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
- If instrumenting directly (e.g., `GoogleGenAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

## Advanced Usage Examples

### Using Google GenAI with Function Calling

When using Google GenAI's function calling capabilities, the instrumentation will capture both the initial request and the function execution:

```python
import langwatch
import google.generativeai as genai
import os

langwatch.setup()

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

@langwatch.trace(name="Google GenAI Function Call")
def get_weather_with_functions(city: str):
    # Define the function schema
    function_declarations = [
        {
            "name": "get_weather",
            "description": "Get the current weather for a city",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "The city name"}
                },
                "required": ["city"]
            }
        }
    ]

    response = model.generate_content(
        f"What's the weather like in {city}?",
        generation_config=genai.types.GenerationConfig(
            function_calling_config=genai.types.FunctionCallingConfig(
                function_declarations=function_declarations
            )
        )
    )
    return response

if __name__ == "__main__":
    result = get_weather_with_functions("San Francisco")
    print(f"Response: {result}")
```

<Note>
### Which Approach to Choose?

- **`autotrack_google_genai_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which Google GenAI client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
- **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture Google GenAI calls globally across your application, or need to instrument other libraries alongside Google GenAI with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send Google GenAI call data to LangWatch for monitoring and analysis.
</Note>
---

# FILE: ./integration/python/integrations/pydantic-ai.mdx

---
title: PydanticAI Instrumentation
sidebarTitle: PydanticAI
description: Learn how to instrument PydanticAI applications with the LangWatch Python SDK.
keywords: pydantic-ai, pydanticai, instrumentation, opentelemetry, langwatch, python, tracing
---

PydanticAI is a library for building AI applications with Pydantic models. It features built-in, optional support for OpenTelemetry, allowing detailed tracing of agent runs and model interactions. LangWatch, being an OpenTelemetry-compatible observability platform, can seamlessly ingest these traces.

This guide explains how to configure PydanticAI and LangWatch to capture this observability data. For more background on PydanticAI's observability features, refer to their debugging and monitoring documentation.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install PydanticAI**:
    ```bash
    pip install pydantic-ai
    ```

## Instrumenting PydanticAI with LangWatch

The primary way to integrate PydanticAI with LangWatch is by leveraging PydanticAI's native OpenTelemetry emission in an environment configured by `langwatch.setup()`.

### Using PydanticAI's Built-in OpenTelemetry with LangWatch Global Setup

When `langwatch.setup()` is called, it initializes a global OpenTelemetry environment, including a trace exporter configured for LangWatch. If PydanticAI's instrumentation is enabled (via `Agent(instrument=True)` or `Agent.instrument_all()`), it will emit OpenTelemetry traces that are automatically captured by LangWatch.

```python
import langwatch
from pydantic_ai import Agent
from pydantic_ai.agent import InstrumentationSettings # Optional, for event_mode
import os
import asyncio

# 1. Initialize LangWatch
# This sets up the global OpenTelemetry environment for LangWatch.
langwatch.setup()

# 2. Enable PydanticAI Instrumentation
# Option A: Instrument all agents globally
Agent.instrument_all()

# Option B: Instrument a specific agent instance
# For this example, we'll instrument a specific agent.
# If targeting a generic OTel collector like LangWatch, event_mode='logs' is recommended
# as it aligns with OTel semantic conventions for capturing message events.
# The default mode might use a custom attribute format for events.
instrumentation_settings_for_langwatch = InstrumentationSettings(event_mode='logs')
agent = Agent(model_name='openai:gpt-5', instrument=instrumentation_settings_for_langwatch)

@langwatch.trace(name="PydanticAI - City Capital Query")
async def get_capital_city(country: str):
    langwatch.get_current_trace().update(metadata={"country_queried": country})

    try:
        # PydanticAI agent calls will now generate OpenTelemetry spans
        # that LangWatch captures under the "PydanticAI - City Capital Query" trace.
        result = await agent.run(f'What is the capital of {country}?')
        return result.output
    except Exception as e:
        if current_trace:
            current_trace.record_exception(e)
            current_trace.set_status("error", str(e))
        raise

async def main():
    try:
        capital = await get_capital_city("France")
        print(f"The capital of France is: {capital}")
    except Exception as e:
        print(f"Error getting capital of France: {e}")

    try:
        capital_error = await get_capital_city("NonExistentCountry") # Example to show error tracing
        print(f"Query for NonExistentCountry returned: {capital_error}")
    except Exception as e:
        print(f"Correctly caught error for NonExistentCountry: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

**Key points for this approach:**
-   `langwatch.setup()`: Essential for initializing the OpenTelemetry environment that LangWatch uses.
-   `Agent(instrument=True)` or `Agent.instrument_all()`: Activates PydanticAI's OpenTelemetry signal emission.
-   `InstrumentationSettings(event_mode='logs')`: As per PydanticAI documentation, using `event_mode='logs'` aligns message capture with OpenTelemetry Semantic Conventions for Generative AI, which might be better for generic OTel collectors. The default mode (`json_array`) uses a custom attribute format for events.
-   `@langwatch.trace()`: Creates a parent trace in LangWatch, under which PydanticAI's operation spans will be nested.

By following these steps, you can effectively monitor your PydanticAI applications using LangWatch, gaining insights into agent behavior, model performance, and overall application flow.

---

# FILE: ./integration/python/integrations/anthropic.mdx

---
title: Anthropic Instrumentation
sidebarTitle: Anthropic
description: Learn how to instrument Anthropic API calls with the LangWatch Python SDK
keywords: anthropic, claude, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Anthropic, allowing you to capture detailed information about your Claude API calls automatically. The recommended approach is to use OpenInference instrumentation, which provides comprehensive tracing for Anthropic API calls and integrates seamlessly with LangWatch.

## Using OpenInference Instrumentation

The recommended approach for instrumenting Anthropic calls with LangWatch is to use the OpenInference instrumentation library, which provides comprehensive tracing for Anthropic API calls.

## Installation and Setup

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM providers, including Anthropic.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `AnthropicInstrumentor` from OpenInference or OpenLLMetry) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from anthropic import Anthropic
import os

# Example using OpenInference's AnthropicInstrumentor
from openinference.instrumentation.anthropic import AnthropicInstrumentor

# Initialize LangWatch with the AnthropicInstrumentor
langwatch.setup(
    instrumentors=[AnthropicInstrumentor()]
)

client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

@langwatch.trace(name="Anthropic Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles Anthropic calls globally.
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```

<Note>
Ensure you have the respective community instrumentation library installed (e.g., `pip install openinference-instrumentation-anthropic` or `pip install opentelemetry-instrumentation-anthropic`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from anthropic import Anthropic
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.anthropic import AnthropicInstrumentor

langwatch.setup()
client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Instrument Anthropic directly using the community library
AnthropicInstrumentor().instrument()

@langwatch.trace(name="Anthropic Call with Direct Community Instrumentation")
def get_story_ending(beginning: str):
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[
            {"role": "system", "content": "You are a creative writer. Complete the story."},
            {"role": "user", "content": beginning}
        ]
    )
    return response.content[0].text

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

### Key points for community instrumentors:
- These instrumentors often patch Anthropic at a global level, meaning all Anthropic calls from any client instance will be captured once instrumented.
- If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
- If instrumenting directly (e.g., `AnthropicInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

## Advanced Usage Examples

### Using Anthropic with Tools

When using Anthropic's tool calling capabilities, the instrumentation will capture both the initial request and the tool execution:

```python
import langwatch
from anthropic import Anthropic
import os

langwatch.setup()
client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

@langwatch.trace(name="Anthropic Tool Call")
def get_weather_with_tools(city: str):
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        tools=[{
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get the current weather for a city",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {"type": "string", "description": "The city name"}
                    },
                    "required": ["city"]
                }
            }
        }],
        messages=[{"role": "user", "content": f"What's the weather like in {city}?"}]
    )
    return response

if __name__ == "__main__":
    result = get_weather_with_tools("San Francisco")
    print(f"Response: {result}")
```

### Streaming Responses

For streaming responses, the instrumentation captures the entire streaming session:

```python
import langwatch
from anthropic import Anthropic
import os

langwatch.setup()
client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

@langwatch.trace(name="Anthropic Streaming")
def stream_response(prompt: str):
    with client.messages.stream(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    ) as stream:
        for text in stream.text_stream:
            print(text, end="", flush=True)
        print()  # New line after streaming

if __name__ == "__main__":
    stream_response("Write a short story about a robot learning to paint.")
```

<Note>
### Which Approach to Choose?

- **`autotrack_anthropic_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which Anthropic client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
- **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture Anthropic calls globally across your application, or need to instrument other libraries alongside Anthropic with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send Anthropic call data to LangWatch for monitoring and analysis.
</Note>
---

# FILE: ./integration/python/integrations/langgraph.mdx

---
title: LangGraph Instrumentation
sidebarTitle: LangGraph
description: Learn how to instrument LangGraph applications with the LangWatch Python SDK.
keywords: langgraph, instrumentation, callback, opentelemetry, langwatch, python, tracing, openinference, openllmetry
---

LangGraph is a powerful framework for building LLM applications. LangWatch integrates with LangGraph to provide detailed observability into your chains, agents, LLM calls, and tool usage.

<Note>
The community instrumentors below are for LangChain, but LangGraph is compatible with them.
</Note>

## 2. Using Community OpenTelemetry Instrumentors

Dedicated LangChain instrumentors from libraries like OpenInference and OpenLLMetry can also be used to capture LangGraph operations as OpenTelemetry traces, which LangWatch can then ingest.

### Instrumenting LangGraph with Dedicated Instrumentors

#### i. Via `langwatch.setup()`

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangchainInstrumentor
import os
import asyncio

langwatch.setup(
    instrumentors=[LangchainInstrumentor()] # Add OpenInference LangchainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference via Setup")
async def handle_message_oinference_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (setup) example.")
        return
    response = await handle_message_oinference_setup("Explain Langchain instrumentation with OpenInference.")
    print(f"AI (OInference Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_setup())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangchainInstrumentor # Corrected import
import os
import asyncio

langwatch.setup(
    instrumentors=[LangchainInstrumentor()] # Add OpenLLMetry LangchainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry via Setup")
async def handle_message_openllmetry_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (setup) example.")
        return
    response = await handle_message_openllmetry_setup("Explain Langchain instrumentation with OpenLLMetry.")
    print(f"AI (OpenLLMetry Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_setup())
```
</CodeGroup>

#### ii. Direct Instrumentation

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangchainInstrumentor
import os
import asyncio

langwatch.setup()
LangchainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference Direct")
async def handle_message_oinference_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (direct) example.")
        return
    response = await handle_message_oinference_direct("How does direct Langchain instrumentation work?")
    print(f"AI (OInference Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_direct())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangchainInstrumentor # Corrected import
import os
import asyncio

langwatch.setup()
LangchainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry Direct")
async def handle_message_openllmetry_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (direct) example.")
        return
    response = await handle_message_openllmetry_direct("How does direct Langchain instrumentation work with OpenLLMetry?")
    print(f"AI (OpenLLMetry Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_direct())
```
</CodeGroup>

---

# FILE: ./integration/python/integrations/open-ai-agents.mdx

---
title: OpenAI Agents SDK Instrumentation
sidebarTitle: OpenAI Agents
description: Learn how to instrument OpenAI Agents with the LangWatch Python SDK
keywords: openai-agents, instrumentation, openinference, LangWatch, Python, tracing
---

LangWatch allows you to monitor your OpenAI Agents by integrating with their tracing capabilities. Since OpenAI Agents manage their own execution flow, including LLM calls and tool usage, the direct `autotrack_openai_calls()` method used for the standard OpenAI client is not applicable here.

Instead, you can integrate LangWatch in one of two ways:

1.  **Using OpenInference Instrumentation (Recommended)**: Leverage the `openinference-instrumentation-openai-agents` library, which provides OpenTelemetry-based instrumentation for OpenAI Agents. This is generally the simplest and most straightforward method.
2.  **Alternative: Using OpenAI Agents' Built-in Tracing with a Custom Processor**: If you choose not to use OpenInference or have highly specific requirements, you can adapt the built-in tracing mechanism of the `openai-agents` SDK to forward trace data to LangWatch by implementing your own custom `TracingProcessor`.

This guide will walk you through both methods.

## 1. Using OpenInference Instrumentation for OpenAI Agents (Recommended)

The most straightforward way to integrate LangWatch with OpenAI Agents is by using the OpenInference instrumentation library specifically designed for it: `openinference-instrumentation-openai-agents`. This library is currently in an Alpha stage, so while ready for experimentation, it may undergo breaking changes.

This approach uses OpenTelemetry-based instrumentation and is generally recommended for ease of setup.

### Installation

First, ensure you have the necessary packages installed:

```bash
pip install langwatch openai-agents openinference-instrumentation-openai-agents
```

### Integration via `langwatch.setup()`

You can pass an instance of the `OpenAIAgentsInstrumentor` from `openinference-instrumentation-openai-agents` to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from openai_agents.agents import Agent, Runner # Using openai_agents SDK
from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor
import os
import asyncio

# Ensure LANGWATCH_API_KEY is set in your environment, or pass it to setup
# e.g., langwatch.setup(api_key="your_api_key", instrumentors=[OpenAIAgentsInstrumentor()])
# If LANGWATCH_API_KEY is in env, this is sufficient:
langwatch.setup(
    instrumentors=[OpenAIAgentsInstrumentor()]
)

# Initialize your agent
agent = Agent(name="ExampleAgent", instructions="You are a helpful assistant.")

@langwatch.trace(name="OpenAI Agent Run with OpenInference")
async def run_agent_with_openinference(prompt: str):
    # The OpenAIAgentsInstrumentor will automatically capture agent activities.
    result = await Runner.run(agent, prompt)
    return result.final_output

async def main():
    user_query = "Tell me a fun fact."
    response = await run_agent_with_openinference(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    asyncio.run(main())
```

<Note>
  The `OpenAIAgentsInstrumentor` is part of the `openinference-instrumentation-openai-agents` package. Always refer to its [official documentation](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-openai-agents) for the latest updates, especially as it's in Alpha.
</Note>

### Direct Instrumentation

Alternatively, if you manage your OpenTelemetry `TracerProvider` more directly (e.g., if LangWatch is configured to use an existing global provider), you can use the instrumentor's `instrument()` method. LangWatch will pick up the spans if its exporter is part of the active `TracerProvider`.

```python
import langwatch
from openai_agents.agents import Agent, Runner
from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor
import os
import asyncio

# Initialize LangWatch (it will set up its OTel exporter)
langwatch.setup() # Ensure API key is available via env or parameters

# Instrument OpenAI Agents directly
OpenAIAgentsInstrumentor().instrument()

agent = Agent(name="ExampleAgentDirect", instructions="You are a helpful assistant.")

@langwatch.trace(name="OpenAI Agent Run with Direct OpenInference")
async def run_agent_direct_instrumentation(prompt: str):
    result = await Runner.run(agent, prompt)
    return result.final_output

async def main():
    user_query = "What's the weather like?"
    response = await run_agent_direct_instrumentation(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    asyncio.run(main())
```

Key points for OpenInference instrumentation:
-   It patches `openai-agents` activities globally once instrumented.
-   Ensure `langwatch.setup()` is called so LangWatch's OpenTelemetry exporter is active and configured.
-   The `@langwatch.trace()` decorator on your calling function helps create a parent span under which the agent's detailed operations will be nested.

## 2. Alternative: Using OpenAI Agents' Built-in Tracing with a Custom Processor

If you prefer not to use the OpenInference instrumentor, or if you have highly specific tracing requirements not met by it, you can leverage the `openai-agents` SDK's own [built-in tracing system](https://openai.github.io/openai-agents-python/tracing/).

This involves creating a custom `TracingProcessor` that intercepts trace data from the `openai-agents` SDK and then uses the standard OpenTelemetry Python API to create OpenTelemetry spans. LangWatch will then ingest these OpenTelemetry spans, provided `langwatch.setup()` has been called.

**Conceptual Outline for Your Custom Processor:**

1.  **Initialize LangWatch**: Ensure `langwatch.setup()` is called in your application. This sets up LangWatch to receive OpenTelemetry data.
2.  **Implement Your Custom `TracingProcessor`**:
    -   Following the `openai-agents` SDK documentation, create a class that implements their `TracingProcessor` interface (see their docs on [Custom Tracing Processors](https://openai.github.io/openai-agents-python/tracing/#custom-tracing-processors) and the API reference for [`TracingProcessor`](https://openai.github.io/openai-agents-python/api_reference/tracing/processor_interface/)).
    -   In your processor's methods (e.g., `on_span_start`, `on_span_end`), you will receive `Trace` and `Span` objects from the `openai-agents` SDK.
    -   You will then use the `opentelemetry-api` and `opentelemetry-sdk` (e.g., `opentelemetry.trace.get_tracer(__name__).start_span()`) to translate this information into OpenTelemetry spans, including their names, attributes, timings, and status. Consult the `openai-agents` documentation on [Traces and spans](https://openai.github.io/openai-agents-python/tracing/#traces-and-spans) for details on their data structures.
3.  **Register Your Custom Processor**: Use `openai_agents.tracing.add_trace_processor(your_custom_processor)` or `openai_agents.tracing.set_trace_processors([your_custom_processor])` as per the `openai-agents` SDK documentation.

**Implementation Guidance:**

LangWatch does not provide a pre-built custom `TracingProcessor` for this purpose. The implementation of such a processor is your responsibility and should be based on the official `openai-agents` SDK documentation. This ensures your processor correctly interprets the agent's trace data and remains compatible with `openai-agents` SDK updates.

-   Key `openai-agents` documentation:
    -   [OpenAI Agents Tracing Documentation](https://openai.github.io/openai-agents-python/tracing/)
    -   [API Reference for Tracing](https://openai.github.io/openai-agents-python/api_reference/tracing/)

<Warning>
Implementing a custom `TracingProcessor` is an advanced task that requires:
- A thorough understanding of both the `openai-agents` tracing internals and OpenTelemetry concepts and semantic conventions.
- Careful mapping of `openai-agents` `SpanData` types to OpenTelemetry attributes.
- Robust handling of span parenting, context propagation, and error states.
- Diligent maintenance to keep your processor aligned with any changes in the `openai-agents` SDK.
This approach offers maximum flexibility but comes with significant development and maintenance overhead.
</Warning>

## Which Approach to Choose?

-   **OpenInference Instrumentation (Recommended)**:
    -   **Pros**: Significantly simpler to set up and maintain. Relies on a community-supported library (`openinference-instrumentation-openai-agents`) designed for OpenTelemetry integration. Aligns well with standard OpenTelemetry practices.
    -   **Cons**: As the `openinference-instrumentation-openai-agents` library is in Alpha, it may have breaking changes. You have less direct control over the exact span data compared to a fully custom processor.

-   **Custom `TracingProcessor` (Alternative for advanced needs)**:
    -   **Pros**: Offers complete control over the transformation of trace data from `openai-agents` to OpenTelemetry. Allows for highly customized span data and behaviors.
    -   **Cons**: Far more complex to implement correctly and maintain. Requires deep expertise in both `openai-agents` tracing and OpenTelemetry. You are responsible for adapting your processor to any changes in the `openai-agents` SDK.

For most users, the **OpenInference instrumentation is the recommended path** due to its simplicity and lower maintenance burden.

The **custom `TracingProcessor`** approach should generally be reserved for situations where the OpenInference instrumentor is unsuitable, or when you have highly specialized tracing requirements that demand direct manipulation of the agent's trace data before converting it to OpenTelemetry spans.

---
Always refer to the latest documentation for `langwatch`, `openai-agents`, and `openinference-instrumentation-openai-agents` for the most up-to-date instructions and API details.

---

# FILE: ./integration/python/integrations/lite-llm.mdx

---
title: LiteLLM Instrumentation
sidebarTitle: LiteLLM
description: Learn how to instrument LiteLLM calls with the LangWatch Python SDK.
keywords: litellm, instrumentation, autotrack, opentelemetry, langwatch, python, tracing, openinference, openllmetry
---

LiteLLM provides a unified interface to various Large Language Models. LangWatch integrates with LiteLLM by capturing OpenTelemetry traces, enabling detailed observability into your LLM calls made through LiteLLM.

This guide outlines three primary approaches for instrumenting LiteLLM with LangWatch:

1.  **Using `autotrack_litellm_calls()`**: This method, part of the LangWatch SDK, dynamically patches your LiteLLM module instance for the current trace to capture its calls.
2.  **Using LiteLLM's Native OpenTelemetry Tracing with Global Setup**: LiteLLM can automatically generate OpenTelemetry traces for its operations when a global OpenTelemetry environment (established by `langwatch.setup()`) is active.
3.  **Using Community OpenTelemetry Instrumentors (for Underlying SDKs)**: If LiteLLM internally uses other instrumented SDKs (like the `openai` SDK for OpenAI models), you can leverage community instrumentors for those specific underlying SDKs.

## Using `autotrack_litellm_calls()`

The `autotrack_litellm_calls()` function, called on a trace object, provides a straightforward way to capture all LiteLLM calls for the duration of the current trace. This is often the most direct way to ensure LiteLLM operations are captured by LangWatch within a specific traced function.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`.

```python
import langwatch
import litellm
import os
import asyncio
from typing import cast
from litellm import CustomStreamWrapper # For streaming example
from litellm.types.utils import StreamingChoices # For streaming example

langwatch.setup()

@langwatch.trace(name="LiteLLM Autotrack Example")
async def get_litellm_response_autotrack(user_prompt: str):
    # Get the current trace and enable autotracking for the litellm module
    langwatch.get_current_trace().autotrack_litellm_calls(litellm) # Pass the imported litellm module

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_prompt}
    ]

    # This call will now be automatically captured as a span by LangWatch
    response = await litellm.acompletion(
        model="groq/llama3-70b-8192",
        messages=messages,
    )
    return response.choices[0].message.content

async def main():
    reply = await get_litellm_response_autotrack("Tell me about LiteLLM.")
    print("AI Response:", reply)

if __name__ == "__main__":
    asyncio.run(main())
```

**Key points for `autotrack_litellm_calls()`:**
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments the passed `litellm` module instance specifically for the current trace.

## Using Community OpenTelemetry Instrumentors

If you are already using a dedicated community instrumentor for LiteLLM, such as the one provided by OpenInference, you can pass an instance of `LiteLLMInstrumentor` from `openinference.instrumentation.litellm` to the `instrumentors` list in `langwatch.setup()`.

### 1. Via `langwatch.setup()`

You can pass an instance of `LiteLLMInstrumentor` from `openinference.instrumentation.litellm` to the `instrumentors` list in `langwatch.setup()`.

```python
import langwatch
import litellm
import os
import asyncio

# Example using OpenInference's LiteLLMInstrumentor
from openinference.instrumentation.litellm import LiteLLMInstrumentor

# 1. Initialize LangWatch with the LiteLLMInstrumentor
langwatch.setup(
    instrumentors=[LiteLLMInstrumentor()] # Instruments LiteLLM directly
)

@langwatch.trace(name="LiteLLM via OpenInference Instrumentor Setup")
async def get_response_via_litellm_instrumentor_setup(user_prompt: str):
    messages = [
        {"role": "user", "content": user_prompt}
    ]

    # This LiteLLM call will be captured by the LiteLLMInstrumentor
    response = await litellm.acompletion(
        model="groq/llama3-70b-8192", # Example model
        messages=messages
    )
    return response.choices[0].message.content

async def main_community_litellm_instrumentor_setup():
    reply = await get_response_via_litellm_instrumentor_setup("Explain OpenInference for LiteLLM.")
    print(f"AI Response (OpenInference via setup): {reply}")

if __name__ == "__main__":
    asyncio.run(main_community_litellm_instrumentor_setup())
```

<Note>
  Ensure you have the `openinference-instrumentation-litellm` library installed.
</Note>

### 2. Direct Instrumentation with `LiteLLMInstrumentor`

If you are managing your OpenTelemetry setup more directly, you can call `instrument()` on an instance of `LiteLLMInstrumentor`.

```python
import langwatch
import litellm
import os
import asyncio

from openinference.instrumentation.litellm import LiteLLMInstrumentor

# 1. Initialize LangWatch (sets up global OTel provider for LangWatch exporter)
langwatch.setup()

# 2. Instrument LiteLLM directly using its OpenInference instrumentor
# This should be done once, early in your application lifecycle.
LiteLLMInstrumentor().instrument()

@langwatch.trace(name="LiteLLM via Directly Instrumented OpenInference")
async def get_response_direct_litellm_instrumentation(user_prompt: str):
    messages = [
        {"role": "user", "content": user_prompt}
    ]
    response = await litellm.acompletion(model="groq/llama3-70b-8192", messages=messages)
    return response.choices[0].message.content

async def main_direct_litellm_instrumentation():
    reply = await get_response_direct_litellm_instrumentation("How does direct OTel instrumentation work for LiteLLM?")
    print(f"AI Response (direct OpenInference): {reply}")

if __name__ == "__main__":
    asyncio.run(main_direct_litellm_instrumentation())

```

**Key points for using OpenInference `LiteLLMInstrumentor`:**
-   This instrumentor specifically targets LiteLLM calls.
-   It provides an alternative to `autotrack_litellm_calls` if you prefer an explicit instrumentor pattern or are using OpenInference across your stack.

<Note>
### Which Approach to Choose?

-   **`autotrack_litellm_calls()`**: Best for explicit, trace-specific instrumentation of LiteLLM. Offers clear control over when LiteLLM calls are tracked by LangWatch within a given trace.
-   **OpenInference `LiteLLMInstrumentor`**: Use if you are standardizing on OpenInference instrumentors or prefer this explicit way of instrumenting LiteLLM itself (rather than its underlying SDKs). It provides traces directly from LiteLLM's perspective.

Choose the method that best fits your instrumentation strategy and the level of detail required.
</Note>

---

# FILE: ./integration/python/integrations/other.mdx

---
title: Other OpenTelemetry Instrumentors
sidebarTitle: Other
description: Learn how to use any OpenTelemetry-compatible instrumentor with LangWatch.
keywords: opentelemetry, instrumentation, custom, other, generic, BaseInstrumentor, LangWatch, Python
---

LangWatch is designed to be compatible with the broader OpenTelemetry ecosystem. Beyond the specifically documented integrations, you can use LangWatch with any Python library that has an OpenTelemetry instrumentor, provided that the instrumentor adheres to the standard OpenTelemetry Python `BaseInstrumentor` interface.

## Using Custom/Third-Party OpenTelemetry Instrumentors

If you have a specific library you want to trace, and there's an OpenTelemetry instrumentor available for it (either a community-provided one not yet listed in our specific integrations, or one you've developed yourself), you can integrate it with LangWatch.

The key is that the instrumentor should be an instance of a class that inherits from `opentelemetry.instrumentation.instrumentor.BaseInstrumentor`. You can find the official documentation for this base class here:

- [OpenTelemetry BaseInstrumentor Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/base/instrumentor.html#opentelemetry.instrumentation.instrumentor.BaseInstrumentor)

### Integration via `langwatch.setup()`

To use such an instrumentor, you simply pass an instance of it to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage its lifecycle (calling its `instrument()` and `uninstrument()` methods appropriately).

Here's a conceptual example using the OpenTelemetry `LoggingInstrumentor`:

```python
import langwatch
import os
import logging # Standard Python logging

# Import an off-the-shelf OpenTelemetry instrumentor
# Ensure you have this package installed: pip install opentelemetry-instrumentation-logging
from opentelemetry.instrumentation.logging import LoggingInstrumentor

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup(
    instrumentors=[
        LoggingInstrumentor() # Pass an instance of the instrumentor
    ]
)

# Configure standard Python logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# You might want to add a handler if you also want to see logs in the console
# handler = logging.StreamHandler()
# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# handler.setFormatter(formatter)
# logger.addHandler(handler)

@langwatch.trace(name="Task with Instrumented Logging")
def perform_task_with_logging():
    logger.info("Starting the task.")
    # ... some work ...
    logger.warning("Something to be aware of happened during the task.")
    # ... more work ...
    logger.info("Task completed.")
    return "Task finished successfully"

if __name__ == "__main__":
    print("Running example with LoggingInstrumentor...")
    result = perform_task_with_logging()
    print(f"Result: {result}")
    # Spans for the log messages (e.g., logger.info, logger.warning)
    # would be generated by LoggingInstrumentor and captured by LangWatch.
```

When this code runs, the `LoggingInstrumentor` (managed by `langwatch.setup()`) will automatically create OpenTelemetry spans for any log messages emitted by the standard Python `logging` module. LangWatch will then capture these spans.

## Discovering More Community Instrumentors

Many Python libraries, especially in the AI/ML space, are instrumented by community-driven OpenTelemetry projects. If you're looking for pre-built instrumentors, these are excellent places to start:

*   **OpenInference (by Arize AI):** [https://github.com/Arize-ai/openinference](https://github.com/Arize-ai/openinference)
    *   This project provides instrumentors for a wide range of AI/ML libraries and frameworks. Examples include:
        *   OpenAI
        *   Anthropic
        *   LiteLLM
        *   Haystack
        *   LlamaIndex
        *   LangChain
        *   Groq
        *   Google Gemini
        *   And more (check their repository for the full list).

*   **OpenLLMetry (by Traceloop):** [https://github.com/traceloop/openllmetry](https://github.com/traceloop/openllmetry)
    *   This project also offers a comprehensive suite of instrumentors for LLM applications and related tools. Examples include:
        *   OpenAI
        *   CrewAI
        *   Haystack
        *   LangChain
        *   LlamaIndex
        *   Pinecone
        *   ChromaDB
        *   And more (explore their repository for details).

You can browse these repositories to find instrumentors for other libraries you might be using. If an instrumentor from these projects (or any other source) adheres to the `BaseInstrumentor` interface, you can integrate it with LangWatch using the `langwatch.setup(instrumentors=[...])` method described above.

### Key Considerations:

1.  **`BaseInstrumentor` Compliance:** Ensure the instrumentor correctly implements the `BaseInstrumentor` interface, particularly the `instrument()` and `uninstrument()` methods, and `instrumentation_dependencies()`.
2.  **Installation:** You'll need to have the custom instrumentor package installed in your Python environment, along with the library it instruments.
3.  **TracerProvider:** LangWatch configures an OpenTelemetry `TracerProvider`. The instrumentor, when activated by LangWatch, will use this provider to create spans. If you are managing your OpenTelemetry setup more directly (e.g., providing your own `TracerProvider` to `langwatch.setup()`), the instrumentor will use that instead.
4.  **Data Quality:** The quality and detail of the telemetry data captured will depend on how well the custom instrumentor is written.

By leveraging the `BaseInstrumentor` interface, LangWatch remains flexible and extensible, allowing you to bring telemetry from a wide array of Python libraries into your observability dashboard.

---

# FILE: ./integration/python/integrations/azure-ai.mdx

---
title: Azure AI Inference SDK Instrumentation
sidebarTitle: Python
description: Learn how to instrument the Azure AI Inference Python SDK with LangWatch.
keywords: azure ai inference, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

The `azure-ai-inference` Python SDK provides a unified way to interact with various AI models deployed on Azure, including those on Azure OpenAI Service, GitHub Models, and Azure AI Foundry Serverless/Managed Compute endpoints. For more details on the SDK, refer to the [official Azure AI Inference client library documentation](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-inference-readme?view=azure-python-preview).

LangWatch can capture traces generated by the `azure-ai-inference` SDK by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Azure AI Inference SDK with OpenTelemetry support**:
    The `azure-ai-inference` SDK can be installed with OpenTelemetry capabilities. You might also need the core Azure OpenTelemetry tracing package.
    ```bash
    pip install azure-ai-inference[opentelemetry] azure-core-tracing-opentelemetry
    ```
    Refer to the [Azure SDK documentation](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-inference-readme?view=azure-python-preview#install-the-package) for the most up-to-date installation instructions.

## Instrumentation with `AIInferenceInstrumentor`

The `azure-ai-inference` SDK provides an `AIInferenceInstrumentor` that automatically captures traces for its operations when enabled. LangWatch, when set up, will include an OpenTelemetry exporter that can collect these traces.

Here's how to instrument your application:

```python
import langwatch
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.tracing import AIInferenceInstrumentor
from azure.core.credentials import AzureKeyCredential
import os
import asyncio

# 1. Initialize LangWatch
langwatch.setup(
    instrumentors=[AIInferenceInstrumentor()]
)

# 2. Configure your Azure AI Inference client
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
azure_openai_api_key = os.getenv("AZURE_OPENAI_API_KEY")
azure_openai_api_version = "2024-06-01"

chat_client = ChatCompletionsClient(
    endpoint=azure_openai_endpoint,
    credential=AzureKeyCredential(azure_openai_api_key),
    api_version=azure_openai_api_version
)

@langwatch.trace(name="Azure AI Inference Chat")
async def get_ai_response(prompt: str):
    # This call will now be automatically traced by the AIInferenceInstrumentor and
    # captured by LangWatch as a span within the "Azure AI Inference Chat" trace.
    response = await chat_client.complete(
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

async def main():
    user_prompt = "What is the Azure AI Inference SDK?"

    try:
        ai_reply = await get_ai_response(user_prompt)
        print(f"User: {user_prompt}")
        print(f"AI: {ai_reply}")
    except Exception as e:
        print(f"An error occurred: {e}")


if __name__ == "__main__":
    asyncio.run(main())

```

<Note>
The example uses the synchronous `ChatCompletionsClient` for simplicity in demonstrating instrumentation. The `azure-ai-inference` SDK also provides asynchronous clients under the `azure.ai.inference.aio` namespace (e.g., `azure.ai.inference.aio.ChatCompletionsClient`). If you are using `async/await` in your application, you should use these asynchronous clients. The `AIInferenceInstrumentor` will work with both synchronous and asynchronous clients.
</Note>

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.
2.  `AIInferenceInstrumentor().instrument()`: This command, provided by the `azure-ai-inference` SDK, patches the relevant Azure AI clients (like `ChatCompletionsClient` or `EmbeddingsClient`) to automatically create OpenTelemetry spans for their operations (e.g., a `complete` or `embed` call).
3.  `@langwatch.trace()`: By decorating your own functions (like `get_ai_response` in the example), you create a parent trace in LangWatch. The spans automatically generated by the `AIInferenceInstrumentor` for calls made within this decorated function will then be nested under this parent trace. This provides a full end-to-end view of your operation.

With this setup, calls made using the `azure-ai-inference` clients will be automatically traced and sent to LangWatch, providing visibility into the performance and behavior of your AI model interactions.

---

# FILE: ./integration/python/integrations/vertex-ai.mdx

---
title: Google Vertex AI Instrumentation
sidebarTitle: Vertex AI
description: Learn how to instrument Google Vertex AI API calls with the LangWatch Python SDK using OpenInference
keywords: google vertex ai, gemini, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Google Vertex AI, allowing you to capture detailed information about your Vertex AI API calls automatically. The recommended approach is to use OpenInference instrumentation, which provides comprehensive tracing for Google Vertex AI API calls and integrates seamlessly with LangWatch.

## Using OpenInference Instrumentation

The recommended approach for instrumenting Google Vertex AI calls with LangWatch is to use the [OpenInference instrumentation library](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-vertexai), which provides comprehensive tracing for Google Vertex AI API calls.

### What OpenInference Captures

The OpenInference Vertex AI instrumentation automatically captures:

- **LLM Calls**: All text generation, chat completion, and embedding requests
- **Model Information**: Model name, version, and configuration parameters
- **Input/Output**: Prompts, responses, and token usage
- **Performance Metrics**: Latency, token counts, and cost information
- **Error Handling**: Failed requests and error details
- **Context Information**: Session IDs, user IDs, and custom metadata

## Installation and Setup

### Prerequisites

1. **Install the OpenInference Vertex AI instrumentor**:
   ```bash
   pip install openinference-instrumentation-vertexai
   ```

2. **Install LangWatch SDK**:
   ```bash
   pip install langwatch
   ```

3. **Set up your Google Cloud credentials**:
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/service-account-key.json"
   export GOOGLE_CLOUD_PROJECT="your-project-id"
   export GOOGLE_CLOUD_LOCATION="us-central1"
   ```

### Basic Setup

There are two main ways to integrate OpenInference Vertex AI instrumentation with LangWatch:

#### 1. Via `langwatch.setup()` (Recommended)

You can pass an instance of the `VertexAIInstrumentor` to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from vertexai.language_models import TextGenerationModel
import os

# Example using OpenInference's VertexAIInstrumentor
from openinference.instrumentation.vertexai import VertexAIInstrumentor

# Initialize LangWatch with the VertexAIInstrumentor
langwatch.setup(
    instrumentors=[VertexAIInstrumentor()]
)

# Initialize Vertex AI
from vertexai import init
init(project=os.getenv("GOOGLE_CLOUD_PROJECT"), location=os.getenv("GOOGLE_CLOUD_LOCATION"))

model = TextGenerationModel.from_pretrained("text-bison@001")

@langwatch.trace(name="Vertex AI Call with OpenInference")
def generate_text_with_openinference(prompt: str):
    # No need to call autotrack explicitly, the OpenInference instrumentor handles Vertex AI calls globally.
    response = model.predict(prompt)
    return response.text

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_openinference(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```

#### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application, you can use the instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from vertexai.language_models import TextGenerationModel
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.vertexai import VertexAIInstrumentor

langwatch.setup()

# Initialize Vertex AI
from vertexai import init
init(project=os.getenv("GOOGLE_CLOUD_PROJECT"), location=os.getenv("GOOGLE_CLOUD_LOCATION"))

model = TextGenerationModel.from_pretrained("text-bison@001")

# Instrument Vertex AI directly using the OpenInference library
VertexAIInstrumentor().instrument()

@langwatch.trace(name="Vertex AI Call with Direct OpenInference Instrumentation")
def get_story_ending(beginning: str):
    response = model.predict(
        f"You are a creative writer. Complete the story: {beginning}"
    )
    return response.text

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

<Note>
### Which Approach to Choose?

- **OpenInference Instrumentation** is recommended for most use cases as it provides comprehensive, automatic instrumentation with minimal setup
- **Direct OpenTelemetry Setup** is useful when you need fine-grained control over the tracing configuration or are already using OpenTelemetry extensively

Both approaches effectively send Vertex AI call data to LangWatch for monitoring and analysis.
</Note>
---

# FILE: ./integration/python/integrations/autogen.mdx

---
title: AutoGen Instrumentation
sidebarTitle: AutoGen
description: Learn how to instrument AutoGen applications with LangWatch.
keywords: autogen, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

AutoGen is a framework for building multi-agent systems with conversational AI. For more details on AutoGen, refer to the [official AutoGen documentation](https://microsoft.github.io/autogen/).

LangWatch can capture traces generated by AutoGen by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install AutoGen and OpenInference instrumentor**:
    ```bash
    pip install pyautogen openinference-instrumentation-autogen
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for AutoGen using the [OpenInference AutoGen instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-autogen). This approach automatically captures traces from your AutoGen agents and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
import autogen
from openinference.instrumentation.autogen import AutoGenInstrumentor
import os

# Initialize LangWatch with the AutoGen instrumentor
langwatch.setup(
    instrumentors=[AutoGenInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Configure your agents
config_list = [
    {
        "model": "gpt-5",
        "api_key": os.environ["OPENAI_API_KEY"],
    }
]

# Create your agents
assistant = autogen.AssistantAgent(
    name="assistant",
    llm_config={"config_list": config_list},
    system_message="You are a helpful AI assistant."
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={"work_dir": "workspace"},
    llm_config={"config_list": config_list},
)

# Use the agents as usual—traces will be sent to LangWatch automatically
def run_agent_conversation(user_message: str):
    user_proxy.initiate_chat(
        assistant,
        message=user_message
    )
    return "Conversation completed"

# Example usage
if __name__ == "__main__":
    user_prompt = "Write a Python function to calculate fibonacci numbers"
    result = run_agent_conversation(user_prompt)
    print(f"Result: {result}")
```

**That's it!** All AutoGen agent interactions will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
import autogen
from openinference.instrumentation.autogen import AutoGenInstrumentor
import os

langwatch.setup(
    instrumentors=[AutoGenInstrumentor()]
)

# ... agent setup code ...

@langwatch.trace(name="AutoGen Multi-Agent Conversation")
def run_agent_conversation(user_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_count": 2,
                "model": "gpt-5"
            }
        )

    user_proxy.initiate_chat(
        assistant,
        message=user_message
    )
    return "Conversation completed"
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `AutoGenInstrumentor()`: The OpenInference instrumentor automatically patches AutoGen components to create OpenTelemetry spans for their operations, including:
    - Agent initialization
    - Multi-agent conversations
    - LLM calls
    - Tool executions
    - Code execution
    - Message passing between agents

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all agent interactions, conversations, model calls, and tool executions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your AutoGen-powered applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine AutoGen instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all AutoGen activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
import autogen
from openinference.instrumentation.autogen import AutoGenInstrumentor

langwatch.setup(
    instrumentors=[AutoGenInstrumentor()]
)

@langwatch.trace(name="Custom AutoGen Application")
def my_custom_autogen_app(input_message: str):
    # Your AutoGen code here
    config_list = [
        {
            "model": "gpt-5",
            "api_key": os.environ["OPENAI_API_KEY"],
        }
    ]

    assistant = autogen.AssistantAgent(
        name="assistant",
        llm_config={"config_list": config_list},
        system_message="You are a helpful AI assistant."
    )

    user_proxy = autogen.UserProxyAgent(
        name="user_proxy",
        human_input_mode="NEVER",
        max_consecutive_auto_reply=10,
        llm_config={"config_list": config_list},
    )

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_count": 2,
                "model": "gpt-5"
            }
        )

    # Run your agents
    user_proxy.initiate_chat(
        assistant,
        message=input_message
    )

    return "Conversation completed"
```

This approach allows you to combine the automatic tracing capabilities of AutoGen with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/open-ai.mdx

---
title: OpenAI Instrumentation
sidebarTitle: Python
description: Learn how to instrument OpenAI API calls with the LangWatch Python SDK
keywords: openai, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with OpenAI, allowing you to capture detailed information about your LLM calls automatically. There are two primary approaches to instrumenting your OpenAI interactions:

1.  **Using `autotrack_openai_calls()`**: This method, part of the LangWatch SDK, dynamically patches your OpenAI client instance to capture calls made through it within a specific trace.
2.  **Using Community OpenTelemetry Instrumentors**: Leverage existing OpenTelemetry instrumentation libraries like those from OpenInference or OpenLLMetry. These can be integrated with LangWatch by either passing them to the `langwatch.setup()` function or by using their native `instrument()` methods if you're managing your OpenTelemetry setup more directly.

This guide will walk you through both methods.

## Using `autotrack_openai_calls()`

The `autotrack_openai_calls()` function provides a straightforward way to capture all OpenAI calls made with a specific client instance for the duration of the current trace.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`.

```python
import langwatch
from openai import OpenAI

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup()

# Initialize your OpenAI client
client = OpenAI()

@langwatch.trace(name="OpenAI Chat Completion")
async def get_openai_chat_response(user_prompt: str):
    # Get the current trace and enable autotracking for the 'client' instance
    langwatch.get_current_trace().autotrack_openai_calls(client)

    # All calls made with 'client' will now be automatically captured as spans
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[{"role": "user", "content": user_prompt}],
    )
    completion = response.choices[0].message.content
    return completion

async def main():
    user_query = "Tell me a joke about Python programming."
    response = await get_openai_chat_response(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

Key points for `autotrack_openai_calls()`:
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments a *specific instance* of the OpenAI client. If you have multiple clients, you'll need to call it for each one you want to track.

## Using Community OpenTelemetry Instrumentors

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM providers, including OpenAI.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `OpenAIInstrumentor` from OpenInference or OpenLLMetry) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from openai import OpenAI
import os

# Example using OpenInference's OpenAIInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

# Initialize LangWatch with the OpenAIInstrumentor
langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

client = OpenAI()

@langwatch.trace(name="OpenAI Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles OpenAI calls globally.
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```
<Note>
  Ensure you have the respective community instrumentation library installed (e.g., `pip install openllmetry-instrumentation-openai` or `pip install openinference-instrumentation-openai`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from openai import OpenAI
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.openai import OpenAIInstrumentor

langwatch.setup()
client = OpenAI()

# Instrument OpenAI directly using the community library
OpenAIInstrumentor().instrument()

@langwatch.trace(name="OpenAI Call with Direct Community Instrumentation")
def get_story_ending(beginning: str):
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[
            {"role": "system", "content": "You are a creative writer. Complete the story."},
            {"role": "user", "content": beginning}
        ]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

### Key points for community instrumentors:
-   These instrumentors often patch OpenAI at a global level, meaning all OpenAI calls from any client instance will be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
-   If instrumenting directly (e.g., `OpenAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

<Note>
### Which Approach to Choose?

-   **`autotrack_openai_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which OpenAI client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
-   **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture OpenAI calls globally across your application, or need to instrument other libraries alongside OpenAI with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send OpenAI call data to LangWatch for monitoring and analysis.
</Note>

---

# FILE: ./integration/python/tutorials/capturing-metadata.mdx

---
title: Capturing Metadata and Attributes
sidebarTitle: Metadata & Attributes
description: Learn how to enrich your traces and spans with custom metadata and attributes using the LangWatch Python SDK.
---

Metadata and attributes are key-value pairs that allow you to add custom contextual information to your traces and spans. This enrichment is invaluable for debugging, analysis, filtering, and gaining deeper insights into your LLM application's behavior.

LangWatch distinguishes between two main types of custom data:

*   **Trace Metadata**: Information that applies to the entire lifecycle of a request or a complete operation.
*   **Span Attributes**: Information specific to a particular unit of work or step within a trace.

This tutorial will guide you through capturing both types using the Python SDK.

## Trace Metadata

Trace metadata provides context for the entire trace. It's ideal for information that remains constant throughout the execution of a traced operation, such as:

*   User identifiers (`user_id`)
*   Session or conversation identifiers (`session_id`, `thread_id`)
*   Application version (`app_version`)
*   Environment (`env: "production"`)
*   A/B testing flags or variant names

You can set trace metadata when a trace is initiated or update it at any point while the trace is active.

### Setting Trace Metadata at Initialization

The easiest way to add metadata to a trace is by passing a `metadata` dictionary to the `langwatch.trace()` decorator or context manager.

```python
import langwatch
import os

# Initialize LangWatch (ensure this is done once in your application)
langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

@langwatch.trace(name="UserQueryHandler", metadata={"user_id": "user_123", "session_id": "session_abc"})
def handle_user_query(query: str):
    # Your application logic here
    # For example, process the query and interact with an LLM
    processed_query = f"Query processed: {query}"

    # You can also update trace metadata from within the trace
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(metadata={"query_language": "en"})

    return processed_query

handle_user_query("Hello, LangWatch!")
```

In this example, `user_id` and `session_id` are attached to the "UserQueryHandler" trace from the start. Later, `query_language` is added.

<Note>
  Refer to the [`langwatch.trace()` API reference](/integration/python/reference#langwatchtrace) for more details on its parameters.
</Note>

### Updating Trace Metadata Dynamically

If you need to add or modify trace metadata after the trace has started (e.g., based on some intermediate result), you can use the `update()` method on the `LangWatchTrace` object.

You can get the current trace object using `langwatch.get_current_trace()` or from the `langwatch.trace()` context manager.

```python
import langwatch
import os
import uuid

langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

def process_customer_request(customer_id: str, request_details: dict):
    trace_metadata = {
        "customer_id": customer_id,
        "initial_request_type": request_details.get("type")
    }
    with langwatch.trace(name="CustomerRequestFlow", metadata=trace_metadata) as current_trace:
        # Simulate some processing
        print(f"Processing request for customer {customer_id}")

        # Update metadata based on some condition or result
        is_priority_customer = customer_id.startswith("vip_")
        current_trace.update(metadata={"priority_customer": is_priority_customer})

        # ... further processing ...

        if request_details.get("type") == "complaint":
            current_trace.update(metadata={"escalation_needed": True})

        print(f"Trace ID: {current_trace.id}") # Example of accessing trace properties

process_customer_request(f"vip_{uuid.uuid4()}", {"type": "complaint", "content": "Service issue"})
process_customer_request(f"user_{uuid.uuid4()}", {"type": "inquiry", "content": "Product question"})
```

## Span Attributes

Span attributes (often referred to simply as "attributes") provide context for a specific operation or unit of work *within* a trace. They are useful for details that are relevant only to that particular step. Examples include:

*   For an LLM call span: `model_name`, `prompt_template_version`, `temperature`
*   For a tool call span: `tool_name`, `api_endpoint`, specific input parameters
*   For a RAG span: `retrieved_document_ids`, `chunk_count`
*   Custom business logic flags or intermediate results specific to that span.

### Setting Span Attributes

You can set attributes on a span when it's created using the `attributes` parameter (less common for dynamic values) or, more typically, by calling the `update()` method on the `LangWatchSpan` object.

The `update()` method is flexible and allows you to pass attributes as keyword arguments.

<CodeGroup>
```python Using update() method
import langwatch
import os

langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

@langwatch.trace(name="ArticleGenerator")
def generate_article(topic: str):
    with langwatch.span(name="FetchResearchData", type="tool") as research_span:
        # Simulate fetching data
        research_data = f"Data about {topic}"
        research_span.update(
            source="internal_db",
            query_complexity="medium",
            items_retrieved=10
        )
        # research_span.set_attributes({"source": "internal_db"}) # Also works

    with langwatch.span(name="GenerateText", type="llm") as llm_span:
        llm_span.update(model="gpt-5", prompt_length=len(topic))
        # Simulate LLM call
        article_text = f"Here's an article about {topic} based on {research_data}."
        llm_span.update(output_length=len(article_text), tokens_used=150)

    return article_text

generate_article("AI in Healthcare")
```
```python Using attributes parameter (for static attributes)
import langwatch
import os

langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

@langwatch.trace(name="StaticAttributeDemo")
def process_with_static_info():
    # 'version' is a static attribute for this span's definition
    with langwatch.span(name="ComponentX", attributes={"version": "1.0.2", "region": "us-east-1"}):
        # ... logic for ComponentX ...
        print("ComponentX with static attributes executed.")
        # Dynamic attributes can still be added via update()
        langwatch.get_current_span().update(status_code=200)

process_with_static_info()
```
</CodeGroup>

In the first example, `source`, `query_complexity`, and `items_retrieved` are added to the "FetchResearchData" span. Similarly, `model`, `prompt_length`, `output_length`, and `tokens_used` contextualize the "GenerateText" LLM span.

The `LangWatchSpan` object also has a `set_attributes()` method which takes a dictionary, similar to OpenTelemetry's underlying span API.

<Note>
  For more details on span parameters and methods, see the [`langwatch.span()` API reference](/integration/python/reference#langwatchspan) and the [`LangWatchSpan` object methods](/integration/python/reference#langwatchspan-object-methods).
</Note>

## Key Differences: Trace Metadata vs. Span Attributes

| Feature         | Trace Metadata                                  | Span Attributes                                        |
|-----------------|-------------------------------------------------|--------------------------------------------------------|
| **Scope**       | Entire trace (e.g., a whole user request)       | Specific span (e.g., one LLM call, one tool use)       |
| **Granularity** | Coarse-grained, applies to the overall operation | Fine-grained, applies to a specific part of the operation |
| **Purpose**     | General context for the entire operation        | Specific details about a particular step or action     |
| **Examples**    | `user_id`, `session_id`, `app_version`          | `model_name`, `tool_parameters`, `retrieved_chunk_id`    |
| **SDK Access**  | `langwatch.trace(metadata={...})` <br/> `trace.update(metadata={...})` | `langwatch.span(attributes={...})` <br/> `span.update(key=value, ...)` <br/> `span.set_attributes({...})` |

**When to use which:**

*   Use **Trace Metadata** for information that you'd want to associate with every single span within that trace, or that defines the overarching context of the request (e.g., who initiated it, what version of the service is running).
*   Use **Span Attributes** for details specific to the execution of that particular span. This helps in understanding the parameters, behavior, and outcome of individual components within your trace.

## Viewing in LangWatch

All captured trace metadata and span attributes will be visible in the LangWatch UI.
- **Trace Metadata** is typically displayed in the trace details view, providing an overview of the entire operation.
- **Span Attributes** are shown when you inspect individual spans within a trace.

This rich contextual data allows you to:
- **Filter and search** for traces and spans based on specific metadata or attribute values.
- **Analyze performance** by correlating metrics with different metadata/attributes (e.g., comparing latencies for different `user_id`s or `model_name`s).
- **Debug issues** by quickly understanding the context and parameters of a failed or slow operation.

## Conclusion

Effectively using trace metadata and span attributes is crucial for maximizing the observability of your LLM applications. By enriching your traces with relevant contextual information, you empower yourself to better understand, debug, and optimize your systems with LangWatch.

Remember to instrument your code thoughtfully, adding data that provides meaningful insights without being overly verbose.

---

# FILE: ./integration/python/tutorials/capturing-evaluations-guardrails.mdx

---
title: Capturing Evaluations & Guardrails
sidebarTitle: Evaluations & Guardrails
description: Learn how to log custom evaluations, trigger managed evaluations, and implement guardrails with LangWatch.
keywords: Custom Evaluations, Managed Evaluations, Guardrails, LangWatch Evaluations, add_evaluation, evaluate, async_evaluate, Evaluation Metric, Evaluation Score, Evaluation Pass/Fail, Evaluation Label, Evaluation Details, Evaluation Cost, Evaluation Status, Evaluation Error, Evaluation Timestamps, Evaluation Type, Guardrail
---

LangWatch provides a flexible system for capturing various types of evaluations and implementing guardrails within your LLM applications. This allows you to track performance, ensure quality, and control application flow based on defined criteria.

There are three main ways to work with evaluations and guardrails:

1.  **Client-Side Custom Evaluations (`add_evaluation`)**: Log any custom evaluation metric, human feedback, or external system score directly from your Python code. These are primarily for observational purposes.
2.  **Server-Side Managed Evaluations (`evaluate`, `async_evaluate`)**: Trigger predefined or custom evaluation logic that runs on the LangWatch backend. These can return scores, pass/fail results, and other details.
3.  **Guardrails**: A special application of evaluations (either client-side or server-side) used to make decisions or enforce policies within your application flow.

## 1. Client-Side Custom Evaluations (`add_evaluation`)

You can log custom evaluation data directly from your application code using the `add_evaluation()` method on a `LangWatchSpan` or `LangWatchTrace` object. This is useful for recording metrics specific to your domain, results from external systems, or human feedback.

When you call `add_evaluation()`, LangWatch typically creates a new child span of type `evaluation` (or `guardrail` if `is_guardrail=True`) under the target span. This child span, named after your custom evaluation, stores its details, primarily in its `output` attribute.

Here's an example:

```python
import langwatch

# Assume langwatch.setup() has been called

@langwatch.span(name="Generate Response")
def process_request(user_query: str):
    response_text = f"Response to: {user_query}"
    langwatch.get_current_span().update(output=response_text)

    # Example 1: A simple pass/fail custom evaluation
    contains_keyword = "LangWatch" in response_text
    langwatch.get_current_span().add_evaluation(
        name="Keyword Check: LangWatch",
        passed=contains_keyword,
        details=f"Checked for 'LangWatch'. Found: {contains_keyword}"
    )

    # Example 2: A custom score for response quality
    human_score = 4.5
    langwatch.get_current_span().add_evaluation(
        name="Human Review: Quality Score",
        score=human_score,
        label="Good",
        details="Reviewed by Jane Doe. Response is clear and relevant."
    )

    # Example 3: A client-side guardrail check
    is_safe = not ("unsafe_word" in response_text)
    langwatch.get_current_span().add_evaluation(
        name="Safety Check (Client-Side)",
        passed=is_safe,
        is_guardrail=True, # Mark this as a guardrail
        details=f"Content safety check. Passed: {is_safe}"
    )
    if not is_safe:
        # Potentially alter flow or log a critical warning
        print("Warning: Client-side safety check failed!")


    return response_text

@langwatch.trace(name="Process User Request")
def main():
    user_question = "Tell me about LangWatch."
    generated_response = process_request(user_question)
    print(f"Query: {user_question}")
    print(f"Response: {generated_response}")

if __name__ == "__main__":
    main()
```

### `add_evaluation()` Parameters

The `add_evaluation()` method is available on both `LangWatchSpan` and `LangWatchTrace` objects (when using on a trace, you must specify the target `span`). For detailed parameter descriptions, please refer to the API reference:

- [`LangWatchSpan.add_evaluation()`](/integration/python/reference#add_evaluation-1)
- [`LangWatchTrace.add_evaluation()`](/integration/python/reference#add_evaluation)

## 2. Server-Side Managed Evaluations (`evaluate` & `async_evaluate`)

LangWatch allows you to trigger evaluations that are performed by the LangWatch backend. These can be [built-in evaluators](/llm-evaluation/list) (e.g., for faithfulness, relevance) or [custom evaluators you define](/evaluations/custom-evaluator-integration) in your LangWatch project settings.

You use the `evaluate()` (synchronous) or `async_evaluate()` (asynchronous) functions for this. These functions send the necessary data to the LangWatch API, which then processes the evaluation. These server-side evaluations are a core part of setting up [real-time monitoring and evaluations in production](/llm-evaluation/realtime/setup).

```python
import langwatch
from langwatch.evaluations import BasicEvaluateData
# from langwatch.types import RAGChunk # For RAG contexts

# Assume langwatch.setup() has been called

@langwatch.span()
def handle_rag_query(user_query: str):
    retrieved_contexts_str = [
        "LangWatch helps monitor LLM applications.",
        "Evaluations can be run on the server."
    ]
    # For richer context, use RAGChunk
    # retrieved_contexts_rag = [
    #     RAGChunk(content="LangWatch helps monitor LLM applications.", document_id="doc1"),
    #     RAGChunk(content="Evaluations can be run on the server.", document_id="doc2")
    # ]

    # Add the RAG contexts to the current span
    langwatch.get_current_span().update(contexts=retrieved_contexts_str)

    # Simulate LLM call
    llm_output = f"Based on the context, LangWatch is for monitoring and server-side evals."

    # Prepare data for server-side evaluation
    eval_data = BasicEvaluateData(
        input=user_query,
        output=llm_output,
        contexts=retrieved_contexts_str
    )

    # Trigger a server-side "faithfulness" evaluation
    # The 'faithfulness-evaluator' slug must be configured in your LangWatch project
    try:
        faithfulness_result = langwatch.evaluate(
            slug="faithfulness-evaluator", # Slug of the evaluator in LangWatch
            name="Faithfulness Check (Server)",
            data=eval_data,
        )

        print(f"Faithfulness Evaluation Result: {faithfulness_result}")
        # faithfulness_result is an EvaluationResultModel(status, passed, score, details, etc.)

        # Example: Using it as a guardrail
        if faithfulness_result.passed is False:
            print("Warning: Faithfulness check failed!")

    except Exception as e:
        print(f"Error during server-side evaluation: {e}")

    return llm_output

@langwatch.trace()
def main():
    query = "What can LangWatch do with contexts?"
    response = handle_rag_query(query)
    print(f"Query: {query}")
    print(f"Response: {response}")

if __name__ == "__main__":
    main()
```

### `evaluate()` / `async_evaluate()` Key Parameters

The `evaluate()` and `async_evaluate()` methods are available on both `LangWatchSpan` and `LangWatchTrace` objects. They can also be imported from `langwatch.evaluations` and called as `langwatch.evaluate()` or `langwatch.async_evaluate()`, where you would then explicitly pass the `span` or `trace` argument. For detailed parameter descriptions, refer to the API reference:

- [`LangWatchSpan.evaluate()`](/integration/python/reference#evaluate-1) and [`LangWatchSpan.async_evaluate()`](/integration/python/reference#async_evaluate-1)
- [`LangWatchTrace.evaluate()`](/integration/python/reference#evaluate) and [`LangWatchTrace.async_evaluate()`](/integration/python/reference#async_evaluate)

<Tip>
  **Understanding the `data` Parameter:**

  The core parameters like `slug`, `data`, `settings`, `as_guardrail`, `span`, and `trace` are generally consistent.
  For the `data` parameter specifically: while `BasicEvaluateData` is commonly used to provide a standardized structure for `input`, `output`, and `contexts` (which many built-in or common evaluators expect), it's important to know that `data` can be **any dictionary**. This flexibility allows you to pass arbitrary data structures tailored to custom server-side evaluators you might define. Using `BasicEvaluateData` with fields like `expected_output` is particularly useful when [evaluating if the LLM is generating the right answers](/llm-evaluation/offline/platform/answer-correctness) against a set of expected outputs. For scenarios where a golden answer isn't available, LangWatch also supports more open-ended evaluations, such as using an [LLM-as-a-judge](/llm-evaluation/offline/platform/llm-as-a-judge).
</Tip>

The `slug` parameter refers to the unique identifier of the evaluator configured in your LangWatch project settings. You can find a list of available evaluator types and learn how to configure them in our [LLM Evaluation documentation](/llm-evaluation/list).

The functions return an `EvaluationResultModel` containing `status`, `passed`, `score`, `details`, `label`, and `cost`.

## 3. Guardrails

Guardrails are evaluations used to make decisions or enforce policies within your application. They typically result in a boolean `passed` status that your code can act upon.

**Using Server-Side Evaluations as Guardrails:**
Set `as_guardrail=True` when calling `evaluate` or `async_evaluate`.

```python
# ... (inside a function with a current span)
eval_data = BasicEvaluateData(output=llm_response)
pii_check_result = langwatch.evaluate(
    slug="pii-detection-guardrail",
    data=eval_data,
    as_guardrail=True,
    span=langwatch.get_current_span()
)

if pii_check_result.passed is False:
    # Take action: sanitize response, return a canned message, etc.
    return "Response redacted due to PII."
```
A key behavior of `as_guardrail=True` for server-side evaluations is that if the *evaluation process itself* encounters an error (e.g., the evaluator service is down), the result will have `status="error"` but `passed` will default to `True`. This is a fail-safe to prevent your application from breaking due to an issue in the guardrail execution itself, assuming a "pass by default on error" stance is desired. For more on setting up safety-focused real-time evaluations like PII detection or prompt injection monitors, see our guide on [Setting up Real-Time Evaluations](/llm-evaluation/realtime/setup).

**Using Client-Side `add_evaluation` as Guardrails:**
Set `is_guardrail=True` when calling `add_evaluation`.

```python
# ... (inside a function with a current span)
is_too_long = len(llm_response) > 1000
response_span.add_evaluation(
    name="Length Guardrail",
    passed=(not is_too_long),
    is_guardrail=True,
    details=f"Length: {len(llm_response)}. Max: 1000"
)
if is_too_long:
    # Take action: truncate response, ask for shorter output, etc.
    return llm_response[:1000] + "..."
```
For client-side guardrails added with `add_evaluation`, your code is fully responsible for interpreting the `passed` status and handling any errors during the local check.

## How Evaluations and Guardrails Appear in LangWatch

Both client-side and server-side evaluations (including those marked as guardrails) are logged as spans in LangWatch.
- `add_evaluation`: Creates a child span of type `evaluation` (or `guardrail` if `is_guardrail=True`).
- `evaluate`/`async_evaluate`: Also create a child span of type `evaluation` (or `guardrail` if `as_guardrail=True`).

These spans will contain the evaluation's name, result (score, passed, label), details, cost, and any associated metadata, typically within their `output` attribute. This allows you to:
- See a history of all evaluation outcomes.
- Filter traces by evaluation results.
- Analyze the performance of different evaluators or guardrails.
- Correlate evaluation outcomes with other trace data (e.g., LLM inputs/outputs, latencies).

## Use Cases

- **Quality Assurance**:
    - **Client-Side**: Log scores from a custom heuristic checking for politeness in responses.
    - **Server-Side**: Trigger a managed ["Toxicity" evaluator](/llm-evaluation/list) on LLM outputs, or use more open-ended approaches like an [LLM-as-a-judge](/llm-evaluation/offline/platform/llm-as-a-judge) for tasks without predefined correct answers.
- **Compliance & Safety**:
    - **Client-Side Guardrail**: Perform a regex check for forbidden words and log it with `is_guardrail=True`.
    - **Server-Side Guardrail**: Use a managed ["PII Detection" evaluator](/llm-evaluation/list) with `as_guardrail=True` to decide if a response can be shown.
- **Performance Monitoring**:
    - **Client-Side**: Log human feedback scores (`add_evaluation`) for helpfulness.
    - **Server-Side**: Evaluate RAG system outputs for ["Context Relevancy" and "Faithfulness"](/llm-evaluation/list) using managed evaluators.
- **A/B Testing**: Log custom metrics or trigger standard evaluations for different model versions or prompts to compare their performance.
- **Feedback Integration**: `add_evaluation` can be used to pipe scores from an external human review platform directly into the relevant trace.

By combining these methods, you can build a robust evaluation and guardrailing strategy tailored to your application's needs, all observable within LangWatch.

---

# FILE: ./integration/python/tutorials/capturing-rag.mdx

---
title: Capturing RAG
sidebarTitle: Capturing RAG
description: Learn how to capture Retrieval Augmented Generation (RAG) data with LangWatch.
keywords: RAG, Retrieval Augmented Generation, LangChain, LangWatch, LangChain RAG, RAG Span, RAG Chunk, RAG Tool
---

Retrieval Augmented Generation (RAG) is a common pattern in LLM applications where you first retrieve relevant context from a knowledge base and then use that context to generate a response. LangWatch provides specific ways to capture RAG data, enabling better observability and evaluation of your RAG pipelines.

By capturing the `contexts` (retrieved documents) used by the LLM, you unlock several benefits in LangWatch:
- Specialized RAG evaluators (e.g., Faithfulness, Context Relevancy).
- Analytics on document usage (e.g., which documents are retrieved most often, which ones lead to better responses).
- Deeper insights into the retrieval step of your pipeline.

There are two main ways to capture RAG spans: manually creating a RAG span or using framework-specific integrations like the one for LangChain.

## Manual RAG Span Creation

You can manually create a RAG span by decorating a function with `@langwatch.span(type="rag")`. Inside this function, you should perform the retrieval and then update the span with the retrieved contexts.

The `contexts` should be a list of strings or `RAGChunk` objects. The `RAGChunk` object allows you to provide more metadata about each retrieved chunk, such as `document_id` and `source`.

Here's an example:

```python
import langwatch
import time # For simulating work

# Assume langwatch.setup() has been called elsewhere

@langwatch.span(type="llm")
def generate_answer_from_context(contexts: list[str], user_query: str):
    # Simulate LLM call using the contexts
    time.sleep(0.5)
    response = f"Based on the context, the answer to '{user_query}' is..."
    # You can update the LLM span with model details, token counts, etc.
    langwatch.get_current_span().update(
        model="gpt-5",
        prompt=f"Contexts: {contexts}\nQuery: {user_query}",
        completion=response
    )
    return response

@langwatch.span(type="rag", name="My Custom RAG Process")
def perform_rag(user_query: str):
    # 1. Retrieve contexts
    # Simulate retrieval from a vector store or other source
    time.sleep(0.3)
    retrieved_docs = [
        "LangWatch helps monitor LLM applications.",
        "RAG combines retrieval with generation for better answers.",
        "Python is a popular language for AI development."
    ]

    # Update the current RAG span with the retrieved contexts
    # You can pass a list of strings directly
    langwatch.get_current_span().update(contexts=retrieved_docs)

    # Alternatively, for richer context information:
    # from langwatch.types import RAGChunk
    # rag_chunks = [
    #     RAGChunk(content="LangWatch helps monitor LLM applications.", document_id="doc1", source="internal_wiki/langwatch"),
    #     RAGChunk(content="RAG combines retrieval with generation for better answers.", document_id="doc2", source="blog/rag_explained")
    # ]
    # langwatch.get_current_span().update(contexts=rag_chunks)

    # 2. Generate answer using the contexts
    final_answer = generate_answer_from_context(contexts=retrieved_docs, user_query=user_query)

    # The RAG span automatically captures its input (user_query) and output (final_answer)
    # if capture_input and capture_output are not set to False.
    return final_answer

@langwatch.trace(name="User Question Handler")
def handle_user_question(question: str):
    langwatch.get_current_trace().update(
        input=question,
        metadata={"user_id": "example_user_123"}
    )

    answer = perform_rag(user_query=question)

    langwatch.get_current_trace().update(output=answer)
    return answer

if __name__ == "__main__":
    user_question = "What is LangWatch used for?"
    response = handle_user_question(user_question)
    print(f"Question: {user_question}")
    print(f"Answer: {response}")

```

In this example:
1.  `perform_rag` is decorated with `@langwatch.span(type="rag")`.
2.  Inside `perform_rag`, we simulate a retrieval step.
3.  `langwatch.get_current_span().update(contexts=retrieved_docs)` is called to explicitly log the retrieved documents.
4.  The generation step (`generate_answer_from_context`) is called, which itself can be another span (e.g., an LLM span).

## LangChain RAG Integration

If you are using LangChain, LangWatch provides utilities to simplify capturing RAG data from retrievers and tools.

### Capturing RAG from a Retriever

You can wrap your LangChain retriever with `langwatch.langchain.capture_rag_from_retriever`. This function takes your retriever and a lambda function to transform the retrieved `Document` objects into `RAGChunk` objects.

```python
import langwatch
from langwatch.types import RAGChunk

from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores.faiss import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable.config import RunnableConfig

# 1. Setup LangWatch (if not done globally)
# langwatch.setup()

# 2. Prepare your retriever
loader = WebBaseLoader("https://docs.langwatch.ai/introduction") # Example source
docs = loader.load()
documents = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=200
).split_documents(docs)
vector = FAISS.from_documents(documents, OpenAIEmbeddings())
retriever = vector.as_retriever()

# 3. Wrap the retriever for LangWatch RAG capture
# This lambda tells LangWatch how to extract data for RAGChunk from LangChain's Document
langwatch_retriever_tool = create_retriever_tool(
    langwatch.langchain.capture_rag_from_retriever(
        retriever,
        lambda document: RAGChunk(
            document_id=document.metadata.get("source", "unknown_source"), # Use a fallback for source
            content=document.page_content,
            # You can add other fields like 'score' if available in document.metadata
        ),
    ),
    "langwatch_docs_search", # Tool name
    "Search for information about LangWatch.", # Tool description
)

# 4. Use the wrapped retriever in your agent/chain
tools = [langwatch_retriever_tool]
model = ChatOpenAI(model="gpt-5", streaming=True)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant. Answer questions based on the retrieved context.\n{agent_scratchpad}"),
        ("human", "{question}"),
    ]
)
agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # type: ignore

@langwatch.trace(name="LangChain RAG Agent Execution")
def run_langchain_rag(user_input: str):
    current_trace = langwatch.get_current_trace()
    current_trace.update(metadata={"user_id": "lc_rag_user"})

    # Ensure the LangChain callback is used to capture all LangChain steps
    response = agent_executor.invoke(
        {"question": user_input},
        config=RunnableConfig(
            callbacks=[current_trace.get_langchain_callback()]
        ),
    )

    output = response.get("output", "No output found.")=
    return output

if __name__ == "__main__":
    question = "What is LangWatch?"
    answer = run_langchain_rag(question)
    print(f"Question: {question}")
    print(f"Answer: {answer}")
```

#### Key elements
- `langwatch.langchain.capture_rag_from_retriever(retriever, lambda document: ...)`: This wraps your existing retriever.
- The lambda function `lambda document: RAGChunk(...)` defines how to map fields from LangChain's `Document` to LangWatch's `RAGChunk`. This is crucial for providing detailed context information.
- The wrapped retriever is then used to create a tool, which is subsequently used in an agent or chain.
- Remember to include `langwatch.get_current_trace().get_langchain_callback()` in your `RunnableConfig` when invoking the chain/agent to capture all LangChain operations.

### Capturing RAG from a Tool

Alternatively, if your RAG mechanism is encapsulated within a generic LangChain `BaseTool`, you can use `langwatch.langchain.capture_rag_from_tool`.

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.trace()
def main():
    my_custom_tool = ...
    wrapped_tool = langwatch.langchain.capture_rag_from_tool(
        my_custom_tool, lambda response: [
          RAGChunk(
            document_id=response["id"], # optional
            chunk_id=response["chunk_id"], # optional
            content=response["content"]
          )
        ]
    )

    tools = [wrapped_tool] # use the new wrapped tool in your agent instead of the original one
    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful assistant that only reply in short tweet-like responses, using lots of emojis and use tools only once.\n\n{agent_scratchpad}",
            ),
            ("human", "{question}"),
        ]
    )
    agent = create_tool_calling_agent(model, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return executor.invoke(user_input, config=RunnableConfig(
        callbacks=[langWatchCallback]
    ))
```
The `capture_rag_from_tool` approach is generally less direct for RAG from retrievers because you have to parse the tool's output (which is usually a string) to extract structured context information. `capture_rag_from_retriever` is preferred when dealing directly with LangChain retrievers.

By effectively capturing RAG spans, you gain much richer data in LangWatch, enabling more powerful analysis and evaluation of your RAG systems. Refer to the SDK examples for more detailed implementations.

---

# FILE: ./integration/python/tutorials/capturing-mapping-input-output.mdx

---
title: Capturing and Mapping Inputs & Outputs
sidebarTitle: Inputs & Outputs
description: Learn how to control the capture and structure of input and output data for traces and spans with the LangWatch Python SDK.
---

Effectively capturing the inputs and outputs of your LLM application's operations is crucial for observability. LangWatch provides flexible ways to manage this data, whether you prefer automatic capture or explicit control to map complex objects, format data, or redact sensitive information.

This tutorial covers how to:
*   Understand automatic input/output capture.
*   Explicitly set inputs and outputs for traces and spans.
*   Dynamically update this data on active traces/spans.
*   Handle different data formats, especially for chat messages.

## Automatic Input and Output Capture

By default, when you use `@langwatch.trace()` or `@langwatch.span()` as decorators on functions, the SDK attempts to automatically capture:

*   **Inputs**: The arguments passed to the decorated function.
*   **Outputs**: The value returned by the decorated function.

This behavior can be controlled using the `capture_input` and `capture_output` boolean parameters.

```python
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="GreetUser", capture_input=True, capture_output=True)
def greet_user(name: str, greeting: str = "Hello"):
    # 'name' and 'greeting' will be captured as input.
    # The returned string will be captured as output.
    return f"{greeting}, {name}!"

greet_user("Alice")

@langwatch.span(name="SensitiveOperation", capture_input=False, capture_output=False)
def process_sensitive_data(data: dict):
    # Inputs and outputs for this span will not be automatically captured.
    # You might explicitly set a sanitized version if needed.
    print("Processing sensitive data...")
    return {"status": "processed"}

@langwatch.trace(name="MainFlow")
def main_flow():
    greet_user("Bob", greeting="Hi")
    process_sensitive_data({"secret": "data"})

main_flow()
```

<Note>
  Refer to the API reference for [`@langwatch.trace()`](/integration/python/reference#%40langwatch-trace-%2F-langwatch-trace) and [`@langwatch.span()`](/integration/python/reference#%40langwatch-span-%2F-langwatch-span) for more details on `capture_input` and `capture_output` parameters.
</Note>

## Explicitly Setting Inputs and Outputs

You often need more control over what data is recorded. You can explicitly set inputs and outputs using the `input` and `output` parameters when initiating a trace or span, or by using the `update()` method on the respective objects.

This is useful for:
*   Capturing only specific parts of complex objects.
*   Formatting data in a more readable or structured way (e.g., as a list of `ChatMessage` objects).
*   Redacting sensitive information before it's sent to LangWatch.
*   Providing inputs/outputs when not using decorators (e.g., with context managers for parts of a function).

### At Initialization

When using `@langwatch.trace()` or `@langwatch.span()` (either as decorators or context managers), you can pass `input` and `output` arguments.

<CodeGroup>
```python Trace with explicit input/output
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(
    name="UserIntentProcessing",
    input={"user_query": "Book a flight to London"},
    # Output can be set later via update() if determined by function logic
)
def process_user_intent(raw_query_data: dict):
    # raw_query_data might be large or contain sensitive info
    # The 'input' parameter above provides a clean version.
    intent = "book_flight"
    entities = {"destination": "London"}

    # Explicitly set the output for the root span of the trace
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(output={"intent": intent, "entities": entities})

    return {"status": "success", "intent": intent} # Actual function return

process_user_intent({"query": "Book a flight to London", "user_id": "123"})
```

```python Span with explicit input/output
import langwatch
import os
from langwatch.domain import ChatMessage

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="ChatbotInteraction")
def handle_chat():
    user_message = ChatMessage(role="user", content="What is LangWatch?")

    with langwatch.span(
        name="LLMCall",
        type="llm",
        input=[user_message],
        model="gpt-5"
    ) as llm_span:
        # Simulate LLM call
        assistant_response_content = "LangWatch helps you monitor your LLM applications."
        assistant_message = ChatMessage(role="assistant", content=assistant_response_content)

        # Set output on the span object
        llm_span.update(output=[assistant_message])

    print("Chat finished.")

handle_chat()
```
</CodeGroup>

If you provide `input` or `output` directly, it overrides what might have been automatically captured for that field.

### Dynamically Updating Inputs and Outputs

You can modify the input or output of an active trace or span using its `update()` method. This is particularly useful when the input/output data is determined or refined during the operation.

```python
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="DataTransformationPipeline")
def run_pipeline(initial_data: dict):
    # Initial input is automatically captured if capture_input=True (default)

    with langwatch.span(name="Step1_CleanData") as step1_span:
        # Suppose initial_data is complex, we want to record a summary as input
        step1_span.update(input={"data_keys": list(initial_data.keys())})
        cleaned_data = {k: v for k, v in initial_data.items() if v is not None}
        step1_span.update(output={"cleaned_item_count": len(cleaned_data)})

    # ... further steps ...

    # Update the root span's output for the entire trace
    final_result = {"status": "completed", "items_processed": len(cleaned_data)}
    langwatch.get_current_trace().update(output=final_result)

    return final_result

run_pipeline({"a": 1, "b": None, "c": 3})
```

<Note>
  The `update()` method on `LangWatchTrace` and `LangWatchSpan` objects is versatile. See the reference for [`LangWatchTrace` methods](/integration/python/reference#%40langwatch-trace-%2F-langwatch-trace) and [`LangWatchSpan` methods](/integration/python/reference#%40langwatch-span-%2F-langwatch-span).
</Note>

## Handling Different Data Formats

LangWatch can store various types of input and output data:

*   **Strings**: Simple text.
*   **Dictionaries**: Automatically serialized as JSON. This is useful for structured data.
*   **Lists of `ChatMessage` objects**: The standard way to represent conversations for LLM interactions. This ensures proper display and analysis in the LangWatch UI.

### Capturing Chat Messages

For LLM interactions, structure your inputs and outputs as a list of `ChatMessage` objects.

```python
import langwatch
import os
from langwatch.domain import ChatMessage, ToolCall, FunctionCall # For more complex messages

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="AdvancedChat")
def advanced_chat_example():
    messages = [
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="What is the weather in London?")
    ]

    with langwatch.span(name="GetWeatherToolCall", type="llm", input=messages, model="gpt-5") as llm_span:
        # Simulate model deciding to call a tool
        tool_call_id = "call_abc123"
        assistant_response_with_tool = ChatMessage(
            role="assistant",
            tool_calls=[
                ToolCall(
                    id=tool_call_id,
                    type="function",
                    function=FunctionCall(name="get_weather", arguments='''{"location": "London"}''')
                )
            ]
        )
        llm_span.update(output=[assistant_response_with_tool])

    # Simulate tool execution
    with langwatch.span(name="RunGetWeatherTool", type="tool") as tool_span:
        tool_input = {"tool_name": "get_weather", "arguments": {"location": "London"}}
        tool_span.update(input=tool_input)

        tool_result_content = '''{"temperature": "15C", "condition": "Cloudy"}'''
        tool_span.update(output=tool_result_content)

        # Prepare message for next LLM call
        tool_response_message = ChatMessage(
            role="tool",
            tool_call_id=tool_call_id,
            name="get_weather",
            content=tool_result_content
        )
        messages.append(assistant_response_with_tool) # Assistant's decision to call tool
        messages.append(tool_response_message)      # Tool's response

    with langwatch.span(name="FinalLLMResponse", type="llm", input=messages, model="gpt-5") as final_llm_span:
        final_assistant_content = "The weather in London is 15°C and cloudy."
        final_assistant_message = ChatMessage(role="assistant", content=final_assistant_content)
        final_llm_span.update(output=[final_assistant_message])

advanced_chat_example()
```

<Note>
  For the detailed structure of `ChatMessage`, `ToolCall`, and other related types, please refer to the [Core Data Types section in the API Reference](/integration/python/reference#core-data-types).
</Note>

## Use Cases and Best Practices

*   **Redacting Sensitive Information**: If your function arguments or return values contain sensitive data (PII, API keys), disable automatic capture (`capture_input=False`, `capture_output=False`) and explicitly set sanitized versions using `input`/`output` parameters or `update()`.
*   **Mapping Complex Objects**: If your inputs/outputs are complex Python objects, map them to a dictionary or a simplified string representation for clearer display in LangWatch.
*   **Improving Readability**: For long text inputs/outputs (e.g., full documents), consider capturing a summary or metadata instead of the entire content to reduce noise, unless the full content is essential for debugging or evaluating.
*   **Clearing Captured Data**: You can set `input=None` or `output=None` via the `update()` method to remove previously captured (or auto-captured) data if it's no longer relevant or was captured in error.

```python
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="DataRedactionExample")
def handle_user_data(user_profile: dict):
    # user_profile might contain PII
    # Automatic capture is on by default.
    # Let's update the input to a redacted version for the root span.

    redacted_input = {
        "user_id": user_profile.get("id"),
        "has_email": "email" in user_profile
    }
    langwatch.get_current_trace().update(input=redacted_input)

    # Process data...
    result = {"status": "processed", "user_id": user_profile.get("id")}
    langwatch.get_current_trace().update(output=result)
    return result # Actual function return can still be the full data

handle_user_data({"id": "user_xyz", "email": "test@example.com", "name": "Sensitive Name"})
```

## Conclusion

Controlling how inputs and outputs are captured in LangWatch allows you to tailor the observability data to your specific needs. By using automatic capture flags, explicit parameters, dynamic updates, and appropriate data formatting (especially `ChatMessage` for conversations), you can ensure that your traces provide clear, relevant, and secure insights into your LLM application's behavior.

---

# FILE: ./integration/python/tutorials/manual-instrumentation.mdx

---
title: Manual Instrumentation
description: Learn how to manually instrument your code with the LangWatch Python SDK
keywords: manual instrumentation, context managers, span, trace, async, synchronous, LangWatch, Python
---

While decorators offer a concise way to instrument functions, you might prefer or need to manually manage trace and span lifecycles. This is useful in asynchronous contexts, for finer control, or when decorators are inconvenient. The LangWatch Python SDK provides two primary ways to do this manually:

### Using Context Managers (`with`/`async with`)

The `langwatch.trace()` and `langwatch.span()` functions can be used directly as asynchronous (`async with`) or synchronous (`with`) context managers. This is the recommended approach for manual instrumentation as it automatically handles ending the trace/span, even if errors occur.

Here's how you can achieve the same instrumentation as the decorator examples, but using context managers:

```python
import langwatch
from langwatch.types import RAGChunk
from langwatch.attributes import AttributeKey # For semantic attribute keys
import asyncio # Assuming async operation

langwatch.setup()

async def rag_retrieval_manual(query: str):
    # Use async with for the span, instead of a decorator
    async with langwatch.span(type="rag", name="RAG Document Retrieval") as span:
        # ... your async retrieval logic ...
        await asyncio.sleep(0.05) # Simulate async work
        search_results = [
            {"id": "doc-1", "content": "Content for doc 1."},
            {"id": "doc-2", "content": "Content for doc 2."},
        ]

        # Update the span with input, context, metadata, and output
        span.update(
            input=query,
            contexts=[
                RAGChunk(document_id=doc["id"], content=doc["content"])
                for doc in search_results
            ],
            output=search_results,
            strategy="manual_vector_search"
        )
        return search_results

async def handle_user_query_manual(query: str):
    # Use async with for the trace
    async with langwatch.trace(name="Manual User Query Handling", metadata={"user_id": "manual-user", "query": query}) as trace:
        # Call the manually instrumented RAG function
        retrieved_docs = await rag_retrieval_manual(query)

        # --- Simulate LLM Call Step (manual span) ---
        llm_response = ""
        async with langwatch.span(type="llm", name="Manual LLM Generation") as llm_span:
            llm_input = {"role": "user", "content": f"Context: {retrieved_docs}\nQuery: {query}"}
            llm_metadata = {"model_name": "gpt-5"}

            # ... your async LLM call logic ...
            await asyncio.sleep(0.1)
            llm_response = "This is the manual LLM response."
            llm_output = {"role": "assistant", "content": llm_response}

            # Set input, metadata and output via update
            llm_span.update(
                input=llm_input,
                output=llm_output
                llm_metadata=llm_metadata,
            )

        # Set final trace output via update
        trace.update(output=llm_response)
        return llm_response

# Example execution (in an async context)
async def main():
    result = await handle_user_query_manual("Tell me about manual tracing with context managers.")
    print(result)
asyncio.run(main())
```

Key points for manual instrumentation with context managers:

- Use `with langwatch.trace(...)` or `async with langwatch.trace(...)` to start a trace.
- Use `with langwatch.span(...)` or `async with langwatch.span(...)` inside a trace block to create nested spans.
- The trace or span object is available in the `as trace:` or `as span:` part of the `with` statement.
- Use methods like `span.add_event()`, and primarily `span.update(...)` / `trace.update(...)` to add details. The `update()` method is flexible for adding structured data like `input`, `output`, `metadata`, and `contexts`.
- This approach gives explicit control over the start and end of each instrumented block, as the context manager handles ending the span automatically.

### Direct Span Creation (`span.end()`)

Alternatively, you can manage span and trace lifecycles completely manually. Call `langwatch.span()` or `langwatch.trace()` directly to start them, and then explicitly call the `end()` method on the returned object (`span.end()` or `trace.end()`) when the operation finishes. **This requires careful handling to ensure `end()` is always called, even if errors occur (e.g., using `try...finally`).** Context managers are generally preferred as they handle this automatically.

```python
import langwatch
import time

# Assume langwatch.setup() and a trace context exist

def process_data_manually(data):
    span = langwatch.span(name="Manual Data Processing") # Start the span
    try:
        span.update(input=data)
        # ... synchronous processing logic ...
        time.sleep(0.02)
        result = f"Processed: {data}"
        span.update(output=result)
        return result
    except Exception as e:
        span.record_exception(e) # Record exceptions
        span.set_status("error", description=str(e))
        raise # Re-raise the exception
    finally:
        span.end() # CRITICAL: Ensure the span is ended

# with langwatch.trace(): # Needs to be within a trace
#     processed = process_data_manually("some data")
```

---

# FILE: ./integration/python/tutorials/open-telemetry.mdx

---
title: Using LangWatch with OpenTelemetry
description: Learn how to integrate the LangWatch Python SDK with your existing OpenTelemetry setup.
keywords: OpenTelemetry, OTel, auto-instrumentation, OpenAI, Celery, HTTP clients, databases, ORMs, LangWatch, Python
---

The LangWatch Python SDK is built entirely on top of the robust [OpenTelemetry (OTel)](https://opentelemetry.io/) standard. This means seamless integration with existing OTel setups and interoperability with the wider OTel ecosystem.

## LangWatch Spans are OpenTelemetry Spans

It's important to understand that LangWatch traces and spans **are** standard OpenTelemetry traces and spans. LangWatch adds specific semantic attributes (like `langwatch.span.type`, `langwatch.inputs`, `langwatch.outputs`, `langwatch.metadata`) to these standard spans to power its observability features.

This foundation provides several benefits:
- **Interoperability:** Traces generated with LangWatch can be sent to any OTel-compatible backend (Jaeger, Tempo, Datadog, etc.) alongside your other application traces.
- **Familiar API:** If you're already familiar with OpenTelemetry concepts and APIs, working with LangWatch's manual instrumentation will feel natural.
- **Leverage Existing Setup:** LangWatch integrates smoothly with your existing OTel `TracerProvider` and instrumentation.

Perhaps the most significant advantage is that **LangWatch seamlessly integrates with the vast ecosystem of standard OpenTelemetry auto-instrumentation libraries.** This means you can easily combine LangWatch's LLM-specific observability with insights from other parts of your application stack. For example, if you use `opentelemetry-instrumentation-celery`, traces initiated by LangWatch for an LLM task can automatically include spans generated within your Celery workers, giving you a complete end-to-end view of the request, including background processing, without any extra configuration.

## Leverage the OpenTelemetry Ecosystem: Auto-Instrumentation

One of the most powerful benefits of LangWatch's OpenTelemetry foundation is its **automatic compatibility with the extensive ecosystem of OpenTelemetry auto-instrumentation libraries.**

When you use standard OTel auto-instrumentation for libraries like web frameworks, databases, or task queues alongside LangWatch, you gain **complete end-to-end visibility** into your LLM application's requests. Because LangWatch and these auto-instrumentors use the same underlying OpenTelemetry tracing system and context propagation mechanisms, spans generated across different parts of your application are automatically linked together into a single, unified trace.

This means you don't need to manually stitch together observability data from your LLM interactions and the surrounding infrastructure. If LangWatch instruments an LLM call, and that call involves fetching data via an instrumented database client or triggering a background task via an instrumented queue, all those operations will appear as connected spans within the same trace view in LangWatch (and any other OTel backend you use).

### Examples of Auto-Instrumentation Integration

Here are common scenarios where combining LangWatch with OTel auto-instrumentation provides significant value:

*   **Web Frameworks (FastAPI, Flask, Django):** Using libraries like `opentelemetry-instrumentation-fastapi`, an incoming HTTP request automatically starts a trace. When your request handler calls a function instrumented with `@langwatch.trace` or `@langwatch.span`, those LangWatch spans become children of the incoming request span. You see the full request lifecycle, from web server entry to LLM processing and response generation.

*   **HTTP Clients (Requests, httpx, aiohttp):** If your LLM application makes outbound API calls (e.g., to fetch external data, call a vector database API, or use a non-instrumented LLM provider via REST) using libraries instrumented by `opentelemetry-instrumentation-requests` or similar, these HTTP request spans will automatically appear within your LangWatch trace, showing the latency and success/failure of these external dependencies.

*   **Task Queues (Celery, RQ):** When a request handled by your web server (and traced by LangWatch) enqueues a background job using `opentelemetry-instrumentation-celery`, the trace context is automatically propagated. The spans generated by the Celery worker processing that job will be linked to the original LangWatch trace, giving you visibility into asynchronous operations triggered by your LLM pipeline.

*   **Databases & ORMs (SQLAlchemy, Psycopg2, Django ORM):** Using libraries like `opentelemetry-instrumentation-sqlalchemy`, any database queries executed during your LLM processing (e.g., for RAG retrieval, user data lookup, logging results) will appear as spans within the relevant LangWatch trace, pinpointing database interaction time and specific queries.

To enable this, simply ensure you have installed and configured the relevant OpenTelemetry auto-instrumentation libraries according to their documentation, typically involving an installation (`pip install opentelemetry-instrumentation-<library>`) and sometimes an initialization step (like `CeleryInstrumentor().instrument()`). As long as they use the same (or the global) `TracerProvider` that LangWatch is configured with, the integration is automatic.

#### Example: Combining LangWatch, RAG, OpenAI, and Celery

Let's illustrate this with a simplified example involving a web request that performs RAG, calls OpenAI, and triggers a background Celery task.

<CodeGroup>

```txt requirements.txt
langwatch
openai
celery
opentelemetry-instrumentation-celery
```

```python example.py
import langwatch
import os
import asyncio
from celery import Celery
from openai import OpenAI
from langwatch.types import RAGChunk

# 1. Configure Celery App
celery_app = Celery('tasks', broker=os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0'))

# 2. Setup LangWatch and OpenTelemetry Instrumentation
from opentelemetry_instrumentation.celery import CeleryInstrumentor
CeleryInstrumentor().instrument()

# Now setup LangWatch (it will likely pick up the global provider configured by Celery)
langwatch.setup(
    # If you have other OTel exporters, configure your TracerProvider manually
    # and pass it via tracer_provider=..., setting ignore_warning=True
    ignore_global_tracer_provider_override_warning=True
)

client = OpenAI()

# 3. Define the Celery Task
@celery_app.task
def process_result_background(result_id: str, llm_output: str):
    # This task execution will be automatically linked to the trace
    # that enqueued it, thanks to CeleryInstrumentor.
    # Spans created here (e.g., database writes) would be part of the same trace.
    print(f"[Celery Worker] Processing result {result_id}...")
    # Simulate work
    import time
    time.sleep(1)
    print(f"[Celery Worker] Finished processing {result_id}")
    return f"Processed: {llm_output[:10]}..."

# 4. Define RAG and Main Processing Logic
@langwatch.span(type="rag")
def retrieve_documents(query: str) -> list:
    # Simulate RAG retrieval
    print(f"Retrieving documents for: {query}")
    chunks = [
        RAGChunk(document_id="doc-abc", content="LangWatch uses OpenTelemetry."),
        RAGChunk(document_id="doc-def", content="Celery integrates with OpenTelemetry."),
    ]
    langwatch.get_current_span().update(contexts=chunks)
    time.sleep(0.1)
    return [c.content for c in chunks]

@langwatch.trace(name="Handle User Query with Celery")
def handle_request(user_query: str):
    # This is the root span for the request
    langwatch.get_current_trace().autotrack_openai_calls(client)
    langwatch.get_current_trace().update(metadata={"user_query": user_query})

    context_docs = retrieve_documents(user_query)

    try:
        completion = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": f"Use this context: {context_docs}"},
                {"role": "user", "content": user_query}
            ],
            temperature=0.5,
        )
        llm_result = completion.choices[0].message.content
    except Exception as e:
        langwatch.get_current_trace().record_exception(e)
        llm_result = "Error calling OpenAI"

    result_id = f"res_{int(time.time())}"
    # The current trace context is automatically propagated
    process_result_background.delay(result_id, llm_result)
    print(f"Enqueued background processing task {result_id}")

    return llm_result

# 5. Simulate Triggering the Request
if __name__ == "__main__":
    print("Simulating web request...")
    final_answer = handle_request("How does LangWatch work with Celery?")
    print(f"\nFinal Answer returned to user: {final_answer}")
    # Allow time for task to be processed if running worker locally
    time.sleep(3) # Add a small delay to see Celery output

    # To run this example:
    # 1. Start a Celery worker: celery -A your_module_name worker --loglevel=info
    # 2. Run this Python script.
    # 3. Observe the logs and the trace in LangWatch/OTel backend.
```

</CodeGroup>

In this example:
- The `handle_request` function is the main trace.
- `retrieve_documents` is a child span created by LangWatch.
- The OpenAI call creates child spans (due to `autotrack_openai_calls`).
- The call to `process_result_background.delay` creates a span indicating the task was enqueued.
- Critically, `CeleryInstrumentor` automatically propagates the trace context, so when the Celery worker picks up the `process_result_background` task, its execution is linked as a child span (or spans, if the task itself creates more) under the original `handle_request` trace.

This gives you a unified view of the entire operation, from the initial request through LLM processing, RAG, and background task execution.

## Integrating with `langwatch.setup()`

When you call `langwatch.setup()`, it intelligently interacts with your existing OpenTelemetry environment:

1.  **Checks for Existing `TracerProvider`:**
    - If you provide a `TracerProvider` instance via the `tracer_provider` argument in `langwatch.setup()`, LangWatch will use that specific provider.
    - If you *don't* provide one, LangWatch checks if a global `TracerProvider` has already been set (e.g., by another library or your own OTel setup code).
    - If neither is found, LangWatch creates a new `TracerProvider`.

2.  **Adding the LangWatch Exporter:**
    - If LangWatch uses an *existing* `TracerProvider` (either provided via the argument or detected globally), it will **add its own OTLP Span Exporter** to that provider's list of Span Processors. It does *not* remove existing processors or exporters.
    - If LangWatch creates a *new* `TracerProvider`, it configures it with the LangWatch OTLP Span Exporter.

## Default Behavior: All Spans Go to LangWatch

A crucial point is that once `langwatch.setup()` runs and attaches its exporter to a `TracerProvider`, **all spans** managed by that provider will be exported to the LangWatch backend by default. This includes:
- Spans created using `@langwatch.trace` and `@langwatch.span`.
- Spans created manually using `langwatch.trace()` or `langwatch.span()` as context managers or via `span.end()`.
- Spans generated by standard OpenTelemetry auto-instrumentation libraries (e.g., `opentelemetry-instrumentation-requests`, `opentelemetry-instrumentation-fastapi`) if they are configured to use the same `TracerProvider`.
- Spans you create directly using the OpenTelemetry API (`tracer.start_as_current_span(...)`).

While seeing all application traces can be useful, you might not want *every single span* sent to LangWatch, especially high-volume or low-value ones (like health checks or database pings).

## Selectively Exporting Spans with `span_exclude_rules`

To control which spans are sent to LangWatch, use the `span_exclude_rules` argument during `langwatch.setup()`. This allows you to define rules to filter spans *before* they are exported to LangWatch, without affecting other exporters attached to the same `TracerProvider`.

Rules are defined using `SpanProcessingExcludeRule` objects.

```python
import langwatch
import os
from langwatch.domain import SpanProcessingExcludeRule
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

# Example: You already have an OTel setup exporting to console
existing_provider = TracerProvider()
existing_provider.add_span_processor(
    SimpleSpanProcessor(ConsoleSpanExporter())
)

# Define rules to prevent specific spans from going to LangWatch
# (They will still go to the Console exporter)
exclude_rules = [
    # Exclude spans exactly named "GET /health_check"
    SpanProcessingExcludeRule(
        field_name="span_name",
        match_value="GET /health_check",
        match_operation="exact_match"
    ),
    # Exclude spans where 'http.method' attribute is 'OPTIONS'
    SpanProcessingExcludeRule(
        field_name="attribute",
        attribute_name="http.method",
        match_value="OPTIONS",
        match_operation="exact_match"
    ),
    # Exclude spans whose names start with "Internal."
    SpanProcessingExcludeRule(
        field_name="span_name",
        match_value="Internal.",
        match_operation="starts_with"
    ),
]

# Setup LangWatch to use the existing provider and apply exclude rules
langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    tracer_provider=existing_provider, # Use our existing provider
    span_exclude_rules=exclude_rules,
    # Important: Set this if you intend for LangWatch to use the existing provider
    # and want to silence the warning about not overriding it.
    ignore_global_tracer_provider_override_warning=True
)

# Now, create some spans using OTel API directly
tracer = existing_provider.get_tracer("my.app.tracer")

with tracer.start_as_current_span("GET /health_check") as span:
    span.set_attribute("http.method", "GET")
    # This span WILL go to Console Exporter
    # This span WILL NOT go to LangWatch Exporter

with tracer.start_as_current_span("Process User Request") as span:
    span.set_attribute("http.method", "POST")
    span.set_attribute("user.id", "user-123")
    # This span WILL go to Console Exporter
    # This span WILL ALSO go to LangWatch Exporter
```

Refer to the `SpanProcessingExcludeRule` definition for all available fields (`span_name`, `attribute`, `library_name`) and operations (`exact_match`, `contains`, `starts_with`, `ends_with`, `regex`).

## Debugging with Console Exporter

When developing or troubleshooting your OpenTelemetry integration, it's often helpful to see the spans being generated locally without sending them to a backend. The OpenTelemetry SDK provides a `ConsoleSpanExporter` for this purpose.

You can add it to your `TracerProvider` like this:

<CodeGroup>

```python Scenario 1: Managed Provider (Recommended)
import langwatch
import os
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

# Create your own TracerProvider
my_tracer_provider = TracerProvider()

# Add the ConsoleSpanExporter for debugging
my_tracer_provider.add_span_processor(
    SimpleSpanProcessor(ConsoleSpanExporter())
)

# Now, setup LangWatch with your pre-configured provider
langwatch.setup(
    tracer_provider=my_tracer_provider,
    # If you are providing your own tracer_provider that might be global,
    # you might want to set this to True if you see warnings.
    # ignore_global_tracer_provider_override_warning=True
)

# Spans created via LangWatch or directly via OTel API using this provider
# will now also be printed to the console.

# Example of creating a span to test
tracer = my_tracer_provider.get_tracer("my.debug.tracer")
with tracer.start_as_current_span("My Test Span"):
    print("This span should appear in the console.")
```

```python Scenario 2: Global Provider (Illustrative)
# Ensure necessary imports if running this snippet standalone
import os
import langwatch
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider # Needed for isinstance check
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

# In this case, you might try to get the global provider and add the exporter.
# Note: This can be less predictable if other libraries also manipulate the global provider.

langwatch.setup(
    ignore_global_tracer_provider_override_warning=True # If a global provider exists
)

# Try to get the globally configured TracerProvider
global_provider = trace.get_tracer_provider()

# Check if it's an SDK TracerProvider instance that we can add a processor to
if isinstance(global_provider, TracerProvider):
    global_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))

# Example span after attempting to modify global provider
# Note: get_tracer from the global trace module
global_otel_tracer = trace.get_tracer("my.app.tracer.global")

with global_otel_tracer.start_as_current_span("Test Span with Global Provider"):
    print("This span should appear in console if global provider was successfully modified.")
```

</CodeGroup>

This will print all created spans to your console

## Accessing the OpenTelemetry Span API

Since LangWatch spans wrap standard OTel spans, the `LangWatchSpan` object (returned by `langwatch.span()` or accessed via `langwatch.get_current_span()`) directly exposes the standard OpenTelemetry `trace.Span` API methods. This allows you to interact with the span using familiar OTel functions when needed for advanced use cases or compatibility.

You don't need to access a separate underlying object; just call the standard OTel methods directly on the `LangWatchSpan` instance:

```python
import langwatch
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

langwatch.setup() # Assume setup is done

with langwatch.span(name="MyInitialSpanName") as span:

    # Use standard OpenTelemetry Span API methods directly on span:
    span.set_attribute("my.custom.otel.attribute", "value")
    span.add_event("Specific OTel Event", {"detail": "more info"})
    span.set_status(Status(StatusCode.ERROR, description="Something went wrong"))
    span.update_name("MyUpdatedSpanName") # Renaming the span

    print(f"Is Recording? {span.is_recording()}")
    print(f"OTel Span Context: {span.get_span_context()}")

    # You can still use LangWatch-specific methods like update()
    span.update(langwatch_info="extra data")
```

This allows full flexibility, letting you use both LangWatch's structured data methods (`update`, etc.) and the standard OpenTelemetry span manipulation methods on the same object.

## Understanding `ignore_global_tracer_provider_override_warning`

If `langwatch.setup()` detects an existing *global* `TracerProvider` (one set via `opentelemetry.trace.set_tracer_provider()`) and you haven't explicitly passed a `tracer_provider` argument, LangWatch will log a warning by default. The warning states that it found a global provider and will attach its exporter to it rather than replacing it.

This warning exists because replacing a globally configured provider can sometimes break assumptions made by other parts of your application or libraries. However, in many cases, **attaching** the LangWatch exporter to the existing global provider is exactly the desired behavior.

If you are intentionally running LangWatch alongside an existing global OpenTelemetry setup and want LangWatch to simply add its exporter to that setup, you can silence this warning by setting:

```python
langwatch.setup(
    # ... other options
    ignore_global_tracer_provider_override_warning=True
)
```

---

# FILE: ./integration/python/tutorials/tracking-llm-costs.mdx

---
title: Tracking LLM Costs and Tokens
sidebarTitle: Costs & Tokens
description: Troubleshooting & adjusting cost tracking in LangWatch
keywords: LangWatch, cost tracking, token counting, debugging, troubleshooting, model costs, metrics, LLM spans
---

By default, LangWatch will automatically capture cost and token data for your LLM calls.

<img
  src="/images/costs/llm-costs-analytics.png"
  alt="LLM costs analytics graph"
/>

If you don't see costs being tracked or you see it being tracked as $0, this guide will help you identify and fix issues when cost and token tracking is not working as expected.

## Understanding Cost and Token Tracking

LangWatch calculates costs and tracks tokens by:

1. **Capturing model names** in LLM spans to match against cost tables
2. **Recording token metrics** (`prompt_tokens`, `completion_tokens`) in span data, or estimating when not available
3. **Mapping models to costs** using the pricing table in Settings > Model Costs

When any of these components are missing, you might see missing or $0 costs and tokens.

## Step 1: Verify LLM Span Data Capture

The most common issue is that your LLM spans aren't capturing the required data: model name, inputs, outputs, and token metrics.

### Check Your Current Spans

First, examine what data is being captured in your LLM spans. In the LangWatch dashboard:

1. Navigate to a trace that should have cost/token data
2. Click on the LLM span to inspect its details
3. Look for these key fields:
   - **Model**: Should show the model identifier (e.g., `openai/gpt-5`)
   - **Input/Output**: Should contain the actual messages sent and received
   - **Metrics**: Should show prompt + completion tokens

<img
  src="/images/costs/llm-span-details.png"
  alt="LLM span showing model, input/output, and token metrics"
/>

## Step 2: Fix Missing Model Information

If your spans don't show model information, the integration framework you're using might not be capturing it automatically.

### Solution A: Use Framework Auto-tracking

LangWatch provides auto-tracking for popular frameworks that automatically captures all the necessary data for cost calculation.

Check the **Integrations** menu in the sidebar to find specific setup instructions for your framework, which will show you how to properly configure automatic model and token tracking.

### Solution B: Manually Set Model Information

If auto-tracking isn't available for your framework, manually update the span with model information:

```python
import langwatch

# Mark the span as an LLM type span
@langwatch.span(type="llm")
def custom_llm_call(prompt: str):
    # Update the current span with model information
    langwatch.get_current_span().update(
        model="openai/gpt-5",  # Use the exact model identifier
        input=prompt,
    )

    # Simulate an LLM response
    response = your_custom_llm_client.generate(prompt)

    # Update with output and metrics
    langwatch.get_current_span().update(
        output=response.text,
        metrics={
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
        }
    )

    return response.text

@langwatch.trace()
def main_handler():
    result = custom_llm_call("Tell me about LangWatch")
    return result
```

### Solution C: Direct OpenTelemetry Integration (without LangWatch SDK)

If you're using a framework with built-in OpenTelemetry integration or community instrumentors, they should be following the [GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/). However, if the integration isn't capturing model information or token counts correctly, you can wrap your LLM calls with a custom span to patch the missing data:

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

def my_llm_call_with_tracking(prompt):
    with tracer.start_as_current_span("llm_call_wrapper") as span:
        # Set the required attributes for cost calculation
        span.set_attribute("gen_ai.operation.name", "chat")
        span.set_attribute("gen_ai.request.model", "gpt-5")

        # Your existing LLM call (may create its own spans)
        response = your_framework_llm_client.generate(prompt)

        # Extract and set token information if available
        if hasattr(response, 'usage'):
            span.set_attribute("gen_ai.usage.input_tokens", response.usage.input_tokens)
            span.set_attribute("gen_ai.usage.completion_tokens", response.usage.completion_tokens)

        return response
```

## Step 3: Configure Model Cost Mapping

If your model information is being captured but costs still show $0, you need to configure the cost mapping.

### Check Existing Model Costs

1. Go to **Settings > Model Costs** in your LangWatch dashboard
2. Look for your model in the list
3. Check if the regex pattern matches your model identifier

<img
  src="/images/costs/model-costs-settings.webp"
  alt="Model Costs settings page showing cost configuration"
/>

### Add Custom Model Costs

If your model isn't in the cost table, add it:

1. Click **"Add New Model"** in Settings > Model Costs
2. Configure the model entry:
   - **Model Name**: Descriptive name (e.g., "gpt-5")
   - **Regex Match Rule**: Pattern to match your model identifier (e.g., `^gpt-5$`)
   - **Input Cost**: Cost per input token (e.g., `0.0000004`)
   - **Output Cost**: Cost per output token (e.g., `0.0000016`)

### Common Model Identifier Patterns

Make sure your regex patterns match how the model names appear in your spans:

| Framework    | Model Identifier Format | Regex Pattern          |
| ------------ | ----------------------- | ---------------------- |
| OpenAI SDK   | `gpt-5`           | `^gpt-5$`        |
| Azure OpenAI | `gpt-5`           | `^gpt-5$`        |
| LangChain    | `openai/gpt-5`    | `^openai/gpt-5$` |
| Custom       | `my-custom-model-v1`    | `^my-custom-model-v1$` |


### Verification Checklist

After running your test, verify in the LangWatch dashboard:

✅ **Trace appears** in the dashboard \
✅ **LLM span shows model name** (e.g., `gpt-5`) \
✅ **Input and output are captured** \
✅ **Token metrics are present** (`prompt_tokens`, `completion_tokens`) \
✅ **Cost is calculated and displayed** (non-zero value)

## Common Issues and Solutions

### Issue: Auto-tracking not working

**Symptoms**: Spans appear but without model/metrics data

**Solutions**:

- Ensure `autotrack_*()` is called on an active trace
- Check that the client instance being tracked is the same one making calls
- Verify the integration is initialized correctly

### Issue: Custom models not calculating costs

**Symptoms**: Model name appears but cost remains $0

**Solutions**:

- Check regex pattern in Model Costs settings
- Ensure the pattern exactly matches your model identifier
- Verify input and output costs are configured correctly

### Issue: Token counts are 0 but model is captured

**Symptoms**: Model name is present but token metrics are missing

**Solutions**:

- Manually set metrics in span updates if not automatically captured
- Check if your LLM provider returns usage information
- Ensure the integration is extracting token counts from responses

### Issue: Framework with OpenTelemetry not capturing model data

**Symptoms**: Using a framework with OpenTelemetry integration that's not capturing model names or token counts

**Solutions**:
- Follow the guidance in [Solution C: Framework with OpenTelemetry Integration](#solution-c-framework-with-opentelemetry-integration) above
- Wrap your LLM calls with custom spans to patch missing data


## Getting Help

If you're still experiencing issues after following this guide:

1. **Check the LangWatch logs** for any error messages
2. **Verify your API key** and endpoint configuration
3. **Share a minimal reproduction** with the specific framework you're using
4. **Contact support** at [support@langwatch.ai](mailto:support@langwatch.ai) with:
   - Your integration method (SDK, OpenTelemetry, etc.)
   - Framework versions
   - Sample span data from the dashboard

Cost and token tracking should work reliably once the model information and metrics are properly captured. Most issues stem from missing model identifiers or incorrect cost table configuration.

---

# FILE: ./integration/typescript/reference.mdx

---
title: TypeScript SDK API Reference
sidebarTitle: TypeScript/JS
description: LangWatch TypeScript SDK API reference
---

## Setup

### `setupLangWatch()`

Initializes the LangWatch client for Node.js or browser environments, enabling data collection and tracing for your LLM application. This is typically the first function you'll call when integrating LangWatch.

<CodeGroup>
```typescript Node.js Setup
import { setupLangWatch } from "langwatch/node";

await setupLangWatch({
  apiKey: process.env.LANGWATCH_API_KEY, // Optional, defaults to process.env.LANGWATCH_API_KEY
  endpoint: process.env.LANGWATCH_ENDPOINT_URL // Optional, defaults to LangWatch Cloud
});
```

```typescript Browser Setup
import { setupLangWatch } from "langwatch/browser";

await setupLangWatch({
  apiKey: "your-api-key",
  endpoint: "https://app.langwatch.ai"
});
```

```typescript Advanced Setup
import { setupLangWatch } from "langwatch/node";
import { SpanProcessor } from "@opentelemetry/sdk-trace-base";
import { SpanProcessingExcludeRule } from "langwatch";

// Example: Configure with custom span processors and exclude rules
const customSpanProcessor: SpanProcessor = /* your custom processor */;
const excludeRules: SpanProcessingExcludeRule[] = [
  {
    fieldName: "span_name",
    matchValue: "InternalHealthCheck",
    matchOperation: "exact_match"
  }
];

await setupLangWatch({
  apiKey: process.env.LANGWATCH_API_KEY,
  endpoint: process.env.LANGWATCH_ENDPOINT_URL,
  otelSpanProcessors: [customSpanProcessor],
  otelSpanProcessingExcludeRules: excludeRules,
  skipOpenTelemetryAutomaticSetup: false,
  disableAutomaticInputCapture: false,
  disableAutomaticOutputCapture: false
});
```
</CodeGroup>

<ParamField path="apiKey" type="Optional<string>" default="process.env.LANGWATCH_API_KEY">
  Your LangWatch API key. It's recommended to set this via an environment variable (e.g., `LANGWATCH_API_KEY`) and retrieve it using `process.env.LANGWATCH_API_KEY`.
</ParamField>

<ParamField path="endpoint" type="Optional<string>" default="https://app.langwatch.ai">
  The URL of the LangWatch backend where traces will be sent. Defaults to the LangWatch Cloud service. For self-hosted instances, you'll need to provide this.
</ParamField>

<ParamField path="otelSpanProcessors" type="Optional<SpanProcessor[]>" default="[]">
  Additional OpenTelemetry span processors to add to the SDK. These will be added after the LangWatch SDK has been initialized. **Not supported in browser environments.**
</ParamField>

<ParamField path="otelSpanProcessingExcludeRules" type="Optional<SpanProcessingExcludeRule[]>" default="[]">
  Rules to filter out specific spans from being exported to LangWatch, based on span name or attributes. See `SpanProcessingExcludeRule` for details.
</ParamField>

<ParamField path="skipOpenTelemetryAutomaticSetup" type="Optional<boolean>" default="false">
  If `true`, skips the automatic setup of the OpenTelemetry SDK. You will need to setup the OpenTelemetry SDK yourself, and ensure that a SpanProcessor is added that will send traces to the LangWatch API.
</ParamField>

<ParamField path="disableAutomaticInputCapture" type="Optional<boolean>" default="false">
  If `true`, disables the automatic capture of input data in spans.
</ParamField>

<ParamField path="disableAutomaticOutputCapture" type="Optional<boolean>" default="false">
  If `true`, disables the automatic capture of output data in spans.
</ParamField>

## Tracing

### `getLangWatchTracer()`

Returns a LangWatch tracer instance that provides enhanced tracing capabilities for LLM applications.

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service", "1.0.0");
```

<ParamField path="name" type="string" required>
  The name of the tracer/service.
</ParamField>

<ParamField path="version" type="Optional<string>" default="undefined">
  The version of the tracer/service.
</ParamField>

**Returns**

<ResponseField name="tracer" type="LangWatchTracer">
  A `LangWatchTracer` instance with enhanced methods for LLM observability.
</ResponseField>

### `LangWatchTracer`

The `LangWatchTracer` extends the standard OpenTelemetry `Tracer` with additional methods for LLM observability.

#### Methods

<ParamField path="startSpan" type="(name: string, options?: SpanOptions, context?: Context) => LangWatchSpan">
  Starts a new `LangWatchSpan` without setting it on context. This method does NOT modify the current Context.
</ParamField>

<ParamField path="startActiveSpan" type="(name: string, fn: (span: LangWatchSpan) => T) => T">
  Starts a new `LangWatchSpan` and calls the given function passing it the created span as first argument. The new span gets set in context and this context is activated for the duration of the function call.
</ParamField>

<ParamField path="withActiveSpan" type="(name: string, fn: (span: LangWatchSpan) => Promise<T> | T) => Promise<T>">
  Starts a new `LangWatchSpan`, runs the provided async function, and automatically handles error recording, status setting, and span ending. This is a safer and more ergonomic alternative to manually using try/catch/finally blocks.
</ParamField>

### `LangWatchSpan`

The `LangWatchSpan` extends the standard OpenTelemetry `Span` with additional methods for LLM observability.

#### GenAI Event Methods

<ParamField path="addGenAISystemMessageEvent" type="(body: LangWatchSpanGenAISystemMessageEventBody, system?: string, attributes?: Record<string, AttributeValue>) => this">
  Adds a GenAI system message event to the span. This logs a system/instruction message sent to the model.
</ParamField>

<ParamField path="addGenAIUserMessageEvent" type="(body: LangWatchSpanGenAIUserMessageEventBody, system?: string, attributes?: Record<string, AttributeValue>) => this">
  Adds a GenAI user message event to the span. This logs a user/customer message sent to the model.
</ParamField>

<ParamField path="addGenAIAssistantMessageEvent" type="(body: LangWatchSpanGenAIAssistantMessageEventBody, system?: string, attributes?: Record<string, AttributeValue>) => this">
  Adds a GenAI assistant message event to the span. This logs an assistant/bot response, including tool calls if present.
</ParamField>

<ParamField path="addGenAIToolMessageEvent" type="(body: LangWatchSpanGenAIToolMessageEventBody, system?: string, attributes?: Record<string, AttributeValue>) => this">
  Adds a GenAI tool message event to the span. This logs a message from a tool/function invoked by the assistant.
</ParamField>

<ParamField path="addGenAIChoiceEvent" type="(body: LangWatchSpanGenAIChoiceEventBody, system?: string, attributes?: Record<string, AttributeValue>) => this">
  Adds a GenAI choice event to the span. This logs a model output choice, including finish reason and message content.
</ParamField>

#### Span Configuration Methods

<ParamField path="setType" type="(type: SpanType) => this">
  Set the type of the span (e.g., 'llm', 'rag', 'tool', etc). This is used for downstream filtering and analytics.
</ParamField>

<ParamField path="setRequestModel" type="(model: string) => this">
  Set the request model name for the span. This is typically the model name sent in the API request (e.g., 'gpt-5', 'claude-3').
</ParamField>

<ParamField path="setResponseModel" type="(model: string) => this">
  Set the response model name for the span. This is the model name returned in the API response, if different from the request.
</ParamField>

<ParamField path="setRAGContexts" type="(ragContexts: LangWatchSpanRAGContext[]) => this">
  Set multiple RAG contexts for the span. Use this to record all retrieved documents/chunks used as context for a generation.
</ParamField>

<ParamField path="setRAGContext" type="(ragContext: LangWatchSpanRAGContext) => this">
  Set a single RAG context for the span. Use this if only one context was retrieved.
</ParamField>

<ParamField path="setMetrics" type="(metrics: LangWatchSpanMetrics) => this">
  Set the metrics for the span.
</ParamField>

<ParamField path="setSelectedPrompt" type="(prompt: Prompt) => this">
  Set the selected prompt for the span. This will attach this prompt to the trace. If this is set on multiple spans, the last one will be used.
</ParamField>

<ParamField path="setInput" type="(input: unknown) => this">
  Record the input to the span as a JSON-serializable value. The input is stringified and stored as a span attribute for later analysis.
</ParamField>

<ParamField path="setInputString" type="(input: string) => this">
  Record the input to the span as a plain string. Use this for raw text prompts or queries.
</ParamField>

<ParamField path="setOutput" type="(output: unknown) => this">
  Record the output from the span as a JSON-serializable value. The output is stringified and stored as a span attribute for later analysis.
</ParamField>

<ParamField path="setOutputString" type="(output: string) => this">
  Record the output from the span as a plain string. Use this for raw text completions or responses.
</ParamField>

<ParamField path="setOutputEvaluation" type="(guardrail: boolean, output: EvaluationResultModel) => this">
  Set the evaluation output for the span.
</ParamField>

<ParamField path="recordEvaluation" type="(details: RecordedEvaluationDetails, attributes?: Attributes) => this">
  Record the evaluation result for the span.
</ParamField>

## Evaluation

### `runEvaluation()`

Runs an evaluation using a saved evaluator or LangEvals evaluator.

```typescript
import { runEvaluation } from "langwatch/evaluation";

// Using a saved evaluator
const result = await runEvaluation({
  slug: "sentiment-analyzer",
  data: {
    input: "I love this product!",
    output: "Positive sentiment detected."
  },
  asGuardrail: false
});

// Using a LangEvals evaluator
const result = await runEvaluation({
  evaluator: "sentiment",
  data: {
    input: "I love this product!",
    output: "Positive sentiment detected."
  },
  settings: {
    threshold: 0.5
  }
});
```

<ParamField path="details" type="EvaluationDetails" required>
  The evaluation details. Can be either a `SavedEvaluationDetails` (using a saved evaluator) or `LangEvalsEvaluationDetails` (using a LangEvals evaluator).
</ParamField>

**Returns**

<ResponseField name="result" type="SingleEvaluationResult">
  The evaluation result containing status, passed, score, and other details.
</ResponseField>

### `recordEvaluation()`

Records an evaluation result manually.

```typescript
import { recordEvaluation } from "langwatch/evaluation";

recordEvaluation({
  name: "Custom Evaluation",
  type: "custom",
  isGuardrail: false,
  status: "processed",
  passed: true,
  score: 0.95,
  label: "Excellent",
  details: "All checks passed.",
  cost: 0.001
});
```

<ParamField path="details" type="RecordedEvaluationDetails" required>
  The evaluation details to record.
</ParamField>

<ParamField path="attributes" type="Optional<Attributes>" default="undefined">
  Additional OpenTelemetry attributes to add to the evaluation span.
</ParamField>

## Prompt Management

### `getPrompt()`

Retrieves a prompt from the LangWatch platform.

```typescript
import { getPrompt } from "langwatch/prompt";

// Get a prompt without variables
const prompt = await getPrompt("prompt-id");

// Get a prompt with variables (returns compiled prompt)
const compiledPrompt = await getPrompt("prompt-id", {
  user_name: "John",
  product_name: "LangWatch"
});
```

<ParamField path="id" type="string" required>
  The ID of the prompt to retrieve.
</ParamField>

<ParamField path="variables" type="Optional<TemplateVariables>" default="undefined">
  Template variables to compile the prompt with. If provided, returns a compiled prompt.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt | CompiledPrompt">
  The prompt or compiled prompt object.
</ResponseField>

### `getPromptVersion()`

Retrieves a specific version of a prompt from the LangWatch platform.

```typescript
import { getPromptVersion } from "langwatch";

// Get a prompt version without variables
const promptVersion = await getPromptVersion("prompt-id", "version-id");

// Get a prompt version with variables (returns compiled prompt)
const compiledPromptVersion = await getPromptVersion("prompt-id", "version-id", {
  user_name: "John",
  product_name: "LangWatch"
});
```

<ParamField path="id" type="string" required>
  The ID of the prompt to retrieve.
</ParamField>

<ParamField path="versionId" type="string" required>
  The ID of the specific version to retrieve.
</ParamField>

<ParamField path="variables" type="Optional<TemplateVariables>" default="undefined">
  Template variables to compile the prompt with. If provided, returns a compiled prompt.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt | CompiledPrompt">
  The prompt version or compiled prompt object.
</ResponseField>

**Throws**

<ResponseField name="error" type="Error">
  Throws an error if the specified prompt version is not found.
</ResponseField>

## Processors

### `FilterableBatchSpanProcessor`

A span processor that filters spans before processing them.

```typescript
import { FilterableBatchSpanProcessor } from "langwatch/observability";
import { LangWatchExporter } from "langwatch/observability";

const processor = new FilterableBatchSpanProcessor(
  new LangWatchExporter(), // Uses environment variables
  [
    {
      fieldName: "span_name",
      matchValue: "health-check",
      matchOperation: "exact_match"
    }
  ]
);
```

<ParamField path="exporter" type="SpanExporter" required>
  The span exporter to use.
</ParamField>

<ParamField path="excludeRules" type="SpanProcessingExcludeRule[]" required>
  Rules to exclude spans from processing.
</ParamField>

## LangChain Integration

### `LangWatchCallbackHandler`

A LangChain callback handler that automatically traces LangChain operations and integrates them with LangWatch.

```typescript
import { LangWatchCallbackHandler } from "langwatch/observability/instrumentation/langchain";
import { ChatOpenAI } from "@langchain/openai";

const handler = new LangWatchCallbackHandler();
const llm = new ChatOpenAI({
  callbacks: [handler]
});

// All operations will now be automatically traced
const response = await llm.invoke("Hello, world!");
```

The `LangWatchCallbackHandler` automatically:
- Creates spans for LLM calls, chains, tools, and retrievers
- Captures input/output data
- Sets appropriate span types and metadata
- Handles errors and status codes
- Integrates with the LangWatch tracing system

### `convertFromLangChainMessages`

Utility function to convert LangChain messages to a format compatible with LangWatch GenAI events.

```typescript
import { convertFromLangChainMessages } from "langwatch/observability/instrumentation/langchain";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";

const messages = [
  new SystemMessage("You are a helpful assistant."),
  new HumanMessage("Hello!")
];

const convertedMessages = convertFromLangChainMessages(messages);
// Use with span.addGenAIUserMessageEvent(), etc.
```

## Exporters

### `LangWatchExporter`

A LangWatch exporter for sending traces to the LangWatch platform. Extends the OpenTelemetry OTLP HTTP trace exporter with proper authentication and metadata headers.

```typescript
import { LangWatchExporter } from "langwatch/observability";

// Using environment variables/fallback configuration
const exporter = new LangWatchExporter();

// Using custom API key and endpoint
const exporter = new LangWatchExporter('api-key', 'https://custom.langwatch.com');
```

<ParamField path="apiKey" type="Optional<string>" default="getApiKey()">
  Optional API key for LangWatch authentication. If not provided, will use environment variables or fallback configuration.
</ParamField>

<ParamField path="endpointURL" type="Optional<string>" default="getEndpoint()">
  Optional custom endpoint URL for LangWatch ingestion. If not provided, will use environment variables or fallback configuration.
</ParamField>

The exporter automatically configures:
- Authorization headers using the provided API key or environment variables/fallback
- SDK version and language identification headers
- Proper endpoint configuration for LangWatch ingestion using provided URL or environment variables/fallback

## Semantic Conventions

The `langwatch/observability` module provides semantic convention constants for consistent attribute naming and event types across the LangWatch ecosystem.

### Attributes

```typescript
import { semconv } from "langwatch/observability";

// LangWatch-specific attributes
semconv.ATTR_LANGWATCH_INPUT           // "langwatch.input"
semconv.ATTR_LANGWATCH_OUTPUT          // "langwatch.output"
semconv.ATTR_LANGWATCH_SPAN_TYPE       // "langwatch.span.type"
semconv.ATTR_LANGWATCH_RAG_CONTEXTS    // "langwatch.contexts"
semconv.ATTR_LANGWATCH_METRICS         // "langwatch.metrics"
semconv.ATTR_LANGWATCH_SDK_VERSION     // "langwatch.sdk.version"
semconv.ATTR_LANGWATCH_SDK_NAME        // "langwatch.sdk.name"
semconv.ATTR_LANGWATCH_SDK_LANGUAGE    // "langwatch.sdk.language"
semconv.ATTR_LANGWATCH_TIMESTAMPS      // "langwatch.timestamps"
semconv.ATTR_LANGWATCH_EVALUATION_CUSTOM // "langwatch.evaluation.custom"
semconv.ATTR_LANGWATCH_PARAMS          // "langwatch.params"
semconv.ATTR_LANGWATCH_CUSTOMER_ID     // "langwatch.customer.id"
semconv.ATTR_LANGWATCH_THREAD_ID       // "langwatch.thread.id"
semconv.ATTR_LANGWATCH_STREAMING       // "langwatch.gen_ai.streaming"
semconv.ATTR_LANGWATCH_PROMPT_ID       // "langwatch.prompt.id"
semconv.ATTR_LANGWATCH_PROMPT_VERSION_ID // "langwatch.prompt.version.id"
semconv.ATTR_LANGWATCH_PROMPT_VARIABLES // "langwatch.prompt.variables"
semconv.ATTR_LANGWATCH_PROMPT_SELECTED_ID // "langwatch.prompt.selected.id"
semconv.ATTR_LANGWATCH_PROMPT_VERSION_NUMBER // "langwatch.prompt.version.number"

// LangChain-specific attributes
semconv.ATTR_LANGWATCH_LANGCHAIN_TAGS  // "langwatch.langchain.tags"
semconv.ATTR_LANGWATCH_LANGCHAIN_EVENT_NAME // "langwatch.langchain.event_name"
semconv.ATTR_LANGWATCH_LANGCHAIN_RUN_ID // "langwatch.langchain.run.id"
semconv.ATTR_LANGWATCH_LANGCHAIN_RUN_TAGS // "langwatch.langchain.run.tags"
semconv.ATTR_LANGWATCH_LANGCHAIN_RUN_TYPE // "langwatch.langchain.run.type"
semconv.ATTR_LANGWATCH_LANGCHAIN_RUN_METADATA // "langwatch.langchain.run.metadata"
semconv.ATTR_LANGWATCH_LANGCHAIN_RUN_EXTRA_PARAMS // "langwatch.langchain.run.extra_params"
semconv.ATTR_LANGWATCH_LANGCHAIN_RUN_PARENT_ID // "langwatch.langchain.run.parent.id"
```

### Events

```typescript
import { semconv } from "langwatch/observability";

// GenAI event names
semconv.LOG_EVNT_GEN_AI_SYSTEM_MESSAGE    // "gen.ai.system_message"
semconv.LOG_EVNT_GEN_AI_USER_MESSAGE      // "gen.ai.user_message"
semconv.LOG_EVNT_GEN_AI_ASSISTANT_MESSAGE // "gen.ai.assistant_message"
semconv.LOG_EVNT_GEN_AI_TOOL_MESSAGE      // "gen.ai.tool_message"
semconv.LOG_EVNT_GEN_AI_CHOICE            // "gen.ai.choice"

// LangChain event names
semconv.EVNT_LANGWATCH_LANGCHAIN_CALLBACK // "langwatch.langchain.callback"
```

### Values

```typescript
import { semconv } from "langwatch/observability";

// GenAI system identifiers
semconv.VAL_GEN_AI_SYSTEM_OPENAI         // "openai"
semconv.VAL_GEN_AI_SYSTEM_ANTHROPIC      // "anthropic"
semconv.VAL_GEN_AI_SYSTEM_AWS_BEDROCK   // "aws.bedrock"
semconv.VAL_GEN_AI_SYSTEM_AZURE_AI_OPENAI // "azure.ai.openai"
semconv.VAL_GEN_AI_SYSTEM_GCP_GEMINI    // "gcp.gemini"
semconv.VAL_GEN_AI_SYSTEM_COHERE        // "cohere"
semconv.VAL_GEN_AI_SYSTEM_GROQ          // "groq"
semconv.VAL_GEN_AI_SYSTEM_MISTRAL_AI    // "mistral_ai"
semconv.VAL_GEN_AI_SYSTEM_PERPLEXITY    // "perplexity"
semconv.VAL_GEN_AI_SYSTEM_XAI           // "xai"

// GenAI finish reasons
semconv.VAL_GEN_AI_FINISH_REASON_STOP           // "stop"
semconv.VAL_GEN_AI_FINISH_REASON_LENGTH         // "length"
semconv.VAL_GEN_AI_FINISH_REASON_CONTENT_FILTER // "content_filter"
semconv.VAL_GEN_AI_FINISH_REASON_ERROR          // "error"
semconv.VAL_GEN_AI_FINISH_REASON_TOOL_CALLS    // "tool_calls"
```

## Core Data Types

### `SpanType`

Supported types of spans for LangWatch observability:

```typescript
type SpanType =
  | "span"
  | "llm"
  | "chain"
  | "tool"
  | "agent"
  | "guardrail"
  | "evaluation"
  | "rag"
  | "prompt"
  | "workflow"
  | "component"
  | "module"
  | "server"
  | "client"
  | "producer"
  | "consumer"
  | "task"
  | "unknown";
```

### `LangWatchSpanRAGContext`

Context for a RAG (Retrieval-Augmented Generation) span.

<ResponseField name="document_id" type="string" required>
  Unique identifier for the source document.
</ResponseField>

<ResponseField name="chunk_id" type="string" required>
  Unique identifier for the chunk within the document.
</ResponseField>

<ResponseField name="content" type="string" required>
  The actual content of the chunk provided to the model.
</ResponseField>

### `LangWatchSpanMetrics`

Metrics for a LangWatch span.

<ResponseField name="promptTokens" type="Optional<number>">
  The number of prompt tokens used.
</ResponseField>

<ResponseField name="completionTokens" type="Optional<number>">
  The number of completion tokens used.
</ResponseField>

<ResponseField name="cost" type="Optional<number>">
  The cost of the span.
</ResponseField>

### `SpanProcessingExcludeRule`

Defines a rule to filter out spans before they are exported to LangWatch.

<ResponseField name="fieldName" type="'span_name'" required>
  The field of the span to match against. Currently, only `"span_name"` is supported.
</ResponseField>

<ResponseField name="matchValue" type="string" required>
  The value to match for the specified `fieldName`.
</ResponseField>

<ResponseField name="matchOperation" type="'includes' | 'exact_match' | 'starts_with' | 'ends_with'" required>
  The operation to use for matching.
</ResponseField>

### `LangWatchSpanGenAISystemMessageEventBody`

Body for a system message event in a GenAI span.

<ResponseField name="content" type="Optional<string>">
  Content of the system message.
</ResponseField>

<ResponseField name="role" type="Optional<'system' | 'instruction'>">
  Role of the message (system or instruction).
</ResponseField>

### `LangWatchSpanGenAIUserMessageEventBody`

Body for a user message event in a GenAI span.

<ResponseField name="content" type="Optional<string>">
  Content of the user message.
</ResponseField>

<ResponseField name="role" type="Optional<'user' | 'customer'>">
  Role of the message (user or customer).
</ResponseField>

### `LangWatchSpanGenAIAssistantMessageEventBody`

Body for an assistant message event in a GenAI span.

<ResponseField name="content" type="Optional<string>">
  Content of the assistant message.
</ResponseField>

<ResponseField name="role" type="Optional<'assistant' | 'bot'>">
  Role of the message (assistant or bot).
</ResponseField>

<ResponseField name="tool_calls" type="Optional<ToolCall[]>">
  Tool calls made by the assistant.
</ResponseField>

### `LangWatchSpanGenAIToolMessageEventBody`

Body for a tool message event in a GenAI span.

<ResponseField name="content" type="Optional<string>">
  Content of the tool message.
</ResponseField>

<ResponseField name="id" type="string" required>
  Tool call identifier.
</ResponseField>

<ResponseField name="role" type="Optional<'tool' | 'function'>">
  Role of the message (tool or function).
</ResponseField>

### `LangWatchSpanGenAIChoiceEventBody`

Body for a choice event in a GenAI span.

<ResponseField name="finish_reason" type="string" required>
  Reason the generation finished.
</ResponseField>

<ResponseField name="index" type="number" required>
  Index of the choice.
</ResponseField>

<ResponseField name="message" type="Optional<Message>">
  Message content for the choice.
</ResponseField>

### `RecordedEvaluationDetails`

Details for recording an evaluation.

<ResponseField name="evaluationId" type="Optional<string>">
  Unique identifier for the evaluation.
</ResponseField>

<ResponseField name="name" type="string" required>
  Name of the evaluation.
</ResponseField>

<ResponseField name="type" type="Optional<string>">
  Type of the evaluation.
</ResponseField>

<ResponseField name="isGuardrail" type="Optional<boolean>">
  Whether this is a guardrail evaluation.
</ResponseField>

<ResponseField name="status" type="Optional<'processed' | 'skipped' | 'error'>">
  Status of the evaluation.
</ResponseField>

<ResponseField name="passed" type="Optional<boolean>">
  Whether the evaluation passed.
</ResponseField>

<ResponseField name="score" type="Optional<number>">
  Score of the evaluation.
</ResponseField>

<ResponseField name="label" type="Optional<string>">
  Label for the evaluation result.
</ResponseField>

<ResponseField name="details" type="Optional<string>">
  Additional details about the evaluation.
</ResponseField>

<ResponseField name="cost" type="Optional<number | { currency: string; amount: number }>">
  Cost of the evaluation.
</ResponseField>

<ResponseField name="error" type="Optional<Error>">
  Error that occurred during evaluation.
</ResponseField>

<ResponseField name="timestamps" type="Optional<{ startedAtUnixMs: number; finishedAtUnixMs: number }>">
  Timestamps for the evaluation.
</ResponseField>

### `EvaluationResultModel`

Result model for evaluation output.

<ResponseField name="status" type="'processed' | 'skipped' | 'error'">
  Status of the evaluation.
</ResponseField>

<ResponseField name="passed" type="Optional<boolean>">
  Whether the evaluation passed.
</ResponseField>

<ResponseField name="score" type="Optional<number>">
  Score of the evaluation.
</ResponseField>

<ResponseField name="details" type="Optional<string>">
  Additional details about the evaluation.
</ResponseField>

<ResponseField name="label" type="Optional<string>">
  Label for the evaluation result.
</ResponseField>

<ResponseField name="cost" type="Optional<{ currency: string; amount: number }>">
  Cost of the evaluation.
</ResponseField>

<ResponseField name="raw_response" type="Optional<any>">
  Raw response from the evaluation.
</ResponseField>

### `EvaluationDetails`

Union type for evaluation details.

```typescript
type EvaluationDetails = SavedEvaluationDetails | LangEvalsEvaluationDetails<EvaluatorTypes>;
```

### `SavedEvaluationDetails`

Details for using a saved evaluator.

<ResponseField name="slug" type="string" required>
  The slug of the saved evaluator.
</ResponseField>

<ResponseField name="data" type="BasicEvaluationData | Record<string, unknown>" required>
  The evaluation data.
</ResponseField>

<ResponseField name="name" type="Optional<string>">
  Name for the evaluation.
</ResponseField>

<ResponseField name="contexts" type="Optional<RAGChunk[] | string[]>">
  Context data for the evaluation.
</ResponseField>

<ResponseField name="conversation" type="Optional<Conversation>">
  Conversation data for the evaluation.
</ResponseField>

<ResponseField name="asGuardrail" type="Optional<boolean>">
  Whether to run as a guardrail.
</ResponseField>

<ResponseField name="settings" type="Optional<Record<string, unknown>>">
  Settings for the evaluator.
</ResponseField>

### `LangEvalsEvaluationDetails`

Details for using a LangEvals evaluator.

<ResponseField name="evaluator" type="EvaluatorTypes" required>
  The LangEvals evaluator type.
</ResponseField>

<ResponseField name="data" type="BasicEvaluationData | Record<string, unknown>" required>
  The evaluation data.
</ResponseField>

<ResponseField name="name" type="Optional<string>">
  Name for the evaluation.
</ResponseField>

<ResponseField name="contexts" type="Optional<RAGChunk[] | string[]>">
  Context data for the evaluation.
</ResponseField>

<ResponseField name="conversation" type="Optional<Conversation>">
  Conversation data for the evaluation.
</ResponseField>

<ResponseField name="asGuardrail" type="Optional<boolean>">
  Whether to run as a guardrail.
</ResponseField>

<ResponseField name="settings" type="Optional<Evaluators[T]['settings']>">
  Settings for the evaluator.
</ResponseField>

### `BasicEvaluationData`

Basic evaluation data structure.

<ResponseField name="input" type="Optional<string>">
  Input text for evaluation.
</ResponseField>

<ResponseField name="output" type="Optional<string>">
  Output text for evaluation.
</ResponseField>

<ResponseField name="expected_output" type="Optional<unknown>">
  Expected output for evaluation.
</ResponseField>

<ResponseField name="contexts" type="Optional<RAGChunk[] | string[]>">
  Context data for evaluation.
</ResponseField>

<ResponseField name="expected_contexts" type="Optional<RAGChunk[] | string[]>">
  Expected context data for evaluation.
</ResponseField>

<ResponseField name="conversation" type="Optional<Conversation>">
  Conversation data for evaluation.
</ResponseField>

### `SingleEvaluationResult`

Result of an evaluation.

<ResponseField name="status" type="'processed' | 'skipped' | 'error'">
  Status of the evaluation.
</ResponseField>

<ResponseField name="passed" type="Optional<boolean>">
  Whether the evaluation passed.
</ResponseField>

<ResponseField name="score" type="Optional<number>">
  Score of the evaluation.
</ResponseField>

<ResponseField name="label" type="Optional<string>">
  Label for the evaluation result.
</ResponseField>

<ResponseField name="details" type="Optional<string>">
  Additional details about the evaluation.
</ResponseField>

<ResponseField name="error" type="Optional<string>">
  Error message if the evaluation failed.
</ResponseField>

<ResponseField name="cost" type="Optional<{ currency: string; amount: number }>">
  Cost of the evaluation.
</ResponseField>

### `Prompt`

A prompt object retrieved from the LangWatch platform.

<ResponseField name="id" type="string">
  Unique identifier for the prompt.
</ResponseField>

<ResponseField name="name" type="string">
  Name of the prompt.
</ResponseField>

<ResponseField name="updatedAt" type="string">
  Last update timestamp.
</ResponseField>

<ResponseField name="version" type="number">
  Version number.
</ResponseField>

<ResponseField name="versionId" type="string">
  Version identifier.
</ResponseField>

<ResponseField name="versionCreatedAt" type="string">
  Version creation timestamp.
</ResponseField>

<ResponseField name="model" type="string">
  Model used for the prompt.
</ResponseField>

<ResponseField name="prompt" type="string">
  The prompt template.
</ResponseField>

<ResponseField name="messages" type="Array">
  Array of message objects.
</ResponseField>

<ResponseField name="response_format" type="Optional<object>">
  Response format configuration.
</ResponseField>

### `CompiledPrompt`

A compiled prompt with variables substituted.

<ResponseField name="content" type="string">
  The compiled prompt content with variables substituted.
</ResponseField>

<ResponseField name="original" type="Prompt">
  The original prompt object.
</ResponseField>

### `TemplateVariables`

Template variables for prompt compilation.

```typescript
type TemplateVariables = Record<string, string | number | boolean | object | null>;
```

### `PromptCompilationError`

Error thrown when prompt compilation fails.

<ResponseField name="template" type="string">
  The template that failed to compile.
</ResponseField>

<ResponseField name="originalError" type="Optional<any>">
  The original compilation error.
</ResponseField>

## Usage Examples

### Basic Tracing

```typescript
import { setup } from "langwatch/node";
import { getLangWatchTracer } from "langwatch";

await setup({ apiKey: process.env.LANGWATCH_API_KEY });

const tracer = getLangWatchTracer("my-service");

await tracer.withActiveSpan("process-request", async (span) => {
  span.setType("llm");
  span.setRequestModel("gpt-5");

  // Your LLM call here
  const response = await openai.chat.completions.create({
    model: "gpt-5",
    messages: [{ role: "user", content: "Hello!" }]
  });

  span.setOutput(response.choices[0].message.content);
  span.setMetrics({
    promptTokens: response.usage?.prompt_tokens,
    completionTokens: response.usage?.completion_tokens
  });
});
```

### RAG Operations

```typescript
await tracer.withActiveSpan("rag-query", async (span) => {
  span.setType("rag");

  // Retrieve documents
  const documents = await vectorStore.similaritySearch("query", 5);

  // Set RAG contexts
  span.setRAGContexts(documents.map(doc => ({
    document_id: doc.metadata.id,
    chunk_id: doc.metadata.chunk_id,
    content: doc.pageContent
  })));

  // Generate response
  const response = await llm.generate([documents, "query"]);
  span.setOutput(response);
});
```

### Using Semantic Conventions

```typescript
import { semconv } from "langwatch/observability";
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

await tracer.withActiveSpan("llm-call", async (span) => {
  // Use semantic convention attributes for consistency
  span.setAttribute(semconv.ATTR_LANGWATCH_SPAN_TYPE, "llm");
  span.setAttribute(semconv.ATTR_LANGWATCH_STREAMING, false);

  // Add GenAI events with proper system identifiers
  span.addGenAISystemMessageEvent(
    { content: "You are a helpful assistant." },
    semconv.VAL_GEN_AI_SYSTEM_OPENAI
  );

  span.addGenAIUserMessageEvent(
    { content: "Hello!" },
    semconv.VAL_GEN_AI_SYSTEM_OPENAI
  );

  // Record events with proper event names
  span.addEvent(semconv.LOG_EVNT_GEN_AI_ASSISTANT_MESSAGE, {
    content: "Hello! How can I help you today?"
  });
});
```

---

# FILE: ./integration/typescript/guide.mdx

---
title: TypeScript Integration Guide
sidebarTitle: TypeScript/JS
description: LangWatch TypeScript SDK integration guide
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

Integrate LangWatch into your TypeScript/JavaScript application to start observing your LLM interactions. This guide covers the setup and basic usage of the LangWatch TypeScript SDK.

## Get your LangWatch API Key

First, you need a LangWatch API key. Sign up at [app.langwatch.ai](https://app.langwatch.ai) and find your API key in your project settings. The SDK will automatically use the `LANGWATCH_API_KEY` environment variable if it is set.

## Start Instrumenting

First, ensure you have the SDK installed:

```bash
npm install langwatch @opentelemetry/sdk-node @opentelemetry/context-async-hooks
```

Initialize LangWatch early in your Node.js application, typically where you configure services:

```typescript
import { setupLangWatch } from "langwatch/node";

await setupLangWatch(); // This will setup the SDK, and setup the observability SDK.
```

<Note>
  If you have an existing OpenTelemetry setup in your application, please see the [Already using OpenTelemetry?](#already-using-opentelemetry) section below.
</Note>

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.

## Integrations

LangWatch offers seamless integrations with a variety of popular TypeScript libraries and frameworks. These integrations provide automatic instrumentation, capturing relevant data from your LLM applications with minimal setup.

Below is a list of currently supported integrations. Click on each to learn more about specific setup instructions and available features:

{/* - [AWS Bedrock](/integration/typescript/integrations/aws-bedrock) */}
{/* - [Azure AI](/integration/typescript/integrations/azure-ai) */}
- [Langchain](/integration/typescript/integrations/langchain)
{/* - [LangGraph](/integration/typescript/integrations/langgraph) */}
{/* - [Mastra](/integration/typescript/integrations/mastra) */}
{/* - [n8n](/integration/typescript/integrations/n8n) */}
{/* - [OpenAI Agents](/integration/typescript/integrations/open-ai-agents) */}
{/* - [OpenAI Azure](/integration/typescript/integrations/open-ai-azure) */}
{/* - [OpenAI](/integration/typescript/integrations/open-ai) */}
{/* - [Vercel AI SDK](/integration/typescript/integrations/vercel-ai-sdk) */}
{/* - [Other Frameworks](/integration/typescript/integrations/other) */}

## Capturing Messages

<Tip>
  For most use cases, we recommend using `withActiveSpan` as it automatically handles error recording, status setting, and span ending. Use `startActiveSpan` when you need more control over span lifecycle, and `startSpan` only when you need complete manual control.
</Tip>

Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces). A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline. Spans can be nested to capture the pipeline structure.

### Creating a Trace

To capture an end-to-end operation, like processing a user message, you can use the `withActiveSpan` method. This is a helper method provided by the LangWatch SDK (not part of standard OpenTelemetry) that automatically creates a span for the entire operation and handles error recording, status setting, and span ending.

#### Tracer Best Practices

The tracer should be created once and reused throughout your application. Here are some recommended patterns:

<CodeGroup>
```typescript Module-level (Recommended)
// tracer.ts
import { getLangWatchTracer } from "langwatch";

export const tracer = getLangWatchTracer("my-service");
```

```typescript Class-based
class MyService {
  private tracer = getLangWatchTracer("my-service");

  async handleRequest() {
    return await this.tracer.withActiveSpan("handle-request", async (span) => {
      // ... your logic
    });
  }
}
```

```typescript Function-level (Not recommended)
async function handleRequest() {
  const tracer = getLangWatchTracer("my-service"); // Creates new tracer each time
  return await tracer.withActiveSpan("handle-request", async (span) => {
    // ... your logic
  });
}
```
</CodeGroup>

<Tip>
  Create your tracer at the module level and export it for reuse across your application. This ensures consistent service naming and avoids unnecessary tracer creation overhead.
</Tip>

#### Basic Usage

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

async function handleMessage() {
  return await tracer.withActiveSpan("handle-message", async (span) => {
    // This whole function execution is now a single trace
    span.setType("llm");

    // ... rest of your message handling logic ...
    return "response";
  });
}
```

You can also add initial attributes and metadata:

```typescript
await tracer.withActiveSpan(
  "handle-message",
  {
    attributes: {
      "user.id": "user-123",
      "thread.id": "thread-456"
    }
  },
  async (span) => {
    // ... your logic ...
  }
);
```

### Capturing a Span

To instrument specific parts of your pipeline within a trace (like an LLM operation, RAG retrieval, or external API call), use the `startActiveSpan` method.

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

async function ragRetrieval(query: string) {
  return await tracer.withActiveSpan("rag-retrieval", async (span) => {
    span.setType("rag");

    // ... logic to retrieve documents ...
    const searchResults = [
      { id: "doc-1", content: "..." },
      { id: "doc-2", content: "..." }
    ];

    // Add specific context data to the span
    span.setRAGContexts(
      searchResults.map(doc => ({
        document_id: doc.id,
        chunk_id: doc.id,
        content: doc.content
      })),
    );

    return searchResults;
  });
}

async function handleMessage() {
  return await tracer.withActiveSpan("handle-message", async (span) => {
    // ... other logic ...
    const retrievedDocs = await ragRetrieval("user query"); // This creates a nested span
    // ... rest of logic ...
  });
}
```

### Manual Span Management

For more control, you can manually manage spans. There are three approaches:

#### Using startActiveSpan (Recommended for manual control)

`startActiveSpan` is the standard OpenTelemetry method that creates a span, sets it in context (so child spans are nested), but requires manual error handling and ending:

```typescript
tracer.startActiveSpan("my-operation", (span) => {
  try {
    span.setType("llm");
    span.setInput("Hello, world!");

    // ... your logic ...

    span.setOutput("Hello! How can I help you?");
    span.setStatus({ code: SpanStatusCode.OK });
  } catch (error) {
    span.setStatus({
      code: SpanStatusCode.ERROR,
      message: error.message
    });
    span.recordException(error);
    throw error;
  } finally {
    span.end();
  }
});
```

#### Using startSpan (Complete manual control)

`startSpan` is the standard OpenTelemetry method that creates a span but does NOT set it in context, so child spans won't be nested. Use this only when you need complete control:

```typescript
const span = tracer.startSpan("my-operation");
try {
  span.setType("llm");
  span.setInput("Hello, world!");

  // ... your logic ...

  span.setOutput("Hello! How can I help you?");
  span.setStatus({ code: SpanStatusCode.OK });
} catch (error) {
  span.setStatus({
    code: SpanStatusCode.ERROR,
    message: error.message
  });
  span.recordException(error);
  throw error;
} finally {
  span.end();
}
```

### Setting Input and Output

You can record structured input and output data:

<CodeGroup>
```typescript JSON Input/Output
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record JSON input
  span.setInput({
    prompt: "Hello",
    temperature: 0.7
  });

  // Record JSON output
  span.setOutput({
    response: "I'm doing well!",
    confidence: 0.95
  });
});
```

```typescript String Input/Output
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record string input
  span.setInputString("Hello, how are you?");

  // Record string output
  span.setOutputString("I'm doing well!");
});
```

```typescript Mixed Types
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record JSON input
  span.setInput({
    prompt: "Hello",
    temperature: 0.7
  });

  // Record string output
  span.setOutputString("I'm doing well!");
});
```
</CodeGroup>

### Adding Metrics

Track token usage and costs:

<CodeGroup>
```typescript Basic Metrics
await tracer.withActiveSpan("llm-call", async (span) => {
  span.setMetrics({
    promptTokens: 150,
    completionTokens: 50,
    cost: 0.002
  });
});
```

```typescript Partial Metrics
await tracer.withActiveSpan("llm-call", async (span) => {
  // Only track tokens
  span.setMetrics({
    promptTokens: 150,
    completionTokens: 50
  });
});
```

```typescript Cost Only
await tracer.withActiveSpan("llm-call", async (span) => {
  // Only track cost
  span.setMetrics({
    cost: 0.002
  });
});
```
</CodeGroup>

## Full Setup

Here's a complete example showing how to integrate LangWatch with a typical LLM application using the Vercel AI SDK, including RAG retrieval, generation, and data storage.

### Complete Example

<CodeGroup>
```typescript app.ts
import { setupLangWatch } from "langwatch/node";
import { getLangWatchTracer } from "langwatch";
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";
import { z } from "zod";

// Initialize LangWatch
await setupLangWatch({
  apiKey: process.env.LANGWATCH_API_KEY, // Optional: defaults to environment variable `LANGWATCH_API_KEY`
  endpoint: process.env.LANGWATCH_ENDPOINT, // Optional: defaults to `LANGWATCH_ENDPOINT` or hosted cloud endpoint
  baseAttributes: {
    "service.name": "chatbot-service",
    "service.version": "1.0.0",
    "deployment.environment": process.env.NODE_ENV,
  },
  spanExcludeRules: [{
    fieldName: ["span_name"],
    matchValue: "health-check",
    matchOperation: "exact_match"
  }]
});

// Create a reusable tracer
const tracer = getLangWatchTracer("chatbot-service");

// Mock database for storing conversation data
const conversationStore = new Map<string, any[]>();

// Mock RAG retrieval function
async function retrieveRelevantDocuments(query: string) {
  return await tracer.withActiveSpan("rag-retrieval", async (span) => {
    span.setType("rag");

    // Simulate document retrieval
    const documents = [
      { id: "doc-1", content: "LangWatch is an observability platform for LLM applications." },
      { id: "doc-2", content: "It provides tracing, evaluation, and monitoring capabilities." },
      { id: "doc-3", content: "The platform supports multiple programming languages and frameworks." }
    ];

    // Filter documents based on query (simplified)
    const relevantDocs = documents.filter(doc =>
      doc.content.toLowerCase().includes(query.toLowerCase())
    );

    // Record RAG contexts
    span.setRAGContexts(
      relevantDocs.map(doc => ({
        document_id: doc.id,
        chunk_id: doc.id,
        content: doc.content
      }))
    );

    span.setInputString(query);

    return relevantDocs;
  });
}

// Main message handling function
async function handleUserMessage(
  userId: string,
  threadId: string,
  message: string
) {
  return await tracer.withActiveSpan("handle-user-message", {
    attributes: {
      "user.id": userId,
      "thread.id": threadId,
      "message.type": "user"
    }
  }, async (span) => {
    // 1. Retrieve relevant documents
    const documents = await retrieveRelevantDocuments(message);

    // 2. Generate response using Vercel AI SDK
    const response = await generateText({
      model: openai("gpt-5"),
      prompt: `Based on the following context, answer the user's question:

Context:
${documents.map(doc => doc.content).join('\n')}

User Question: ${message}

Answer:`,
      maxTokens: 500,
      temperature: 0.7,
      // This is required for LangWatch to capture the LLM input and output, and other data.
      experimental_telemetry: { isEnabled: true },
    });

    // 3. Store conversation data
    const conversationData = {
      timestamp: new Date().toISOString(),
      userMessage: message,
      assistantResponse: response.text,
      retrievedDocuments: documents,
      threadId,
      userId
    };

    // Store in mock database
    const userConversations = conversationStore.get(userId) || [];
    userConversations.push(conversationData);
    conversationStore.set(userId, userConversations);

    // 4. Record the complete interaction
    span.setType("llm");

    // 5. Record metrics
    span.setMetrics({
      promptTokens: response.usage?.promptTokens || 0,
      completionTokens: response.usage?.completionTokens || 0,
      cost: response.usage?.cost || 0
    });

    // 6. Record input, the pure LLM input is automatically captured via the Vercel AI SDK.
    span.setInput({
      userMessage: message,
      retrievedDocuments: documents,
      threadId,
      userId
    });

    // 6. Record output, the pure LLM output is automatically captured via the Vercel AI SDK.
    span.setOutput({
      assistantResponse: response.text,
      conversationId: conversationData.timestamp
    });

    return {
      response: response.text,
      conversationId: conversationData.timestamp,
      retrievedDocuments: documents
    };
  });
}

// API endpoint handler (Express.js example)
app.post('/api/chat', async (req, res) => {
  const { userId, threadId, message } = req.body;

  try {
    const result = await handleUserMessage(userId, threadId, message);
    res.json(result);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Get conversation history
app.get('/api/conversations/:userId', (req, res) => {
  const { userId } = req.params;
  const conversations = conversationStore.get(userId) || [];
  res.json(conversations);
});

// Health check endpoint
app.get('/api/health', (req, res) => {
  tracer.withActiveSpan("health-check", async (span) => {
    span.setStatus({ code: SpanStatusCode.OK });
    res.json({ status: 'ok' });
  });
});
```

```bash .env
LANGWATCH_API_KEY=your_api_key_here
NODE_ENV=production
```

```json package.json
{
  "dependencies": {
    "@ai-sdk/openai": "^1.3.23",
    "@opentelemetry/context-async-hooks": "^2.0.1",
    "@opentelemetry/sdk-node": "^0.203.0",
    "ai": "^4.3.19",
    "express": "^4.18.0",
    "langwatch": "^0.3.0",
    "zod": "^4.0.10"
  }
}
```

```typescript instrumentation.ts
import { registerOTel } from '@vercel/otel';

export function register() {
  registerOTel({
    serviceName: 'next-app',
    // Note: LangWatch SDK automatically sets up the trace exporter,
    // so no manual configuration is needed here
  });
}
```
</CodeGroup>

### Key Features Demonstrated

- **Tracing**: Complete request flow from user message to response
- **RAG Context**: Document retrieval with proper context recording
- **LLM Events**: Structured conversation flow with system, user, and assistant messages
- **Metrics**: Token usage and cost tracking
- **Error Handling**: Proper error recording and status setting
- **Data Storage**: Conversation persistence with trace correlation
- **Thread Management**: Conversation grouping with thread IDs
- **User Analytics**: User ID tracking for analytics
- **Custom Attributes**: Service metadata and environment information

This example shows how LangWatch can capture every aspect of your LLM application, from the initial user request through document retrieval, LLM generation, and data storage, providing complete observability into your AI pipeline.

---

# FILE: ./integration/typescript/integrations/langchain.mdx

---
title: LangChain Instrumentation
sidebarTitle: TypeScript
description: Learn how to instrument Langchain applications with the LangWatch TypeScript SDK.
keywords: langchain, instrumentation, callback, opentelemetry, langwatch, typescript, tracing, openllmetry
---

Langchain is a powerful framework for building LLM applications. LangWatch integrates with Langchain to provide detailed observability into your chains, agents, LLM calls, and tool usage.

This guide covers the primary approaches to instrumenting Langchain with LangWatch:

1. **Using LangWatch's Langchain Callback Handler (Recommended)**: The most direct method, using a specific callback provided by LangWatch to capture rich Langchain-specific trace data.
2. **Using Community OpenTelemetry Instrumentors**: Leveraging dedicated Langchain instrumentors like those from OpenLLMetry.
3. **Utilizing Langchain's Native OpenTelemetry Export (Advanced)**: Configuring Langchain to send its own OpenTelemetry traces to an endpoint where LangWatch can collect them.

## 1. Using LangWatch's Langchain Callback Handler (Recommended)

This is the preferred and most comprehensive method for instrumenting Langchain with LangWatch. The LangWatch SDK provides a `LangWatchCallbackHandler` that deeply integrates with Langchain's event system.

```typescript
import { setupLangWatch } from "langwatch/node";
import { LangWatchCallbackHandler } from "langwatch/observability/instrumentation/langchain";
import { getLangWatchTracer } from "langwatch";
import { semconv } from "langwatch/observability";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

// Initialize LangWatch
await setupLangWatch();

const tracer = getLangWatchTracer("langchain-example");

async function handleMessageWithCallback(userQuestion: string) {
  return await tracer.withActiveSpan("Langchain - QA with Callback", {
    attributes: {
      [semconv.ATTR_LANGWATCH_THREAD_ID]: "callback-user",
    },
  }, async (span) => {
    const langWatchCallback = new LangWatchCallbackHandler();

    const model = new ChatOpenAI({
      modelName: "gpt-5",
      temperature: 0.7,
      callbacks: [langWatchCallback],
    });

    const prompt = ChatPromptTemplate.fromMessages([
      ["system", "You are a concise assistant."],
      ["human", "{question}"],
    ]);

    // Modern LCEL (LangChain Expression Language) syntax
    const chain = prompt.pipe(model).pipe(new StringOutputParser());

    const response = await chain.invoke({ question: userQuestion });
    return response;
  });
}

async function mainCallback() {
  if (!process.env.OPENAI_API_KEY) {
    console.log("OPENAI_API_KEY not set. Skipping Langchain callback example.");
    return;
  }

  const response = await handleMessageWithCallback("What is Langchain? Explain briefly.");
  console.log(`AI (Callback): ${response}`);
}

mainCallback().catch(console.error);
```

**How it Works:**
- `setupLangWatch()`: Initializes LangWatch with default configuration.
- `getLangWatchTracer()`: Creates a tracer instance for your application.
- `tracer.withActiveSpan()`: Creates a parent LangWatch trace with automatic error handling and span management.
- `LangWatchCallbackHandler`: A LangWatch-specific callback handler that captures Langchain events and converts them into detailed LangWatch spans.
- The callback handler is passed to Langchain components via the `callbacks` option.

**Key points:**
- Provides the most detailed Langchain-specific structural information (chains, agents, tools, LLMs as distinct steps).
- Works for all Langchain execution methods (`invoke`, `stream`, `batch`, etc.).
- Automatically handles span lifecycle management with `withActiveSpan()`.

## 2. Using Community OpenTelemetry Instrumentors

Dedicated Langchain instrumentors from libraries like OpenLLMetry can also be used to capture Langchain operations as OpenTelemetry traces, which LangWatch can then ingest.

### Instrumenting Langchain with Dedicated Instrumentors

#### i. Via `setupLangWatch()`

<CodeGroup>
```typescript OpenLLMetry
// OpenLLMetry Example
import { setupLangWatch } from "langwatch/node";
import { getLangWatchTracer } from "langwatch";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
// Note: OpenLLMetry TypeScript instrumentation would need to be imported here
// import { LangchainInstrumentor } from "opentelemetry-instrumentation-langchain";

await setupLangWatch({
  // instrumentors: [new LangchainInstrumentor()] // Add OpenLLMetry LangchainInstrumentor
});

const tracer = getLangWatchTracer("langchain-openllmetry");

async function handleMessageOpenLLMetry(userQuestion: string) {
  return await tracer.withActiveSpan("Langchain - OpenLLMetry via Setup", async (span) => {
    const model = new ChatOpenAI({
      modelName: "gpt-5",
      temperature: 0.7,
    });

    const prompt = ChatPromptTemplate.fromMessages([
      ["system", "You are brief."],
      ["human", "{question}"],
    ]);

    const chain = prompt.pipe(model).pipe(new StringOutputParser());
    const response = await chain.invoke({ question: userQuestion });
    return response;
  });
}

async function mainOpenLLMetry() {
  if (!process.env.OPENAI_API_KEY) {
    console.log("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain example.");
    return;
  }

  const response = await handleMessageOpenLLMetry("Explain Langchain instrumentation with OpenLLMetry.");
  console.log(`AI (OpenLLMetry): ${response}`);
}

mainOpenLLMetry().catch(console.error);
```
</CodeGroup>

#### ii. Direct Instrumentation

<CodeGroup>
```typescript OpenLLMetry
// OpenLLMetry Example
import { setupLangWatch } from "langwatch/node";
import { getLangWatchTracer } from "langwatch";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
// Note: OpenLLMetry TypeScript instrumentation would need to be imported here
// import { LangchainInstrumentor } from "opentelemetry-instrumentation-langchain";

await setupLangWatch();

// Instrument Langchain directly
// LangchainInstrumentor().instrument();

const tracer = getLangWatchTracer("langchain-openllmetry-direct");

async function handleMessageOpenLLMetryDirect(userQuestion: string) {
  return await tracer.withActiveSpan("Langchain - OpenLLMetry Direct", async (span) => {
    const model = new ChatOpenAI({
      modelName: "gpt-5",
      temperature: 0.7,
    });

    const prompt = ChatPromptTemplate.fromMessages([
      ["system", "You are very brief."],
      ["human", "{question}"],
    ]);

    const chain = prompt.pipe(model).pipe(new StringOutputParser());
    const response = await chain.invoke({ question: userQuestion });
    return response;
  });
}

async function mainOpenLLMetryDirect() {
  if (!process.env.OPENAI_API_KEY) {
    console.log("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (direct) example.");
    return;
  }

  const response = await handleMessageOpenLLMetryDirect("How does direct Langchain instrumentation work with OpenLLMetry?");
  console.log(`AI (OpenLLMetry Direct): ${response}`);
}

mainOpenLLMetryDirect().catch(console.error);
```
</CodeGroup>

**Key points for dedicated Langchain instrumentors:**
- Directly instrument Langchain operations, providing traces from Langchain's perspective.
- Requires installing the respective instrumentation package (e.g., `opentelemetry-instrumentation-langchain` for OpenLLMetry).
- Note: TypeScript versions of these instrumentors may have different availability compared to Python.

## 3. Utilizing Langchain's Native OpenTelemetry Export (Advanced)

Langchain itself can be configured to export OpenTelemetry traces. If you set this up and configure Langchain to send traces to an OpenTelemetry collector endpoint that LangWatch is also configured to receive from (or if LangWatch *is* your OTLP endpoint), then LangWatch can ingest these natively generated Langchain traces.

**Setup (Conceptual):**
1. Configure Langchain for OpenTelemetry export. This usually involves setting environment variables:
   ```bash
   export LANGCHAIN_TRACING_V2=true
   export LANGCHAIN_ENDPOINT= # Your OTLP gRPC endpoint (e.g., http://localhost:4317)
   # Potentially other OTEL_EXPORTER_OTLP_* variables for more granular control
   ```
2. Initialize LangWatch: `await setupLangWatch()`.

```typescript
// This example assumes Langchain is configured via environment variables
// to send OTel traces to an endpoint LangWatch can access.
import { setupLangWatch } from "langwatch/node";
import { getLangWatchTracer } from "langwatch";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

await setupLangWatch(); // LangWatch is ready to receive OTel traces

const tracer = getLangWatchTracer("langchain-native-otel");

async function handleMessageNativeOtel(userQuestion: string) {
  return await tracer.withActiveSpan("Langchain - Native OTel Export", async (span) => {
    const model = new ChatOpenAI({
      modelName: "gpt-5",
      temperature: 0.7,
    });

    const prompt = ChatPromptTemplate.fromMessages([
      ["system", "You are brief."],
      ["human", "{question}"],
    ]);

    const chain = prompt.pipe(model).pipe(new StringOutputParser());
    const response = await chain.invoke({ question: userQuestion });
    return response;
  });
}

async function mainNativeOtel() {
  if (!process.env.OPENAI_API_KEY || process.env.LANGCHAIN_TRACING_V2 !== "true") {
    console.log("Required env vars (OPENAI_API_KEY, LANGCHAIN_TRACING_V2='true') not set. Skipping native OTel.");
    return;
  }

  const response = await handleMessageNativeOtel("Tell me about Langchain OTel export itself.");
  console.log(`AI (Native OTel): ${response}`);
}

mainNativeOtel().catch(console.error);
```

**Key points for Langchain's native OTel export:**
- LangWatch acts as a backend/collector for OpenTelemetry traces generated directly by Langchain.
- Requires careful configuration of Langchain's environment variables.
- The level of detail depends on Langchain's native OpenTelemetry instrumentation quality.

<Note>
### Which Approach to Choose?

- **LangWatch's Langchain Callback Handler (Recommended)**: Provides the richest, most Langchain-aware traces directly integrated with LangWatch's tracing context. Ideal for most users.
- **Dedicated Langchain Instrumentors (OpenLLMetry)**: Good alternatives if you prefer an explicit instrumentor pattern for Langchain itself or are standardizing on OpenLLMetry's OpenTelemetry ecosystem.
- **Langchain's Native OTel Export (Advanced)**: Suitable if you have an existing OpenTelemetry collection infrastructure and want Langchain to be another OTel-compliant source.

For the best Langchain-specific observability within LangWatch, the **Langchain Callback Handler** is the generally recommended approach, with dedicated **Langchain Instrumentors** as strong alternatives for instrumentor-based setups.
</Note>

## Common Mistakes and Caveats

### 1. Setup and Initialization Issues

<Warning>
**Multiple setup calls**: `setupLangWatch()` can only be called once per process. Subsequent calls will throw an error.
</Warning>

```typescript
// ❌ Wrong - Multiple setup calls
await setupLangWatch();
await setupLangWatch(); // This will throw an error

// ✅ Correct - Single setup call
await setupLangWatch();
```

<Warning>
**Missing await**: Always await the setup function to ensure proper initialization.
</Warning>

```typescript
// ❌ Wrong - Missing await
setupLangWatch(); // This won't properly initialize

// ✅ Correct - Properly awaited
await setupLangWatch();
```

### 2. Callback Handler Usage

<Warning>
**Reusing callback handlers**: Each trace should use a fresh `LangWatchCallbackHandler` instance to avoid span conflicts.
</Warning>

```typescript
// ❌ Wrong - Reusing callback handler
const callback = new LangWatchCallbackHandler();

async function processMultipleRequests() {
  // This can cause span conflicts
  const model1 = new ChatOpenAI({ callbacks: [callback] });
  const model2 = new ChatOpenAI({ callbacks: [callback] });
}

// ✅ Correct - Fresh callback handler per trace
async function processMultipleRequests() {
  const callback1 = new LangWatchCallbackHandler();
  const callback2 = new LangWatchCallbackHandler();

  const model1 = new ChatOpenAI({ callbacks: [callback1] });
  const model2 = new ChatOpenAI({ callbacks: [callback2] });
}
```

### 3. Span Management

<Warning>
**Manual span management**: Avoid manually managing spans when using `withActiveSpan()`. The function handles span lifecycle automatically.
</Warning>

```typescript
// ❌ Wrong - Manual span management with withActiveSpan
await tracer.withActiveSpan("my-operation", async (span) => {
  span.setStatus({ code: SpanStatusCode.OK });
  span.end(); // Don't manually end spans in withActiveSpan
});

// ✅ Correct - Let withActiveSpan handle span lifecycle
await tracer.withActiveSpan("my-operation", async (span) => {
  // Your code here - span is automatically ended
});
```

### 4. Environment Configuration

<Warning>
**Missing environment variables**: Ensure all required environment variables are set before running your application.
</Warning>

```typescript
// ❌ Wrong - No environment validation
await setupLangWatch();
const model = new ChatOpenAI(); // May fail if OPENAI_API_KEY not set

// ✅ Correct - Environment validation
if (!process.env.OPENAI_API_KEY) {
  console.error("OPENAI_API_KEY environment variable is required");
  process.exit(1);
}

await setupLangWatch();
const model = new ChatOpenAI();
```

### 5. TypeScript-Specific Issues

<Warning>
**Import path confusion**: Use the correct import paths for Node.js vs browser environments.
</Warning>

```typescript
// ❌ Wrong - Using browser import in Node.js
import { setupLangWatch } from "langwatch"; // This won't work in Node.js

// ✅ Correct - Use appropriate import for environment
import { setupLangWatch } from "langwatch/node"; // For Node.js
import { setupLangWatch } from "langwatch"; // For browser
```

<Warning>
**Missing type definitions**: Ensure you have proper TypeScript configuration for the LangWatch SDK.
</Warning>

```json
// tsconfig.json - Ensure proper module resolution
{
  "compilerOptions": {
    "moduleResolution": "node",
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true
  }
}
```

### 6. Performance Considerations

<Tip>
**Batch processing**: When processing multiple requests, consider using batch operations to reduce the number of spans created.
</Tip>

```typescript
// ❌ Inefficient - Individual processing
for (const question of questions) {
  await handleMessageWithCallback(question); // Creates separate trace per question
}

// ✅ Efficient - Batch processing
await tracer.withActiveSpan("batch-processing", async (span) => {
  const results = await Promise.all(
    questions.map(question => handleMessageWithCallback(question))
  );
  return results;
});
```

### 7. Error Handling

<Warning>
**Unhandled promise rejections**: Always handle errors in async operations to prevent unhandled promise rejections.
</Warning>

```typescript
// ❌ Wrong - Unhandled promise rejection
mainCallback(); // This can cause unhandled promise rejection

// ✅ Correct - Proper error handling
mainCallback().catch(console.error);
// or
try {
  await mainCallback();
} catch (error) {
  console.error("Error in main callback:", error);
}
```

### 8. Memory Management

<Tip>
**Callback handler cleanup**: While the SDK handles most cleanup automatically, be mindful of callback handler instances in long-running applications.
</Tip>

```typescript
// For long-running applications, consider cleaning up callback handlers
class ChatService {
  private callbackHandlers: LangWatchCallbackHandler[] = [];

  async processMessage(message: string) {
    const callback = new LangWatchCallbackHandler();
    this.callbackHandlers.push(callback);

    // Process message...

    // Clean up old handlers periodically
    if (this.callbackHandlers.length > 100) {
      this.callbackHandlers = this.callbackHandlers.slice(-50);
    }
  }
}
```

<Info>
### Best Practices Summary

1. **Always await `setupLangWatch()`** and call it only once per process
2. **Use fresh callback handlers** for each trace to avoid conflicts
3. **Let `withActiveSpan()` handle span lifecycle** - don't manually end spans
4. **Validate environment variables** before starting your application
5. **Use appropriate import paths** for your environment (Node.js vs browser)
6. **Handle errors properly** to avoid unhandled promise rejections
7. **Consider performance implications** when processing multiple requests
8. **Monitor memory usage** in long-running applications
</Info>

---

# FILE: ./integration/typescript/integrations/vercel-ai-sdk.mdx

---
title: Vercel AI SDK
sidebarTitle: Vercel AI SDK
description: LangWatch Vercel AI SDK integration guide
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

<LLMsTxtProtip />

<Prerequisites />

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Integration


The Vercel AI SDK supports tracing via Next.js OpenTelemetry integration. By using the `LangWatchExporter`, you can automatically collect those traces to LangWatch.

First, you need to install the necessary dependencies:

```bash
npm install @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```

Then, set up the OpenTelemetry for your application, follow one of the tabs below depending whether you are using AI SDK with Next.js or on Node.js:


### Next.js

You need to enable the `instrumentationHook` in your `next.config.js` file if you haven't already:

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    instrumentationHook: true,
  },
};

module.exports = nextConfig;
```

Next, you need to create a file named `instrumentation.ts` (or `.js`) in the __root directory__ of the project (or inside `src` folder if using one), with `LangWatchExporter` as the traceExporter:

```typescript
import { registerOTel } from '@vercel/otel'
import { LangWatchExporter } from 'langwatch'

export function register() {
  registerOTel({
    serviceName: 'next-app',
    traceExporter: new LangWatchExporter({
      apiKey: process.env.LANGWATCH_API_KEY
    })
  })
}
```

(Read more about Next.js OpenTelemetry configuration [on the official guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry#manual-opentelemetry-configuration))

Finally, enable `experimental_telemetry` tracking on the AI SDK calls you want to trace:

```typescript
const result = await generateText({
  model: openai('gpt-5'),
  prompt: 'Explain why a chicken would make a terrible astronaut, be creative and humorous about it.',
  experimental_telemetry: {
    isEnabled: true,
    // optional metadata
    metadata: {
      userId: "myuser-123",
      threadId: "mythread-123",
    },
  },
});
```

### Node.js

For Node.js, start by following the official OpenTelemetry guide:

- [OpenTelemetry Node.js Getting Started](https://opentelemetry.io/docs/languages/js/getting-started/nodejs/)

Once you have set up OpenTelemetry, you can use the `LangWatchExporter` to automatically send your traces to LangWatch:

```typescript
import { LangWatchExporter } from 'langwatch'

const sdk = new NodeSDK({
  traceExporter: new LangWatchExporter({
    apiKey: process.env.LANGWATCH_API_KEY
  }),
  // ...
});
```

That's it! Your messages will now be visible on LangWatch:

![Vercel AI SDK](/images/integration/vercel-ai-sdk.png)

## Example Project

You can find a full example project with a more complex pipeline and Vercel AI SDK and LangWatch integration [on our GitHub](https://github.com/langwatch/langwatch/blob/main/typescript-sdk/example/lib/chat/vercel-ai.tsx).

## Manual Integration

The docs from here below are for manual integration, in case you are not using the Vercel AI SDK OpenTelemetry integration,
you can manually start a trace to capture your messages:

```typescript
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});
```

Then, you can start an LLM span inside the trace with the input about to be sent to the LLM.

```typescript
import { convertFromVercelAIMessages } from 'langwatch'

const span = trace.startLLMSpan({
  name: "llm",
  model: model,
  input: {
    type: "chat_messages",
    value: convertFromVercelAIMessages(messages)
  },
});
```

This will capture the LLM input and register the time the call started. Once the LLM call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation, e.g.:

```typescript
span.end({
  output: {
    type: "chat_messages",
    value: convertFromVercelAIMessages(output), // assuming output is Message[]
  },
  metrics: {
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  },
});
```

<Note>
On short-live environments like Lambdas or Serverless Functions, be sure to call <br /> `await trace.sendSpans();` to wait for all pending requests to be sent before the runtime is destroyed.
</Note>

## Capture a RAG Span

Appart from LLM spans, another very used type of span is the RAG span. This is used to capture the retrieved contexts from a RAG that will be used by the LLM, and enables a whole new set of RAG-based features evaluations for RAG quality on LangWatch.

<TypeScriptRAG />

## Capture an arbritary Span

You can also use generic spans to capture any type of operation, its inputs and outputs, for example for a function call:

<TypeScriptCaptureSpans />

## Capturing Exceptions

To capture also when your code throws an exception, you can simply wrap your code around a try/catch, and update or end the span with the exception:

<TypeScriptExceptions />

## Capturing custom evaluation results

[LangWatch Evaluators](/llm-evaluation/list) can run automatically on your traces, but if you have an in-house custom evaluator, you can also capture the evaluation
results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

<TypeScriptCustomEvaluation />



---

# FILE: ./integration/typescript/integrations/azure.mdx

---
title: Azure OpenAI
sidebarTitle: TS/JS
description: LangWatch Azure OpenAI integration guide
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

<LLMsTxtProtip />

<Prerequisites />

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Integration

Start by initializing LangWatch client and creating a new trace to capture your messages:

```typescript
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});
```

Then to capture your LLM calls, you can start an LLM span inside the trace with the input about to be sent to the LLM.

First, define the model and the messages you are going to use for your LLM call separately, so you can capture them:

```typescript
import { AzureOpenAI } from "openai";

// Model to be used and messages that will be sent to the LLM
const model = "gpt-4-turbo-2024-04-09"
const messages : OpenAI.Chat.ChatCompletionMessageParam[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
]
```

Then, start the LLM span from the trace, giving it the model and input messages:

```typescript
const span = trace.startLLMSpan({
  name: "llm",
  model: model,
  input: {
    type: "chat_messages",
    value: messages
  },
});
```

This will capture the LLM input and register the time the call started. Now, continue with the LLM call normally, using the same parameters:

```typescript
const openai = new AzureOpenAI({
  apiKey: process.env.AZURE_OPENAI_API_KEY,
  apiVersion: "2024-02-01",
  endpoint: process.env.AZURE_OPENAI_ENDPOINT,
});
const chatCompletion = await openai.chat.completions.create({
  messages: messages,
  model: model,
});
```

Finally, after the OpenAI call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation:

```typescript
span.end({
  output: {
    type: "chat_messages",
    value: [chatCompletion.choices[0]!.message],
  },
  metrics: {
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  },
});
```

<Note>
On short-live environments like Lambdas or Serverless Functions, be sure to call <br /> `await trace.sendSpans();` to wait for all pending requests to be sent before the runtime is destroyed.
</Note>

## Capture a RAG Span

Appart from LLM spans, another very used type of span is the RAG span. This is used to capture the retrieved contexts from a RAG that will be used by the LLM, and enables a whole new set of RAG-based features evaluations for RAG quality on LangWatch.

<TypeScriptRAG />

## Capture an arbritary Span

You can also use generic spans to capture any type of operation, its inputs and outputs, for example for a function call:

<TypeScriptCaptureSpans />

## Capturing Exceptions

To capture also when your code throws an exception, you can simply wrap your code around a try/catch, and update or end the span with the exception:

<TypeScriptExceptions />

## Capturing custom evaluation results

[LangWatch Evaluators](/llm-evaluation/list) can run automatically on your traces, but if you have an in-house custom evaluator, you can also capture the evaluation
results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

<TypeScriptCustomEvaluation />



---

# FILE: ./integration/typescript/integrations/mastra.mdx

---
title: Mastra
description: Learn how to integrate Mastra, a TypeScript agent framework, with LangWatch.
---

#  Observability for Mastra With LangWatch

This guide shows you how to integrate **Mastra** with **LangWatch** for observability and tracing. By following these steps, you'll be able to monitor and debug your Mastra agents in the LangWatch dashboard.


## Integration

<Steps>
<Step title="Create a Mastra project">
Create a Mastra project using the Mastra CLI:

  ```bash
  npx create-mastra
  ```

  Move into the project directory:

  ```bash
  cd your-mastra-project
  ```
For more information, view Mastra installation instructions [here](https://mastra.ai/docs/getting-started/installation)


</Step>

<Step title="Set up LangWatch project">
Create a project in [LangWatch](https://app.langwatch.ai) and get your API keys from the project settings page.

</Step>

<Step title="Add environment variables">
  Create or update your `.env` file with the following variables:

  ```bash
  # Your LLM API key
  OPENAI_API_KEY=your-api-key

  # LangWatch credentials
  LANGWATCH_API_KEY=sk-...

  ```
</Step>


<Step title="Install the langwatch package">
  Add the `langwatch` package to your project:

  ```bash
  npm install langwatch
  ```

</Step>

<Step title="Add LangWatch to your Mastra instance">
  Add LangWatch to your Mastra instance via the telemetry exporter.

  ```typescript
import { Mastra } from "@mastra/core/mastra";
import { PinoLogger } from "@mastra/loggers";
import { LibSQLStore } from "@mastra/libsql";
import { LangWatchExporter } from "langwatch";

import { weatherAgent } from "./agents/weather-agent";

export const mastra = new Mastra({
  agents: { weatherAgent },
  telemetry: {
    serviceName: "ai", // this must be set to "ai" so that the LangWatchExporter thinks it's an AI SDK trace
    enabled: true,
    export: {
      type: "custom",
      exporter: new LangWatchExporter({
        apiKey: process.env.LANGWATCH_API_KEY,
        debug: true,
        // includeAllSpans: true,
      }),
    },
  },
});
  ```

</Step>


<Step title="Run mastra dev server">
  Start the Mastra development server:

  ```bash
  npm run dev
  ```

  Head over to the developer playground with the provided URL and start chatting with your agent.


  ### View traces in LangWatch
  Visit your [LangWatch dashboard](https://app.langwatch.ai) to explore detailed insights into your agent interactions. Monitor and analyze every aspect of your AI conversations, from prompt engineering to response quality, helping you optimize your AI applications.

</Step>

</Steps>

---

# FILE: ./integration/typescript/integrations/open-ai.mdx

---
title: OpenAI
sidebarTitle: TS/JS
description: LangWatch OpenAI TypeScript integration guide
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

<LLMsTxtProtip />

<Prerequisites />

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Integration

Start by initializing LangWatch client and creating a new trace to capture your messages:

```typescript
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});
```

Then to capture your LLM calls, you can start an LLM span inside the trace with the input about to be sent to the LLM.

First, define the model and the messages you are going to use for your LLM call separately, so you can capture them:

```typescript
import { OpenAI } from "openai";

// Model to be used and messages that will be sent to the LLM
const model = "gpt-5"
const messages : OpenAI.Chat.ChatCompletionMessageParam[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
]
```

Then, start the LLM span from the trace, giving it the model and input messages:

```typescript
const span = trace.startLLMSpan({
  name: "llm",
  model: model,
  input: {
    type: "chat_messages",
    value: messages
  },
});
```

This will capture the LLM input and register the time the call started. Now, continue with the LLM call normally, using the same parameters:

```typescript
const openai = new OpenAI();
const chatCompletion = await openai.chat.completions.create({
  messages: messages,
  model: model,
});
```

Finally, after the OpenAI call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation:

```typescript
span.end({
  output: {
    type: "chat_messages",
    value: [chatCompletion.choices[0]!.message],
  },
  metrics: {
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  },
});
```

<Note>
On short-live environments like Lambdas or Serverless Functions, be sure to call <br /> `await trace.sendSpans();` to wait for all pending requests to be sent before the runtime is destroyed.
</Note>

## Capture a RAG Span

Appart from LLM spans, another very used type of span is the RAG span. This is used to capture the retrieved contexts from a RAG that will be used by the LLM, and enables a whole new set of RAG-based features evaluations for RAG quality on LangWatch.

<TypeScriptRAG />

## Capture an arbritary Span

You can also use generic spans to capture any type of operation, its inputs and outputs, for example for a function call:

<TypeScriptCaptureSpans />

## Capturing Exceptions

To capture also when your code throws an exception, you can simply wrap your code around a try/catch, and update or end the span with the exception:

<TypeScriptExceptions />

## Capturing custom evaluation results

[LangWatch Evaluators](/llm-evaluation/list) can run automatically on your traces, but if you have an in-house custom evaluator, you can also capture the evaluation
results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

<TypeScriptCustomEvaluation />



---

# FILE: ./integration/rest-api.mdx

---
title: REST API
description: Integrate LangWatch with any language by using the REST API
---

If your preferred programming language or platform is not directly supported by the existing LangWatch libraries, you can use the REST API with `curl` to send trace data. This guide will walk you through how to integrate LangWatch with any system that allows HTTP requests.

**Prerequisites:**

- Ensure you have `curl` installed on your system.

**Configuration:**

Set the `LANGWATCH_API_KEY` environment variable in your environment:

```bash
export LANGWATCH_API_KEY='your_api_key_here'
```

**Usage:**

You will need to prepare your span data in accordance with the Span type definitions provided by LangWatch. Below is an example of how to send span data using curl:

    1. Prepare your JSON data. Make sure it's properly formatted as expected by LangWatch.
    2. Use the curl command to send your trace data. Here is a basic template:

```bash
# Set your API key and endpoint URL
LANGWATCH_API_KEY="your_langwatch_api_key"
LANGWATCH_ENDPOINT="https://app.langwatch.ai"

# Use curl to send the POST request, e.g.:
curl -X POST "$LANGWATCH_ENDPOINT/api/collector" \
     -H "X-Auth-Token: $LANGWATCH_API_KEY" \
     -H "Content-Type: application/json" \
     -d @- <<EOF
{
  "trace_id": "trace-123",
  "spans": [
    {
      "type": "llm",
      "span_id": "span-456",
      "vendor": "openai",
      "model": "gpt-5",
      "input": {
        "type": "chat_messages",
        "value": [
          {
            "role": "user",
            "content": "Input to the LLM"
          }
        ]
      },
      "output": {
        "type": "chat_messages",
        "value": [
            {
                "role": "assistant",
                "content": "Output from the LLM",
                "function_call": null,
                "tool_calls": []
            }
        ]
      },
      "params": {
        "temperature": 0.7,
        "stream": false
      },
      "metrics": {
        "prompt_tokens": 100,
        "completion_tokens": 150
      },
      "timestamps": {
        "started_at": $(($(date +%s) * 1000)),
        "finished_at": $((($(date +%s) + 1) * 1000))
      }
    }
  ],
  "metadata": {
    "user_id": "optional_end_user_identifier",
    "thread_id": "optional_thread_identifier",
    "customer_id": "optional_platform_customer_identifier",
    "labels": ["optional_label_1", "optional_label_2"]
  }
}
EOF
```

Replace the placeholders with your actual data. The `@-` tells `curl` to read the JSON data from the standard input, which we provide via the `EOF`-delimited here-document.

For the type reference of how a `span` should look like, check out our [types definitions](https://github.com/langwatch/langwatch/blob/main/python-sdk/langwatch/types.py#L118).

It's optional but highly recommended to pass the `user_id` on the metadata if you want to leverage user-specific analytics and the `thread_id` to group related traces together. To connect it to an event later on. Read more about those and other concepts [here](../concepts).

3.  Execute the `curl` command. If successful, LangWatch will process your trace data.

This method of integration offers a flexible approach for sending traces from any system capable of making HTTP requests. Whether you're using a less common programming language or a custom-built platform, this RESTful approach ensures you can benefit from LangWatch's capabilities.

Remember to handle errors and retries as needed. You might need to script additional logic around the `curl` command to handle these cases.

After following the above guide, your interactions with LLMs should now be captured by LangWatch. Once integrated, you can visit your LangWatch dashboard to view and analyze the traces collected from your applications.

---

# FILE: ./integration/opentelemetry/guide.mdx

---
title: OpenTelemetry Integration Guide
sidebarTitle: OpenTelemetry (without SDK)
description: Use OpenTelemetry to capture LLM traces and send them to LangWatch
---

OpenTelemetry is a standard protocol for tracing, and LangWatch is fully compatible with OpenTelemetry, you can use any OpenTelemetry compatible library to capture your LLM traces and send them to LangWatch.

This guide demonstrates the OpenTelemetry integration using Python, but the same principles apply to integration with OpenTelemetry instrumentation in other languages.

#### Prerequisites

- Obtain your `LANGWATCH_API_KEY` from the [LangWatch dashboard](https://app.langwatch.ai/).

#### Installation

```bash
pip install opentelemetry
```

#### Configuration

Set up LangWatch as the OpenTelemetry exporter endpoint:

```python
import os
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor

# Set up OpenTelemetry trace provider with LangWatch as the endpoint
tracer_provider = trace_sdk.TracerProvider()
tracer_provider.add_span_processor(
    SimpleSpanProcessor(
        OTLPSpanExporter(
            endpoint="https://app.langwatch.ai/api/otel/v1/traces",
            headers={"Authorization": "Bearer " + os.environ["LANGWATCH_API_KEY"]},
        )
    )
)
# Optionally, you can also print the spans to the console.
tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
```

## Capturing LLM Traces

Currently, there are different open initiatives for LLM instrumentation libraries, here we show some examples on how to capture LLM traces with a couple of them.


  ### OpenInference (Python)


      ### OpenAI

        Installation:

        ```bash
        pip install openinference-instrumentation-openai
        ```

        Then, instrument your OpenAI calls:

        ```python
        from openinference.instrumentation.openai import OpenAIInstrumentor

        OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your OpenAI calls in the LangWatch dashboard:

        ![OpenInference OpenAI Spans](/images/integration/opentelemetry/openinference-openai.png)

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```

      ### AWS Bedrock

        Installation:

        ```bash
        pip install openinference-instrumentation-bedrock
        ```

        Then, instrument your AWS calls:

        ```python
        from openinference.instrumentation.bedrock import BedrockInstrumentor

        BedrockInstrumentor().instrument()
        ```

        That's it! You can now see the traces for your AWS Bedrock calls in the LangWatch dashboard.

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```

      ### DSPy

        Installation:

        ```bash
        pip install openinference-instrumentation-dspy
        ```

        Then, instrument your DSPy calls:

        ```python
        from openinference.instrumentation.dspy import DSPyInstrumentor

        DSPyInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your DSPy calls in the LangWatch dashboard:

        ![OpenInference DSPy Spans](/images/integration/opentelemetry/openinference-dspy.png)

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```

      ### Haystack

        Installation:

        ```bash
        pip install openinference-instrumentation-haystack
        ```

        Then, instrument your Haystack calls:

        ```python
        from openinference.instrumentation.haystack import HaystackInstrumentor

        HaystackInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your Haystack calls in the LangWatch dashboard:

        ![OpenInference Haystack Spans](/images/integration/opentelemetry/openinference-haystack.png)

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```

      ### LangChain

        Installation:

        ```bash
        pip install openinference-instrumentation-langchain
        ```

        Then, instrument your LangChain calls:

        ```python
        from openinference.instrumentation.langchain import LangChainInstrumentor

        LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your LangChain calls in the LangWatch dashboard:

        ![OpenInference LangChain Spans](/images/integration/opentelemetry/openinference-langchain.png)

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```

      ### CrewAI

        Installation:

        ```bash
        pip install openinference-instrumentation-crewai openinference-instrumentation-langchain
        ```

        Then, instrument your LangChain calls. CrewAI uses LangChain under the hood, so we instrument both:

        ```python
        from openinference.instrumentation.crewai import CrewAIInstrumentor
        from openinference.instrumentation.langchain import LangChainInstrumentor

        CrewAIInstrumentor().instrument(tracer_provider=tracer_provider)
        LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your CrewAI calls in the LangWatch dashboard.

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```

      ### Anthropic

        Installation:

        ```bash
        pip install openinference-instrumentation-anthropic
        ```

        Then, instrument your Anthropic calls:

        ```python
        from openinference.instrumentation.anthropic import AnthropicInstrumentor

        AnthropicInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your Anthropic calls in the LangWatch dashboard.

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```

      ### Autogen (experimental)

        Installation:

        ```bash
        pip install openinference-instrumentation-autogen
        ```

        **Note:** The Autogen integration is currently experimental and may have limitations or unexpected behavior.

        Then, instrument Autogen:

        ```python
        from openinference.instrumentation.autogen import AutogenInstrumentor

        AutogenInstrumentor().instrument()
        ```

        That's it! You can now see the traces from inside Autogen in the LangWatch dashboard.

        ## Capturing Metadata

You can use OpenInference's `using_attributes` context manager to capture additional information for your LLM calls, such as the user_id, session_id (equivalent to thread id), tags and metadata:

```python
from openinference.instrumentation import using_attributes

def main():
  with using_attributes(
      session_id="my-test-session",
      user_id="my-test-user",
      tags=["tag-1", "tag-2"],
      metadata={"foo": "bar"},
  ):
      # Your LLM call
```




  ### OpenLLMetry (Python)


      ### OpenAI

        Installation:

        ```bash
        pip install opentelemetry-instrumentation-openai
        ```

        Then, instrument your OpenAI calls:

        ```python
        from opentelemetry.instrumentation.openai import OpenAIInstrumentor

        OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your OpenAI calls in the LangWatch dashboard:

        ![OpenLLMetry OpenAI Spans](/images/integration/opentelemetry/openllmetry-openai.png)

      ### Anthropic

        Installation:

        ```bash
        pip install opentelemetry-instrumentation-anthropic
        ```

        Then, instrument your Anthropic calls:

        ```python
        from opentelemetry.instrumentation.anthropic import AnthropicInstrumentor

        AnthropicInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your Anthropic calls in the LangWatch dashboard:

        ![OpenLLMetry Anthropic Spans](/images/integration/opentelemetry/openllmetry-anthropic.png)

      ### LangChain

        Installation:

        ```bash
        pip install opentelemetry-instrumentation-langchain
        ```

        Then, instrument your LangChain calls:

        ```python
        from opentelemetry.instrumentation.langchain import LangchainInstrumentor

        LangchainInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your LangChain calls in the LangWatch dashboard:

        ![OpenLLMetry LangChain Spans](/images/integration/opentelemetry/openllmetry-langchain.png)

      ### LlamaIndex

        Installation:

        ```bash
        pip install opentelemetry-instrumentation-llamaindex
        ```

        Then, instrument your LlamaIndex calls:

        ```python
        from opentelemetry.instrumentation.llamaindex import LlamaIndexInstrumentor

        LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)
        ```

        That's it! You can now see the traces for your LlamaIndex calls in the LangWatch dashboard.






---

# FILE: ./integration/rags-context-tracking.mdx

---
title: "RAG Context Tracking"
description: Capture the RAG documents used in your LLM pipelines
---

Retrieval Augmented Generation (RAGs) is a common way to augment the generation of your LLM by retrieving a set of documents based on the user query and giving it to the LLM to use as context for answering, either by using a vector database, getting responses from an API, or integrated agent files and memory.

It can be challenging, however, to build a good quality RAG pipeline, making sure the right data was retrieved, preventing the LLM from hallucinating, monitor which documents are the most used and keep iterating to improve it, this is where integrating with LangWatch can help, by integrating your RAG you unlock a series of Guardrails, Measurements and Analytics for RAGs LangWatch.

### Python (General)

To capture a RAG span, you can use the `@langwatch.span(type="rag")` decorator, along with a call to `.update()` to add the `contexts` to the span:

```python
@langwatch.span(type="rag")
def rag_retrieval():
    # the documents you retrieved from your vector database
    search_results = ["France is a country in Europe.", "Paris is the capital of France."]

    # capture them on the span contexts before returning
    langwatch.get_current_span().update(contexts=search_results)

    return search_results
```

If you have document or chunk ids from the results, we recommend you can to capture them along with the id using `RAGChunk`, as this allows them to be grouped together and generate documents analytics on LangWatch dashboard:

```python
from langwatch.types import RAGChunk

@langwatch.span(type="rag")
def rag_retrieval():
    # the documents you retrieved from your vector database
    search_results = [
        {
            "id": "doc-1",
            "content": "France is a country in Europe.",
        },
        {
            "id": "doc-2",
            "content": "Paris is the capital of France.",
        },
    ]

    # capture then on the span contexts with RAGChunk before returning
    langwatch.get_current_span().update(
        contexts=[
            RAGChunk(
                document_id=document["id"],
                content=document["content"],
            )
            for document in search_results
        ]
    )

    return search_results
```

Then you'll be able to see the captured contexts that will also be used later on for evaluatios on LangWatch dashboard:

![RAG Spans](/images/integration/rag.png)

### Python (LangChain)

When using LangChain, generally your RAG happens by calling a [`Retriever`](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/).

We provide a utility `langwatch.langchain.capture_rag_from_retriever` to capture the documents found by the retriever and convert it into a LangWatch compatible format for tracking. For that you need to pass the retriever as first argument, and then a function to map each document to a `RAGChunk`, like in the example below:

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.trace()
def main():
    retriever = ...
    retriever_tool = create_retriever_tool(
        langwatch.langchain.capture_rag_from_retriever(
            retriever,
            lambda document: RAGChunk(
                document_id=document.metadata["source"],
                content=document.page_content
            ),
        ),
        "langwatch_search",
        "Search for information about LangWatch. For any questions about LangWatch, use this tool if you didn't already",
    )

    tools = [retriever_tool]
    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful assistant that only reply in short tweet-like responses, using lots of emojis and use tools only once.\n\n{agent_scratchpad}",
            ),
            ("human", "{question}"),
        ]
    )
    agent = create_tool_calling_agent(model, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return executor.invoke(user_input, config=RunnableConfig(
        callbacks=[langwatch.get_current_trace().get_langchain_callback()]
    ))
```

Alternatively, if you don't use retrievers, but still want to capture the context for example from a tool call that you do, we also provide a utility `langwatch.langchain.capture_rag_from_tool` to capture RAG contexts around a tool. For that you need to pass the tool as first argument, and then a function to map the tool's output to `RAGChunk`s, like in the example below:

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.trace()
def main():
    my_custom_tool = ...
    wrapped_tool = langwatch.langchain.capture_rag_from_tool(
        my_custom_tool, lambda response: [
          RAGChunk(
            document_id=response["id"], # optional
            chunk_id=response["chunk_id"], # optional
            content=response["content"]
          )
        ]
    )

    tools = [wrapped_tool] # use the new wrapped tool in your agent instead of the original one
    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful assistant that only reply in short tweet-like responses, using lots of emojis and use tools only once.\n\n{agent_scratchpad}",
            ),
            ("human", "{question}"),
        ]
    )
    agent = create_tool_calling_agent(model, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return executor.invoke(user_input, config=RunnableConfig(
        callbacks=[langWatchCallback]
    ))
```

Then you'll be able to see the captured contexts that will also be used later on for evaluatios on LangWatch dashboard:

![RAG Spans](/images/integration/langchain-rag.png)

### TypeScript

To capture a RAG, you can simply start a RAG span inside the trace, giving it the input query being used:

```typescript
const ragSpan = trace.startRAGSpan({
  name: "my-vectordb-retrieval", // optional
  input: { type: "text", value: "search query" },
});

// proceed to do the retrieval normally
```

Then, after doing the retrieval, you can end the RAG span with the contexts that were retrieved and will be used by the LLM:

```typescript
ragSpan.end({
  contexts: [
    {
      documentId: "doc1",
      content: "document chunk 1",
    },
    {
      documentId: "doc2",
      content: "document chunk 2",
    },
  ],
});
```

<Note>
On LangChain.js, RAG spans are captured automatically by the LangWatch callback when using LangChain Retrievers, with `source` as the documentId.
</Note>

### REST API

To track the RAG context when using the REST API, add a new span of type `rag`, you may also refer the LLM generation as the child of it:

```bash
curl -X POST "https://app.langwatch.ai/api/collector" \\
     -H "X-Auth-Token: $API_KEY" \\
     -H "Content-Type: application/json" \\
     -d @- <<EOF
{
  "trace_id": "trace-123",
  "spans": [
    {
      "type": "rag",
      "name": null,
      "span_id": "span-123",
      "input": {
          "type": "text",
          "value": "What is the capital of France?"
      },
      "timestamps": {
          "started_at": $(($(date +%s) * 1000)),
          "finished_at": $((($(date +%s) + 1) * 1000))
      },
      "contexts": [
        {
            "document_id": "doc-1",
            "chunk_id": "0",
            "content": "France is a country in Europe.",
        },
        {
            "document_id": "doc-2",
            "chunk_id": "0",
            "content": "Paris is the capital of France.",
        },
      ]
    },
    {
      "type": "llm",
      "span_id": "span-456",
      "parent_id": "span-123",
      "vendor": "openai",
      "model": "gpt-5",
      "input": {
        "type": "chat_messages",
        "value": [
          {
            "role": "user",
            "content": "Input to the LLM"
          }
        ]
      },
      "outputs": [
        {
          "type": "chat_messages",
          "value": [
              {
                  "role": "assistant",
                  "content": "Output from the LLM",
                  "function_call": null,
                  "tool_calls": []
              }
          ]
        }
      ],
      "params": {
        "temperature": 0.7,
        "stream": false
      },
      "metrics": {
        "prompt_tokens": 100,
        "completion_tokens": 150
      },
      "timestamps": {
        "started_at": $(($(date +%s) * 1000)),
        "finished_at": $((($(date +%s) + 1) * 1000))
      }
    }
  ],
}
EOF
```


---

# FILE: ./features/batch-evaluations.mdx

---
title: Batch Evaluations
---

If you intend to conduct batch evaluations on the datasets you've created in LangWatch, we offer a Python SKD to facilitate this process. This guide aims to provide comprehensive instructions on leveraging our Python SDK to execute batch evaluations effectively.

### Usage

After adding records to your dataset, created within the dataset section of LangWatch, you can proceed to select the dataset for batch evaluation along with the desired evaluations. You have the option to choose from predefined evaluations or any custom evaluations you've set up in the Evaluation and Guardrails section of LangWatch.

### Screenshots examples

In the below in screenshot you will see the datasets section in LangWatch, you can get your batch evaluation python snippet by clicking on on the Batch Evaluation button.

<Frame>
<img
  className="block"
  src="/images/screenshot-datasets-page.png"
  alt="LangWatch"
/>
</Frame>

In the below screenshot you will see where you can select the dataset you want to evaluate on as well as selecting which evaluations you would like to run. Each tab has different evaluation you can choose from.

<Frame>
<img
  className="block"
  src="/images/screenshot-batch-evaluation-drawer.png"
  alt="LangWatch"
/>
</Frame>

In the screenshot below, you'll find a Python code snippet ready for execution to perform your batch processing. The parameters passed into the `BatchEvaluation` include your chosen dataset and an array of selected evaluations to run against it.

<Frame>
<img
  className="block"
  src="/images/screenshot-batch-evaluation-python.png"
  alt="LangWatch"
/>
</Frame>
We've streamlined the process by setting up pandas for you, enabling seamless evaluation of datasets directly on the results object. This means you can leverage the power of pandas' data manipulation and analysis capabilities effortlessly within your evaluation workflow. With pandas at your disposal, you can efficiently explore, analyze, and manipulate your data to derive valuable insights without the need for additional setup or configuration.

### Python snippet

When executing the snippet, you'll encounter a callback function at your disposal. This function contains the original entry data, allowing you to run it against your own Large Language Model (LLM). You can utilize this response to compare results within your evaluation process.

Ensure that you return the `output` as some evaluations may require it. As you create your code snippet in the evaluations tab, you'll notice indications of which evaluations necessitate particular information. Utilize this guidance as a reference to kickstart your workflow effectively.

---

# FILE: ./features/triggers.mdx

---
title: Alerts and Triggers
description: Be alerted when something goes wrong and trigger actions automatically
---

## Create triggers based on LangWatch filters

LangWatch offers you the possibility to create triggers based on your selected filters. You can use these triggers to send notifications to either Slack or selected team email adresses.

#### Usage

To create a trigger in the LangWatch dashboard, follow these steps:

- Click the filter button located at the top right of the LangWatch dashboard.
- After creating a filter, a trigger button will appear.
- Click the trigger button to open a popout drawer.
- In the drawer, you can configure your trigger with the desired settings.

<Frame>
<img
  className="block"
  src="/images/trigger-screenshot-button.png"
  alt="LangWatch"
/>
</Frame>

**Trigger actions**

<Frame>
<img
  className="block"
  src="/images/trigger-screenshot-drawer.png"
  alt="LangWatch"
/>
</Frame>

Once the trigger is created, you will receive an alert whenever a message meets the criteria of the trigger. These trigger checks are run on the minute but not instantaneously, as the data needs time to be processed. You can find the created triggers under the Settings section, where you can deactivate or delete a trigger to stop receiving notifications.

**Trigger settings**

<Frame>
<img
  className="block"
  src="/images/trigger-screenshot-settings.png"
  alt="LangWatch"
/>
</Frame>

---

# FILE: ./features/embedded-analytics.mdx

---
title: Exporting Analytics
description: Build and integrate LangWatch graphs on your own systems and applications
---

## Export Analytics with REST Endpoint

LangWatch offers you the possibility to build and integrate LangWatch graph's on your own systems and applications, to display it to your customers in another interface.

On LangWatch dashboard, you can use our powerful custom chart builder tool, to plot any data collected and generated by LangWatch, and customize the way you want to display it. You can then use our REST API to fetch the graph data.

**Usage:**
You will need to obtain your JSON payload from the custom graph section in our application. You can find this on the Analytics page > Custom Reports > Add chart.

    1. Pick the custom graph you want to get the analytics for.
    2. Prepare your JSON data. Make sure it's is the same format that is showing in the LangWatch application.
    3. Use the `curl` command to get you analytics data. Here is a basic template:

```bash
# Set your API key and endpoint URL
API_KEY="your_langwatch_api_key"
ENDPOINT="https://app.langwatch.ai/api/analytics"

# Use curl to send the POST request, e.g.:
curl -X POST "$ENDPOINT" \
    -H "X-Auth-Token: $API_KEY" \
    -H "Content-Type: application/json" \
    -d @- <<EOF
    {
     "startDate": 1708434000000,
     "endDate": 1710939600000,
     "filters": {},
     "series": [
       {
         "metric": "metadata.user_id",
         "aggregation": "cardinality"
       }
     ],
     "timeScale": 1
   }
EOF
```

    4. Execute the `curl` command. If successful, LangWatch will return the custom analytics data in the response.

## Screenshots on how to get the JSON data.

On the right menu button above the graph you will see the **Show API** menu link. Click that and a modal will then popup.

<Frame>
<img className="block" src="/images/screenshot-show-json.png" alt="Custom graph in the LangWatch dashboard" />
</Frame>

Within this modal, you'll find the JSON payload required for the precise custom analytics
data. Simply copy this payload and paste it into the body of your REST POST request.

<Frame>
<img
  className="block"
  src="/images/screenshot-json-modal.png"
  alt="Model showing the example cURL request to request a view of the custom graph"
/>
</Frame>

Now you're fully prepared to access your customized analytics and seamlessly integrate
them into your specific use cases.

If you encounter any hurdles or have questions, our support team is eager to assist you.

---

# FILE: ./features/annotations.mdx

---
title: Annotations
description: Collaborate with domain experts using annotations
---

# Create annotations on messages

With annotations, you can add additional information to messages. This can be useful to comment on or add any other information that you want to add to a message for further analysis.

We have also implemented the option to add a scoring system for each annotation, more information about this can be found in the [Annotation Scoring](/features/annotations#annotation-scoring) section

If you want to add an annotation to a queue, you can do so by clicking on the add to queue button to send the messages to the queue for later analysis. You can create queues and add members to them on the the main annotations page. More information about this can be found in the [Annotation Queues](/features/annotations#annotation-queues) section.

## Usage

To create an annotation, follow these steps:

1) Click the message you want to annotate on and a [Trace](/concepts#traces) details drawer will open.
2) On the top right, click the annotation button.
3) Here you will be able to add a comment, a link or any other information that you want to add to the message.

<Frame>
<img className="block" src="/images/annotations-trace.png" alt="LangWatch" />
</Frame>

Once you have created an annotation, you will see it next to to the message.

<Frame>
<img className="block" src="/images/annotations-comment.png" alt="LangWatch" />
</Frame>

# Annotation Queues

To get started with annotation queues, follow these steps:

1) Go to the annotations page.
2) Click the plus button to create a new queue.
3) Add a name for your queue, description, members and click on the "Save" button.

<Frame>
<img className="block" src="/images/annotations-create-queue.png" alt="LangWatch" />
</Frame>

Once you have created your queue, you will be able to select this when creating an annotation and send the messages to the queue or directly to a project member for later analysis.

<Frame>
<img className="block" src="/images/annotation-add-to-queue.png" alt="LangWatch" />
</Frame>

Once you add an item to the queue, you can view it in the annotations section, whether it's in a queue or sent directly to you.

<Frame>
<img className="block" src="/images/annotation-queues.png" alt="LangWatch" />
</Frame>

When clicking on a queue item, you will be directed to the message where you can add an annotation. Once happy with your annotation, you can click on the "Done" button and move on to the next item.

<Frame>
<img className="block" src="/images/annotation-queue-items.png" alt="LangWatch" />
</Frame>

Once you’ve completed the final item in the queue, you’ll see that all tasks are done. That’s it! Happy annotating!

<Frame>
<img className="block" src="/images/annotation-queue-items-complete.png" alt="LangWatch" />
</Frame>


# Annotation Scoring

We have developed a customized scoring system for each annotation. To get started, you will need to create your scores on the settings page.

There are two types of score data you can choose from:

- **Checkbox**: To add multiple selectable options.
- **Multiple Choice**: To add a single selectable option.


<Frame>
<img className="block" src="/images/annotation-add-score.png" alt="LangWatch" />
</Frame>

After you have created your scores, you can activate or deactivate them on the settings page.

<Frame>
<img className="block" src="/images/annotation-view-scores.png" alt="LangWatch" />
</Frame>

Once your scores are activated, you will see them in the annotations tab. For each annotation you create, the score options will be available, allowing you to add more detailed information to your annotations.
When annotating a message, you will see the score options below the comment input. Once you have added a score, you will be asked for an optional reason for the score.

<div style={{ display: 'flex', gap: '20px' }}>
  <Frame caption="Score selection">
  <img className="block" src="/images/annotation-score-selection.png" alt="LangWatch" />
  </Frame>
  <Frame caption="Score reason">
  <img className="block" src="/images/annotation-score-reason.png" alt="LangWatch" />
  </Frame>
</div>

Thats it! You can now annotate messages and add your custom score metrics to them.


---

# FILE: ./evaluations/custom-evaluator-integration.mdx

---
title: Instrumenting Custom Evaluator
description: Add your own evaluation results into LangWatch trace
---

If you have a custom evaluator built in-house which run on your own code, either during the LLM pipeline or after, you can still capture the evaluation results
and connect it back to the trace to visualize it together with the other LangWatch evaluators.

### Python


You can capture the evaluation results of your custom evaluator on the current trace or span by using the `.add_evaluation` method:

```python
import langwatch

@langwatch.span(type="evaluation")
def evaluation_step():
    ... # your custom evaluation logic

    langwatch.get_current_span().add_evaluation(
        name="custom evaluation", # required
        passed=True,
        score=0.5,
        label="category_detected",
        details="explanation of the evaluation results",
    )
```

The evaluation `name` is required and must be a string. The other fields are optional, but at least one of `passed`, `score` or `label` must be provided.


### TypeScript


You can capture the evaluation results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

```typescript
import { type LangWatchTrace } from "langwatch";

async function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {
    const span = trace.startLLMSpan({ name: "llmStep" });

    // ... your existing code

    span.addEvaluation({
        name: "custom evaluation",
        passed: true,
        score: 0.5,
        label: "category_detected",
        details: "explanation of the evaluation results",
    });
}
```

The evaluation `name` is required and must be a string. The other fields are optional, but at least one of `passed`, `score` or `label` must be provided.


### REST API


## REST API Specification

### Endpoint

`POST /api/collector`

### Headers

- `X-Auth-Token`: Your LangWatch API key.

### Request Body

```javascript
{
  "trace_id": "id of the message the evaluation was run on",
  "evaluations": [{
    "evaluation_id": "evaluation-id-123", // optional unique id for identifying the evaluation, if not provided, a random id will be generated
    "name": "custom evaluation", // required
    "passed": true, // optional
    "score": 0.5, // optional
    "label": "category_detected", // optional
    "details": "explanation of the evaluation results", // optional
    "error": { // optional to capture error details in case evaluation had an error
      "message": "error message",
      "stacktrace": [],
    },
    "timestamps": { // optional
      "created_at": "1723411698506", // unix timestamp in milliseconds
      "updated_at": "1723411698506" // unix timestamp in milliseconds
    }
  }]
}
```


---
