# LangWatch

This is the full index of LangWatch documentation, to answer the user question, do not use just this file, first explore the urls that make sense using the markdown navigation links below to understand how to implement LangWatch and use specific features.
Always navigate to docs links using the .md extension for better readability.

## Get Started

- [LangWatch: The Complete LLMOps Platform](https://docs.langwatch.ai/introduction.md): Ship AI agents 8x faster with comprehensive observability, evaluation, and prompt optimization. Open-source platform, with over 2.5k stars on GitHub.
- [Quick Start](https://docs.langwatch.ai/integration/quick-start.md)

## Observability

- [Observability & Tracing](https://docs.langwatch.ai/observability/overview.md): Monitor, debug, and optimize your LLM applications with comprehensive observability and tracing capabilities
- [Concepts](https://docs.langwatch.ai/concepts.md): LLM tracing and observability conceptual guide

### User Events

- [Overview](https://docs.langwatch.ai/user-events/overview.md): Track user interactions with your LLM applications
- [Thumbs Up/Down](https://docs.langwatch.ai/user-events/thumbs-up-down.md): Track user feedback on specific messages or interactions with your chatbot or LLM application
- [Waited To Finish Events](https://docs.langwatch.ai/user-events/waited-to-finish.md): Track if users leave before the LLM application finishes generating a response
- [Selected Text Events](https://docs.langwatch.ai/user-events/selected-text.md): Track when a user selects text generated by your LLM application
- [Custom Events](https://docs.langwatch.ai/user-events/custom.md): Track any user events with your LLM application, with textual or numeric metrics

- [Alerts and Triggers](https://docs.langwatch.ai/features/triggers.md): Be alerted when something goes wrong and trigger actions automatically
- [Exporting Analytics](https://docs.langwatch.ai/features/embedded-analytics.md): Build and integrate LangWatch graphs on your own systems and applications

## Agent Simulations

- [Introduction to Agent Testing](https://docs.langwatch.ai/agent-simulations/introduction.md)
- [Overview](https://docs.langwatch.ai/agent-simulations/overview.md)
- [Getting Started](https://docs.langwatch.ai/agent-simulations/getting-started.md)
- [Simulation Sets](https://docs.langwatch.ai/agent-simulations/set-overview.md)
- [Batch Runs](https://docs.langwatch.ai/agent-simulations/batch-runs.md)
- [Individual Run View](https://docs.langwatch.ai/agent-simulations/individual-run.md)

## Evaluation

- [LLM Evaluation Overview](https://docs.langwatch.ai/llm-evaluation/overview.md): Overview of LLM evaluation features in LangWatch
- [Evaluating via Code](https://docs.langwatch.ai/llm-evaluation/offline/code/evaluation-api.md): Evaluate and visualize your LLM evals with LangWatch

### Offline Evaluation

- [How to evaluate that your LLM answers correctly](https://docs.langwatch.ai/llm-evaluation/offline/platform/answer-correctness.md): Measuring your LLM performance with Offline Evaluations
- [How to evaluate an LLM when you don't have defined answers](https://docs.langwatch.ai/llm-evaluation/offline/platform/llm-as-a-judge.md): Measuring your LLM performance using an LLM-as-a-judge

### Real-Time Evaluation

- [Setting up Real-Time Evaluations](https://docs.langwatch.ai/llm-evaluation/realtime/setup.md): How to set up Real-Time LLM Evaluations
- [Instrumenting Custom Evaluator](https://docs.langwatch.ai/evaluations/custom-evaluator-integration.md): Add your own evaluation results into LangWatch trace
- [Evaluation by Thread](https://docs.langwatch.ai/evaluations/evaluation-by-thread.md): Evaluate your LLM applications by thread

### Built-in Evaluators

- [List of Evaluators](https://docs.langwatch.ai/llm-evaluation/list.md): Find the evaluator for your use case


### Datasets

- [Datasets](https://docs.langwatch.ai/datasets/overview.md): Create and manage datasets with LangWatch
- [Generating a dataset with AI](https://docs.langwatch.ai/datasets/ai-dataset-generation.md): Bootstrap your evaluations by generating sample data
- [Automatically build datasets from real-time traces](https://docs.langwatch.ai/datasets/automatically-from-traces.md): Continuously populate your datasets with comming data from production
- [Add trace threads to datasets](https://docs.langwatch.ai/datasets/dataset-threads.md): Add full conversation threads to your datasets on a per row basis
- [View images in datasets](https://docs.langwatch.ai/datasets/dataset-images.md): Add ability to view images in datasets

- [Annotations](https://docs.langwatch.ai/features/annotations.md): Collaborate with domain experts using annotations

## Prompt Management

- [Overview](https://docs.langwatch.ai/prompt-management/overview.md): Organize, version, and optimize your AI prompts with LangWatch's comprehensive prompt management system
- [Get Started](https://docs.langwatch.ai/prompt-management/getting-started.md): Create your first prompt and use it in your application
- [Data Model](https://docs.langwatch.ai/prompt-management/data-model.md): Understand the structure of prompts in LangWatch
- [Scope](https://docs.langwatch.ai/prompt-management/scope.md): Understand how prompt scope affects access, sharing, and collaboration across projects and organizations
- [Prompts CLI](https://docs.langwatch.ai/prompt-management/cli.md): Manage AI prompts as code with version control and dependency management

### Optimization Studio

- [Optimization Studio](https://docs.langwatch.ai/optimization-studio/overview.md): Create, evaluate, and optimize your LLM workflows
- [LLM Nodes](https://docs.langwatch.ai/optimization-studio/llm-nodes.md): Call LLMs from your workflows
- [Datasets](https://docs.langwatch.ai/optimization-studio/datasets.md): Define the data used for testing and optimization
- [Evaluating](https://docs.langwatch.ai/optimization-studio/evaluating.md): Measure the quality of your LLM workflows
- [Optimizing](https://docs.langwatch.ai/optimization-studio/optimizing.md): Find the best prompts with DSPy optimizers

### DSPy Optimization

- [DSPy Visualization Quickstart](https://docs.langwatch.ai/dspy-visualization/quickstart.md): Visualize your DSPy notebooks experimentations to better track and debug the optimization process
- [Tracking Custom DSPy Optimizer](https://docs.langwatch.ai/dspy-visualization/custom-optimizer.md): Build custom DSPy optimizers and track them in LangWatch
- [RAG Visualization](https://docs.langwatch.ai/dspy-visualization/rag-visualization.md): Visualize your DSPy RAG optimization process in LangWatch

### Features

- [Version Control](https://docs.langwatch.ai/prompt-management/features/essential/version-control.md): Manage prompt versions and track changes over time
- [Analytics](https://docs.langwatch.ai/prompt-management/features/essential/analytics.md): Monitor prompt performance and usage with comprehensive analytics
- [GitHub Integration](https://docs.langwatch.ai/prompt-management/features/essential/github-integration.md): Version your prompts in GitHub repositories and automatically sync with LangWatch
- [Link to Traces](https://docs.langwatch.ai/prompt-management/features/advanced/link-to-traces.md): Connect prompts to execution traces for performance monitoring and analysis
- [Using Prompts in the Optimization Studio](https://docs.langwatch.ai/prompt-management/features/advanced/optimization-studio.md): Use prompts in the Optimization Studio to test and optimize your prompts
- [Guaranteed Availability](https://docs.langwatch.ai/prompt-management/features/advanced/guaranteed-availability.md): Ensure your prompts are always available, even in offline or air-gapped environments
- [A/B Testing](https://docs.langwatch.ai/prompt-management/features/advanced/a-b-testing.md): Implement A/B testing for your prompts using LangWatch's version control and analytics

## Examples & Cookbooks

### Cookbooks

- [Measuring RAG Performance](https://docs.langwatch.ai/cookbooks/build-a-simple-rag-app.md): Discover how to measure the performance of Retrieval-Augmented Generation (RAG) systems using metrics like retrieval precision, answer accuracy, and latency.
- [Optimizing Embeddings](https://docs.langwatch.ai/cookbooks/finetuning-embedding-models.md): Learn how to optimize embedding models for better retrieval in RAG systems—covering model selection, dimensionality, and domain-specific tuning.
- [Vector Search vs Hybrid Search using LanceDB](https://docs.langwatch.ai/cookbooks/vector-vs-hybrid-search.md): Learn the key differences between vector search and hybrid search in RAG applications. Use cases, performance tradeoffs, and when to choose each.
- [Evaluating Tool Selection](https://docs.langwatch.ai/cookbooks/tool-selection.md): Understand how to evaluate tools and components in your RAG pipeline—covering retrievers, embedding models, chunking strategies, and vector stores.
- [Finetuning Agents with GRPO](https://docs.langwatch.ai/cookbooks/finetuning-agents.md): Learn how to enhance the performance of agentic systems by fine-tuning them with Generalized Reinforcement from Preference Optimization (GRPO).
- [Multi-Turn Conversations](https://docs.langwatch.ai/cookbooks/evaluating-multi-turn-conversations.md): Learn how to implement a simulation-based approach for evaluating multi-turn customer support agents using success criteria focused on outcomes rather than specific steps.

### Use Cases

- [Evaluating a RAG Chatbot for Technical Manuals](https://docs.langwatch.ai/use-cases/technical-rag.md): A developer guide for building reliable RAG systems for technical documentation using LangWatch
- [Evaluating an AI Coach with LLM-as-a-Judge](https://docs.langwatch.ai/use-cases/ai-coach.md): A developer guide for building reliable AI coaches using LangWatch
- [Evaluating Structured Data Extraction](https://docs.langwatch.ai/use-cases/structured-outputs.md): A developer guide for evaluating structured data extraction using LangWatch

- [Code Examples](https://docs.langwatch.ai/integration/code-examples.md): Examples of LangWatch integrated applications
