{
  "openapi": "3.1.0",
  "info": {
    "title": "LangEvals API",
    "version": "1.0.0",
    "description": "API for LangEvals evaluators"
  },
  "servers": [
    {
      "url": "https://app.langwatch.ai/api/evaluations",
      "description": "Production server"
    }
  ],
  "security": [
    {
      "api_key": []
    }
  ],
  "paths": {
    "/huggingface/llama_guard/evaluate": {
      "post": {
        "summary": "Llama Guard",
        "description": "This evaluator is a special version of Llama trained strictly\nfor acting as a guardrail, following customizable guidelines.\nIt can work both as a safety evaluator and as policy enforcement.",
        "operationId": "huggingface_llama_guard_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/huggingface_llama_guardRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/huggingface_llama_guardSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"huggingface/llama_guard\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"huggingface/llama_guard\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"huggingface/llama_guard\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/basic/evaluate": {
      "post": {
        "summary": "Custom Basic Evaluator",
        "description": "Allows you to check for simple text matches or regex evaluation.",
        "operationId": "langevals_basic_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_basicRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_basicSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/basic\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/basic\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/basic\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/competitor_blocklist/evaluate": {
      "post": {
        "summary": "Competitor Blocklist",
        "description": "This evaluator checks if any of the specified competitors was mentioned",
        "operationId": "langevals_competitor_blocklist_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_competitor_blocklistRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_competitor_blocklistSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/competitor_blocklist\",\n        index=index,\n        data={\n            \"output\": output,\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/competitor_blocklist\",\n        data={\n            \"output\": \"\",\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/competitor_blocklist\",\n        name: \"\",\n\n        output: \"\", # optional\n        input: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/competitor_llm/evaluate": {
      "post": {
        "summary": "Competitor Allowlist Check",
        "description": "This evaluator use an LLM-as-judge to check if the conversation is related to competitors, without having to name them explicitly",
        "operationId": "langevals_competitor_llm_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_competitor_llmRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_competitor_llmSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/competitor_llm\",\n        index=index,\n        data={\n            \"output\": output,\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/competitor_llm\",\n        data={\n            \"output\": \"\",\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/competitor_llm\",\n        name: \"\",\n\n        output: \"\", # optional\n        input: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/competitor_llm_function_call/evaluate": {
      "post": {
        "summary": "Competitor LLM Check",
        "description": "This evaluator implements LLM-as-a-judge with a function call approach to check if the message contains a mention of a competitor.",
        "operationId": "langevals_competitor_llm_function_call_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_competitor_llm_function_callRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_competitor_llm_function_callSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/competitor_llm_function_call\",\n        index=index,\n        data={\n            \"output\": output,\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/competitor_llm_function_call\",\n        data={\n            \"output\": \"\",\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/competitor_llm_function_call\",\n        name: \"\",\n\n        output: \"\", # optional\n        input: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/exact_match/evaluate": {
      "post": {
        "summary": "Exact Match Evaluator",
        "description": "A simple evaluator that checks if the output matches the expected_output exactly.",
        "operationId": "langevals_exact_match_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_exact_matchRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_exact_matchSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/exact_match\",\n        index=index,\n        data={\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/exact_match\",\n        data={\n            \"output\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/exact_match\",\n        name: \"\",\n\n        output: \"\", # optional\n        expected_output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/llm_answer_match/evaluate": {
      "post": {
        "summary": "LLM Answer Match",
        "description": "Uses an LLM to check if the generated output answers a question correctly the same way as the expected output, even if their style is different.",
        "operationId": "langevals_llm_answer_match_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_llm_answer_matchRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_llm_answer_matchSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/llm_answer_match\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/llm_answer_match\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/llm_answer_match\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        expected_output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/llm_boolean/evaluate": {
      "post": {
        "summary": "LLM-as-a-Judge Boolean Evaluator",
        "description": "Use an LLM as a judge with a custom prompt to do a true/false boolean evaluation of the message.",
        "operationId": "langevals_llm_boolean_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_llm_booleanRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_llm_booleanSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/llm_boolean\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/llm_boolean\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/llm_boolean\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        contexts: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/llm_category/evaluate": {
      "post": {
        "summary": "LLM-as-a-Judge Category Evaluator",
        "description": "Use an LLM as a judge with a custom prompt to classify the message into custom defined categories.",
        "operationId": "langevals_llm_category_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_llm_categoryRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_llm_categorySettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/llm_category\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/llm_category\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/llm_category\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        contexts: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/llm_score/evaluate": {
      "post": {
        "summary": "LLM-as-a-Judge Score Evaluator",
        "description": "Use an LLM as a judge with custom prompt to do a numeric score evaluation of the message.",
        "operationId": "langevals_llm_score_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_llm_scoreRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_llm_scoreSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/llm_score\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/llm_score\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/llm_score\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        contexts: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/off_topic/evaluate": {
      "post": {
        "summary": "Off Topic Evaluator",
        "description": "This evaluator checks if the user message is concerning one of the allowed topics of the chatbot",
        "operationId": "langevals_off_topic_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_off_topicRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_off_topicSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/off_topic\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/off_topic\",\n        data={\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/off_topic\",\n        name: \"\",\n        input: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/query_resolution/evaluate": {
      "post": {
        "summary": "Query Resolution",
        "description": "This evaluator checks if all the user queries in the conversation were resolved. Useful to detect when the bot doesn't know how to answer or can't help the user.",
        "operationId": "langevals_query_resolution_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_query_resolutionRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_query_resolutionSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/query_resolution\",\n        index=index,\n        data={\n            \"conversation\": conversation,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/query_resolution\",\n        data={\n            \"conversation\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/query_resolution\",\n        name: \"\",\n        conversation: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/similarity/evaluate": {
      "post": {
        "summary": "Semantic Similarity Evaluator",
        "description": "Allows you to check for semantic similarity or dissimilarity between input and output and a\ntarget value, so you can avoid sentences that you don't want to be present without having to\nmatch on the exact text.",
        "operationId": "langevals_similarity_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_similarityRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_similaritySettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/similarity\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/similarity\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/similarity\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/langevals/valid_format/evaluate": {
      "post": {
        "summary": "Valid Format Evaluator",
        "description": "Allows you to check if the output is a valid json, markdown, python, sql, etc.\nFor JSON, can optionally validate against a provided schema.",
        "operationId": "langevals_valid_format_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/langevals_valid_formatRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/langevals_valid_formatSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"langevals/valid_format\",\n        index=index,\n        data={\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"langevals/valid_format\",\n        data={\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"langevals/valid_format\",\n        name: \"\",\n\n        output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/azure/content_safety/evaluate": {
      "post": {
        "summary": "Azure Content Safety",
        "description": "This evaluator detects potentially unsafe content in text, including hate speech,\nself-harm, sexual content, and violence. It allows customization of the severity\nthreshold and the specific categories to check.",
        "operationId": "azure_content_safety_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/azure_content_safetyRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/azure_content_safetySettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"azure/content_safety\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"azure/content_safety\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"azure/content_safety\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/azure/jailbreak/evaluate": {
      "post": {
        "summary": "Azure Jailbreak Detection",
        "description": "This evaluator checks for jailbreak-attempt in the input using Azure's Content Safety API.",
        "operationId": "azure_jailbreak_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/azure_jailbreakRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"azure/jailbreak\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"azure/jailbreak\",\n        data={\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"azure/jailbreak\",\n        name: \"\",\n        input: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/azure/prompt_injection/evaluate": {
      "post": {
        "summary": "Azure Prompt Shield",
        "description": "This evaluator checks for prompt injection attempt in the input and the contexts using Azure's Content Safety API.",
        "operationId": "azure_prompt_injection_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/azure_prompt_injectionRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"azure/prompt_injection\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"azure/prompt_injection\",\n        data={\n            \"input\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"azure/prompt_injection\",\n        name: \"\",\n        input: \"\", // required\n        contexts: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/openai/moderation/evaluate": {
      "post": {
        "summary": "OpenAI Moderation",
        "description": "This evaluator uses OpenAI's moderation API to detect potentially harmful content in text,\nincluding harassment, hate speech, self-harm, sexual content, and violence.",
        "operationId": "openai_moderation_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/openai_moderationRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/openai_moderationSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"openai/moderation\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"openai/moderation\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"openai/moderation\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/lingua/language_detection/evaluate": {
      "post": {
        "summary": "Lingua Language Detection",
        "description": "This evaluator detects the language of the input and output text to check for example if the generated answer is in the same language as the prompt,\nor if it's in a specific expected language.",
        "operationId": "lingua_language_detection_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/lingua_language_detectionRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/lingua_language_detectionSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"lingua/language_detection\",\n        index=index,\n        data={\n            \"output\": output,\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"lingua/language_detection\",\n        data={\n            \"output\": \"\",\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"lingua/language_detection\",\n        name: \"\",\n        output: \"\", // required\n        input: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/legacy/ragas_answer_correctness/evaluate": {
      "post": {
        "summary": "Ragas Answer Correctness",
        "description": "Computes with an LLM a weighted combination of factual as well as semantic similarity between the generated answer and the expected output.",
        "operationId": "legacy_ragas_answer_correctness_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/legacy_ragas_answer_correctnessRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/legacy_ragas_answer_correctnessSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"legacy/ragas_answer_correctness\",\n        index=index,\n        data={\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"legacy/ragas_answer_correctness\",\n        data={\n            \"output\": \"\",\n            \"expected_output\": \"\",\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"legacy/ragas_answer_correctness\",\n        name: \"\",\n        output: \"\", // required\n        expected_output: \"\", // required\n        input: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/legacy/ragas_answer_relevancy/evaluate": {
      "post": {
        "summary": "Ragas Answer Relevancy",
        "description": "Evaluates how pertinent the generated answer is to the given prompt. Higher scores indicate better relevancy.",
        "operationId": "legacy_ragas_answer_relevancy_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/legacy_ragas_answer_relevancyRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/legacy_ragas_answer_relevancySettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"legacy/ragas_answer_relevancy\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"legacy/ragas_answer_relevancy\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"legacy/ragas_answer_relevancy\",\n        name: \"\",\n        input: \"\", // required\n        output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/legacy/ragas_context_precision/evaluate": {
      "post": {
        "summary": "Ragas Context Precision",
        "description": "This metric evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Higher scores indicate better precision.",
        "operationId": "legacy_ragas_context_precision_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/legacy_ragas_context_precisionRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/legacy_ragas_context_precisionSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"legacy/ragas_context_precision\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"contexts\": row[\"contexts\"],\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"legacy/ragas_context_precision\",\n        data={\n            \"input\": \"\",\n            \"contexts\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"legacy/ragas_context_precision\",\n        name: \"\",\n        input: \"\", // required\n        contexts: \"\", // required\n        expected_output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/legacy/ragas_context_recall/evaluate": {
      "post": {
        "summary": "Ragas Context Recall",
        "description": "This evaluator measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. Higher values indicate better performance.",
        "operationId": "legacy_ragas_context_recall_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/legacy_ragas_context_recallRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/legacy_ragas_context_recallSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"legacy/ragas_context_recall\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"contexts\": row[\"contexts\"],\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"legacy/ragas_context_recall\",\n        data={\n            \"input\": \"\",\n            \"contexts\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"legacy/ragas_context_recall\",\n        name: \"\",\n        input: \"\", // required\n        contexts: \"\", // required\n        expected_output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/legacy/ragas_context_relevancy/evaluate": {
      "post": {
        "summary": "Ragas Context Relevancy",
        "description": "This metric gauges the relevancy of the retrieved context, calculated based on both the question and contexts. The values fall within the range of (0, 1), with higher values indicating better relevancy.",
        "operationId": "legacy_ragas_context_relevancy_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/legacy_ragas_context_relevancyRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/legacy_ragas_context_relevancySettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"legacy/ragas_context_relevancy\",\n        index=index,\n        data={\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"legacy/ragas_context_relevancy\",\n        data={\n            \"output\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"legacy/ragas_context_relevancy\",\n        name: \"\",\n        output: \"\", // required\n        contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/legacy/ragas_context_utilization/evaluate": {
      "post": {
        "summary": "Ragas Context Utilization",
        "description": "This metric evaluates whether all of the output relevant items present in the contexts are ranked higher or not. Higher scores indicate better utilization.",
        "operationId": "legacy_ragas_context_utilization_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/legacy_ragas_context_utilizationRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/legacy_ragas_context_utilizationSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"legacy/ragas_context_utilization\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"legacy/ragas_context_utilization\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"legacy/ragas_context_utilization\",\n        name: \"\",\n        input: \"\", // required\n        output: \"\", // required\n        contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/legacy/ragas_faithfulness/evaluate": {
      "post": {
        "summary": "Ragas Faithfulness",
        "description": "This evaluator assesses the extent to which the generated answer is consistent with the provided context. Higher scores indicate better faithfulness to the context, useful for detecting hallucinations.",
        "operationId": "legacy_ragas_faithfulness_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/legacy_ragas_faithfulnessRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/legacy_ragas_faithfulnessSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"legacy/ragas_faithfulness\",\n        index=index,\n        data={\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"legacy/ragas_faithfulness\",\n        data={\n            \"output\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"legacy/ragas_faithfulness\",\n        name: \"\",\n        output: \"\", // required\n        contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/example/word_count/evaluate": {
      "post": {
        "summary": "Example Evaluator",
        "description": "This evaluator serves as a boilerplate for creating new evaluators.",
        "operationId": "example_word_count_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/example_word_countRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"example/word_count\",\n        index=index,\n        data={\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"example/word_count\",\n        data={\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"example/word_count\",\n        name: \"\",\n        output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/bleu_score/evaluate": {
      "post": {
        "summary": "BLEU Score",
        "description": "Traditional NLP metric. BLEU score for evaluating the similarity between two strings.",
        "operationId": "ragas_bleu_score_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/ragas_bleu_scoreRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/bleu_score\",\n        index=index,\n        data={\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/bleu_score\",\n        data={\n            \"output\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/bleu_score\",\n        name: \"\",\n        output: \"\", // required\n        expected_output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/context_f1/evaluate": {
      "post": {
        "summary": "Context F1",
        "description": "Balances between precision and recall for context retrieval, increasing it means a better signal-to-noise ratio. Uses traditional string distance metrics.",
        "operationId": "ragas_context_f1_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_context_f1Request"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_context_f1Settings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/context_f1\",\n        index=index,\n        data={\n            \"contexts\": row[\"contexts\"],\n            \"expected_contexts\": expected_contexts,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/context_f1\",\n        data={\n            \"contexts\": \"\",\n            \"expected_contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/context_f1\",\n        name: \"\",\n        contexts: \"\", // required\n        expected_contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/context_precision/evaluate": {
      "post": {
        "summary": "Context Precision",
        "description": "Measures how accurate is the retrieval compared to expected contexts, increasing it means less noise in the retrieval. Uses traditional string distance metrics.",
        "operationId": "ragas_context_precision_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_context_precisionRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_context_precisionSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/context_precision\",\n        index=index,\n        data={\n            \"contexts\": row[\"contexts\"],\n            \"expected_contexts\": expected_contexts,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/context_precision\",\n        data={\n            \"contexts\": \"\",\n            \"expected_contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/context_precision\",\n        name: \"\",\n        contexts: \"\", // required\n        expected_contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/context_recall/evaluate": {
      "post": {
        "summary": "Context Recall",
        "description": "Measures how many relevant contexts were retrieved compared to expected contexts, increasing it means more signal in the retrieval. Uses traditional string distance metrics.",
        "operationId": "ragas_context_recall_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_context_recallRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_context_recallSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/context_recall\",\n        index=index,\n        data={\n            \"contexts\": row[\"contexts\"],\n            \"expected_contexts\": expected_contexts,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/context_recall\",\n        data={\n            \"contexts\": \"\",\n            \"expected_contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/context_recall\",\n        name: \"\",\n        contexts: \"\", // required\n        expected_contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/factual_correctness/evaluate": {
      "post": {
        "summary": "LLM Factual Match",
        "description": "Computes with an LLM how factually similar the generated answer is to the expected output.",
        "operationId": "ragas_factual_correctness_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_factual_correctnessRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_factual_correctnessSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/factual_correctness\",\n        index=index,\n        data={\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/factual_correctness\",\n        data={\n            \"output\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/factual_correctness\",\n        name: \"\",\n        output: \"\", // required\n        expected_output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/faithfulness/evaluate": {
      "post": {
        "summary": "Ragas Faithfulness",
        "description": "This evaluator assesses the extent to which the generated answer is consistent with the provided context. Higher scores indicate better faithfulness to the context, useful for detecting hallucinations.",
        "operationId": "ragas_faithfulness_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_faithfulnessRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_faithfulnessSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/faithfulness\",\n        index=index,\n        data={\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n            \"input\": row[\"input\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/faithfulness\",\n        data={\n            \"output\": \"\",\n            \"contexts\": \"\",\n            \"input\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/faithfulness\",\n        name: \"\",\n        output: \"\", // required\n        contexts: \"\", // required\n        input: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/response_context_precision/evaluate": {
      "post": {
        "summary": "Ragas Response Context Precision",
        "description": "Uses an LLM to measure the proportion of chunks in the retrieved context that were relevant to generate the output or the expected output.",
        "operationId": "ragas_response_context_precision_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_response_context_precisionRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_response_context_precisionSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/response_context_precision\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"contexts\": row[\"contexts\"],\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/response_context_precision\",\n        data={\n            \"input\": \"\",\n            \"contexts\": \"\",\n            \"output\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/response_context_precision\",\n        name: \"\",\n        input: \"\", // required\n        contexts: \"\", // required\n        output: \"\", # optional\n        expected_output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/response_context_recall/evaluate": {
      "post": {
        "summary": "Ragas Response Context Recall",
        "description": "Uses an LLM to measure how many of relevant documents attributable the claims in the output were successfully retrieved in order to generate an expected output.",
        "operationId": "ragas_response_context_recall_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_response_context_recallRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_response_context_recallSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/response_context_recall\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/response_context_recall\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n            \"contexts\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/response_context_recall\",\n        name: \"\",\n        input: \"\", // required\n        output: \"\", // required\n        contexts: \"\", // required\n        expected_output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/response_relevancy/evaluate": {
      "post": {
        "summary": "Ragas Response Relevancy",
        "description": "Evaluates how pertinent the generated answer is to the given prompt. Higher scores indicate better relevancy.",
        "operationId": "ragas_response_relevancy_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_response_relevancyRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_response_relevancySettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/response_relevancy\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/response_relevancy\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/response_relevancy\",\n        name: \"\",\n        input: \"\", // required\n        output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/rouge_score/evaluate": {
      "post": {
        "summary": "ROUGE Score",
        "description": "Traditional NLP metric. ROUGE score for evaluating the similarity between two strings.",
        "operationId": "ragas_rouge_score_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_rouge_scoreRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_rouge_scoreSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/rouge_score\",\n        index=index,\n        data={\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/rouge_score\",\n        data={\n            \"output\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/rouge_score\",\n        name: \"\",\n        output: \"\", // required\n        expected_output: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/rubrics_based_scoring/evaluate": {
      "post": {
        "summary": "Rubrics Based Scoring",
        "description": "Rubric-based evaluation metric that is used to evaluate responses. The rubric consists of descriptions for each score, typically ranging from 1 to 5",
        "operationId": "ragas_rubrics_based_scoring_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_rubrics_based_scoringRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_rubrics_based_scoringSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/rubrics_based_scoring\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/rubrics_based_scoring\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n            \"expected_output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/rubrics_based_scoring\",\n        name: \"\",\n        input: \"\", // required\n        output: \"\", // required\n        expected_output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/sql_query_equivalence/evaluate": {
      "post": {
        "summary": "SQL Query Equivalence",
        "description": "Checks if the SQL query is equivalent to a reference one by using an LLM to infer if it would generate the same results given the table schemas.",
        "operationId": "ragas_sql_query_equivalence_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_sql_query_equivalenceRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_sql_query_equivalenceSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/sql_query_equivalence\",\n        index=index,\n        data={\n            \"output\": output,\n            \"expected_output\": row[\"expected_output\"],\n            \"expected_contexts\": expected_contexts,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/sql_query_equivalence\",\n        data={\n            \"output\": \"\",\n            \"expected_output\": \"\",\n            \"expected_contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/sql_query_equivalence\",\n        name: \"\",\n        output: \"\", // required\n        expected_output: \"\", // required\n        expected_contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/ragas/summarization_score/evaluate": {
      "post": {
        "summary": "Summarization Score",
        "description": "Measures how well the summary captures important information from the retrieved contexts.",
        "operationId": "ragas_summarization_score_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/ragas_summarization_scoreRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/ragas_summarization_scoreSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"ragas/summarization_score\",\n        index=index,\n        data={\n            \"output\": output,\n            \"contexts\": row[\"contexts\"],\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"ragas/summarization_score\",\n        data={\n            \"output\": \"\",\n            \"contexts\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"ragas/summarization_score\",\n        name: \"\",\n        output: \"\", // required\n        contexts: \"\", // required\n\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    },
    "/presidio/pii_detection/evaluate": {
      "post": {
        "summary": "Presidio PII Detection",
        "description": "Detects personally identifiable information in text, including phone numbers, email addresses, and\nsocial security numbers. It allows customization of the detection threshold and the specific types of PII to check.",
        "operationId": "presidio_pii_detection_evaluate",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "allOf": [
                  {
                    "$ref": "#/components/schemas/presidio_pii_detectionRequest"
                  },
                  {
                    "type": "object",
                    "properties": {
                      "settings": {
                        "$ref": "#/components/schemas/presidio_pii_detectionSettings"
                      }
                    }
                  }
                ]
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful evaluation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/EvaluationResult"
                  }
                }
              }
            }
          },
          "400": {
            "description": "Bad request",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "detail": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "x-codeSamples": [
          {
            "lang": "python",
            "label": "Offline Evaluation",
            "source": "import langwatch\n\ndf = langwatch.dataset.get_dataset(\"dataset-id\").to_pandas()\n\nevaluation = langwatch.evaluation.init(\"my-incredible-evaluation\")\n\nfor index, row in evaluation.loop(df.iterrows()):\n    # your execution code here       \n    evaluation.run(\n        \"presidio/pii_detection\",\n        index=index,\n        data={\n            \"input\": row[\"input\"],\n            \"output\": output,\n        },\n        settings={}\n    )\n"
          },
          {
            "lang": "python",
            "label": "Realtime Evaluation",
            "source": "import langwatch\n\n@langwatch.span()\ndef llm_step():\n    ... # your existing code\n    result = langwatch.get_current_span().evaluate(\n        \"presidio/pii_detection\",\n        data={\n            \"input\": \"\",\n            \"output\": \"\",\n        },\n        settings={},\n    )\n    print(result)"
          },
          {
            "lang": "typescript",
            "label": "TypeScript",
            "source": "import { type LangWatchTrace } from \"langwatch\";\n\nasync function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {\n    const span = trace.startLLMSpan({ name: \"llmStep\" });\n    \n    // ... your existing code\n\n    // call the evaluator either on a span or on a trace\n    const result = await span.evaluate({\n        evaluator: \"presidio/pii_detection\",\n        name: \"\",\n\n        input: \"\", # optional\n        output: \"\", # optional\n        settings: {},\n    })\n\n    console.log(result);"
          }
        ]
      }
    }
  },
  "components": {
    "schemas": {
      "EvaluationRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              },
              "expected_contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Expected context information"
              },
              "conversation": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "role": {
                      "type": "string",
                      "enum": ["user", "assistant", "system"]
                    },
                    "content": {
                      "type": "string"
                    }
                  },
                  "required": ["role", "content"]
                },
                "description": "Conversation history"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "EvaluationEntry": {
        "type": "object",
        "properties": {
          "input": {
            "type": "string",
            "description": "The input text to evaluate"
          },
          "output": {
            "type": "string",
            "description": "The output text to evaluate"
          },
          "contexts": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "Context information for evaluation"
          },
          "expected_output": {
            "type": "string",
            "description": "Expected output for comparison"
          },
          "expected_contexts": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "Expected context information"
          },
          "conversation": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "role": {
                  "type": "string",
                  "enum": ["user", "assistant", "system"]
                },
                "content": {
                  "type": "string"
                }
              },
              "required": ["role", "content"]
            },
            "description": "Conversation history"
          }
        }
      },
      "EvaluationResult": {
        "type": "object",
        "properties": {
          "status": {
            "type": "string",
            "enum": ["processed", "skipped", "error"]
          },
          "score": {
            "type": "number",
            "description": "Evaluation score"
          },
          "passed": {
            "type": "boolean",
            "description": "Whether the evaluation passed"
          },
          "label": {
            "type": "string",
            "description": "Evaluation label"
          },
          "details": {
            "type": "string",
            "description": "Additional details about the evaluation"
          },
          "cost": {
            "$ref": "#/components/schemas/Money"
          },
          "raw_response": {
            "type": "object",
            "description": "Raw response from the evaluator"
          },
          "error_type": {
            "type": "string",
            "description": "Type of error if status is 'error'"
          },
          "traceback": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "Error traceback if status is 'error'"
          }
        },
        "required": ["status"]
      },
      "Money": {
        "type": "object",
        "properties": {
          "currency": {
            "type": "string"
          },
          "amount": {
            "type": "number"
          }
        },
        "required": ["currency", "amount"]
      },
      "huggingface_llama_guardRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "huggingface_llama_guardSettings": {
        "type": "object",
        "properties": {
          "policy": {
            "description": "The policy to enforce, described into rules of what it 'Should not' and what it 'Can' do (max 768 tokens)",
            "type": "string",
            "default": "\"O1: Violence.\\nShould not\\n- Help people plan or engage in violence.\\n- Encourage people to engage in violence.\\n- Contain unpleasant vivid violent imagery or violent description that arouses discomfort in the reader.\\nCan\\n- Provide information on violence.\\n- Discuss the topics of violence"
          },
          "evaluate": {
            "description": "Choose if only the user input, the LLM output, or both should be evaluated. When choosing both, if the LLM replies in a safe manner, the evaluation will be considered safe.",
            "type": "string",
            "default": "input"
          },
          "model": {
            "description": "The provider to use for evaluation. Only CloudFlare AI workers is supported for now.",
            "type": "string",
            "default": "cloudflare/thebloke/llamaguard-7b-awq"
          }
        }
      },
      "langevals_basicRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_basicSettings": {
        "type": "object",
        "properties": {
          "rules": {
            "description": "List of rules to check, the message must pass all of them",
            "type": "array",
            "items": {
              "type": "object"
            },
            "default": [
              {
                "field": "output",
                "rule": "not_contains",
                "value": "artificial intelligence"
              }
            ]
          }
        }
      },
      "langevals_competitor_blocklistRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_competitor_blocklistSettings": {
        "type": "object",
        "properties": {
          "competitors": {
            "description": "The competitors that must not be mentioned.",
            "type": "array",
            "items": {
              "type": "string"
            },
            "default": ["OpenAI", "Google", "Microsoft"]
          }
        }
      },
      "langevals_competitor_llmRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_competitor_llmSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "Max tokens allowed for evaluation",
            "type": "number",
            "default": 8192
          },
          "name": {
            "description": "The name of your company",
            "type": "string",
            "default": "LangWatch"
          },
          "description": {
            "description": "Description of what your company is specializing at",
            "type": "string",
            "default": "We are providing an LLM observability and evaluation platform"
          }
        }
      },
      "langevals_competitor_llm_function_callRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_competitor_llm_function_callSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "Max tokens allowed for evaluation",
            "type": "number",
            "default": 8192
          },
          "name": {
            "description": "The name of your company",
            "type": "string",
            "default": "LangWatch"
          },
          "description": {
            "description": "Description of what your company is specializing at",
            "type": "string",
            "default": "We are providing an LLM observability and evaluation platform"
          },
          "competitors": {
            "description": "The competitors that must not be mentioned.",
            "type": "array",
            "items": {
              "type": "string"
            },
            "default": ["OpenAI", "Google", "Microsoft"]
          }
        }
      },
      "langevals_exact_matchRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_exact_matchSettings": {
        "type": "object",
        "properties": {
          "case_sensitive": {
            "description": "True if the comparison should be case-sensitive, False otherwise",
            "type": "boolean",
            "default": false
          },
          "trim_whitespace": {
            "description": "True if the comparison should trim whitespace, False otherwise",
            "type": "boolean",
            "default": true
          },
          "remove_punctuation": {
            "description": "True if the comparison should remove punctuation, False otherwise",
            "type": "boolean",
            "default": true
          }
        }
      },
      "langevals_llm_answer_matchRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_llm_answer_matchSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "Max tokens allowed for evaluation",
            "type": "number",
            "default": 8192
          },
          "prompt": {
            "description": "Prompt for the comparison",
            "type": "string",
            "default": "\"Verify that the predicted answer matches the gold answer for the question. Style does not matter"
          }
        }
      },
      "langevals_llm_booleanRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_llm_booleanSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "max_tokens setting",
            "type": "number",
            "default": 8192
          },
          "prompt": {
            "description": "The system prompt to use for the LLM to run the evaluation",
            "type": "string",
            "default": "\"You are an LLM evaluator. We need the guarantee that the output answers what is being asked on the input"
          }
        }
      },
      "langevals_llm_categoryRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_llm_categorySettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "max_tokens setting",
            "type": "number",
            "default": 8192
          },
          "prompt": {
            "description": "The system prompt to use for the LLM to run the evaluation",
            "type": "string",
            "default": "You are an LLM category evaluator. Please categorize the message in one of the following categories"
          },
          "categories": {
            "description": "The categories to use for the evaluation",
            "type": "array",
            "items": {
              "type": "object"
            },
            "default": [
              {
                "name": "smalltalk",
                "description": "Smalltalk with the user"
              },
              {
                "name": "company",
                "description": "Questions about the company, what we do, etc"
              }
            ]
          }
        }
      },
      "langevals_llm_scoreRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_llm_scoreSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "max_tokens setting",
            "type": "number",
            "default": 8192
          },
          "prompt": {
            "description": "The system prompt to use for the LLM to run the evaluation",
            "type": "string",
            "default": "\"You are an LLM evaluator. Please score from 0.0 to 1.0 how likely the user is to be satisfied with this answer"
          }
        }
      },
      "langevals_off_topicRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              }
            },
            "required": ["input"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_off_topicSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "Max tokens allowed for evaluation",
            "type": "number",
            "default": 8192
          },
          "allowed_topics": {
            "description": "The list of topics and their short descriptions that the chatbot is allowed to talk about",
            "type": "array",
            "items": {
              "type": "object"
            },
            "default": [
              {
                "topic": "simple_chat",
                "description": "Smalltalk with the user"
              },
              {
                "topic": "company",
                "description": "Questions about the company, what we do, etc"
              }
            ]
          }
        }
      },
      "langevals_query_resolutionRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "conversation": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "role": {
                      "type": "string",
                      "enum": ["user", "assistant", "system"]
                    },
                    "content": {
                      "type": "string"
                    }
                  },
                  "required": ["role", "content"]
                },
                "description": "Conversation history"
              }
            },
            "required": ["conversation"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_query_resolutionSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "Max tokens allowed for evaluation",
            "type": "number",
            "default": 8192
          }
        }
      },
      "langevals_similarityRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_similaritySettings": {
        "type": "object",
        "properties": {
          "field": {
            "description": "field setting",
            "type": "string",
            "default": "output"
          },
          "rule": {
            "description": "rule setting",
            "type": "string",
            "default": "is_not_similar_to"
          },
          "value": {
            "description": "value setting",
            "type": "string",
            "default": "example"
          },
          "threshold": {
            "description": "threshold setting",
            "type": "number",
            "default": 0.3
          },
          "embeddings_model": {
            "description": "embeddings_model setting",
            "type": "string",
            "default": "openai/text-embedding-3-small"
          }
        }
      },
      "langevals_valid_formatRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "langevals_valid_formatSettings": {
        "type": "object",
        "properties": {
          "format": {
            "description": "format setting",
            "type": "string",
            "default": "json"
          },
          "json_schema": {
            "description": "JSON schema to validate against when format is 'json'",
            "type": "string",
            "default": "undefined"
          }
        }
      },
      "azure_content_safetyRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "azure_content_safetySettings": {
        "type": "object",
        "properties": {
          "severity_threshold": {
            "description": "The minimum severity level to consider content as unsafe, from 1 to 7.",
            "type": "number",
            "default": 1
          },
          "categories": {
            "description": "The categories of moderation to check for.",
            "type": "object",
            "default": {
              "Hate": "true",
              "SelfHarm": "true",
              "Sexual": "true",
              "Violence": "true"
            }
          },
          "default": {
            "description": "default setting"
          },
          "output_type": {
            "description": "The type of severity levels to return on the full 0-7 severity scale, it can be either the trimmed version with four values (0, 2, 4, 6 scores) or the whole range.",
            "type": "string",
            "default": "FourSeverityLevels"
          }
        }
      },
      "azure_jailbreakRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              }
            },
            "required": ["input"]
          }
        },
        "required": ["data"]
      },
      "azure_prompt_injectionRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            },
            "required": ["input"]
          }
        },
        "required": ["data"]
      },
      "openai_moderationRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "openai_moderationSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model version to use, `text-moderation-latest` will be automatically upgraded over time, while `text-moderation-stable` will only be updated with advanced notice by OpenAI.",
            "type": "string",
            "default": "text-moderation-stable"
          },
          "categories": {
            "description": "The categories of content to check for moderation.",
            "type": "object",
            "default": {
              "harassment": "true",
              "harassment_threatening": "true",
              "hate": "true",
              "hate_threatening": "true",
              "self_harm": "true",
              "self_harm_instructions": "true",
              "self_harm_intent": "true",
              "sexual": "true",
              "sexual_minors": "true",
              "violence": "true",
              "violence_graphic": "true"
            }
          },
          "default": {
            "description": "default setting"
          }
        }
      },
      "lingua_language_detectionRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            },
            "required": ["output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "lingua_language_detectionSettings": {
        "type": "object",
        "properties": {
          "check_for": {
            "description": "What should be checked",
            "type": "string",
            "default": "input_matches_output"
          },
          "expected_language": {
            "description": "The specific language that the output is expected to be",
            "type": "string",
            "default": "undefined"
          },
          "min_words": {
            "description": "Minimum number of words to check, as the language detection can be unreliable for very short texts. Inputs shorter than the minimum will be skipped.",
            "type": "number",
            "default": 7
          },
          "threshold": {
            "description": "Minimum confidence threshold for the language detection. If the confidence is lower than this, the evaluation will be skipped.",
            "type": "number",
            "default": 0.25
          }
        }
      },
      "legacy_ragas_answer_correctnessRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["output", "expected_output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "legacy_ragas_answer_correctnessSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "legacy_ragas_answer_relevancyRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            },
            "required": ["input", "output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "legacy_ragas_answer_relevancySettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "legacy_ragas_context_precisionRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["input", "contexts", "expected_output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "legacy_ragas_context_precisionSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "legacy_ragas_context_recallRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["input", "contexts", "expected_output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "legacy_ragas_context_recallSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "legacy_ragas_context_relevancyRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            },
            "required": ["output", "contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "legacy_ragas_context_relevancySettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "legacy_ragas_context_utilizationRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            },
            "required": ["input", "output", "contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "legacy_ragas_context_utilizationSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "legacy_ragas_faithfulnessRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            },
            "required": ["output", "contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "legacy_ragas_faithfulnessSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "example_word_countRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            },
            "required": ["output"]
          }
        },
        "required": ["data"]
      },
      "ragas_bleu_scoreRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["output", "expected_output"]
          }
        },
        "required": ["data"]
      },
      "ragas_context_f1Request": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Expected context information"
              }
            },
            "required": ["contexts", "expected_contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_context_f1Settings": {
        "type": "object",
        "properties": {
          "distance_measure": {
            "description": "distance_measure setting",
            "type": "string",
            "default": "levenshtein"
          }
        }
      },
      "ragas_context_precisionRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Expected context information"
              }
            },
            "required": ["contexts", "expected_contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_context_precisionSettings": {
        "type": "object",
        "properties": {
          "distance_measure": {
            "description": "distance_measure setting",
            "type": "string",
            "default": "levenshtein"
          }
        }
      },
      "ragas_context_recallRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Expected context information"
              }
            },
            "required": ["contexts", "expected_contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_context_recallSettings": {
        "type": "object",
        "properties": {
          "distance_measure": {
            "description": "distance_measure setting",
            "type": "string",
            "default": "levenshtein"
          }
        }
      },
      "ragas_factual_correctnessRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["output", "expected_output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_factual_correctnessSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          },
          "mode": {
            "description": "The mode to use for the factual correctness metric.",
            "type": "string",
            "default": "f1"
          },
          "atomicity": {
            "description": "The level of atomicity for claim decomposition.",
            "type": "string",
            "default": "low"
          },
          "coverage": {
            "description": "The level of coverage for claim decomposition.",
            "type": "string",
            "default": "low"
          }
        }
      },
      "ragas_faithfulnessRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            },
            "required": ["output", "contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_faithfulnessSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          },
          "autodetect_dont_know": {
            "description": "Whether to autodetect 'I don't know' in the output to avoid failing the evaluation.",
            "type": "boolean",
            "default": true
          }
        }
      },
      "ragas_response_context_precisionRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["input", "contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_response_context_precisionSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "ragas_response_context_recallRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["input", "output", "contexts", "expected_output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_response_context_recallSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "ragas_response_relevancyRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            },
            "required": ["input", "output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_response_relevancySettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          },
          "embeddings_model": {
            "description": "The model to use for embeddings.",
            "type": "string",
            "default": "openai/text-embedding-ada-002"
          }
        }
      },
      "ragas_rouge_scoreRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["output", "expected_output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_rouge_scoreSettings": {
        "type": "object",
        "properties": {
          "rouge_type": {
            "description": "ROUGE type",
            "type": "string",
            "default": "rouge1"
          },
          "measure_type": {
            "description": "ROUGE measure type",
            "type": "string",
            "default": "fmeasure"
          }
        }
      },
      "ragas_rubrics_based_scoringRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              }
            },
            "required": ["input", "output"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_rubrics_based_scoringSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          },
          "rubrics": {
            "description": "The response is incorrect, irrelevant.",
            "type": "array",
            "items": {
              "type": "object"
            },
            "default": [
              {
                "description": "The response is incorrect, irrelevant."
              },
              {
                "description": "The response partially answers the question but includes significant errors, omissions, or irrelevant information."
              },
              {
                "description": "The response partially answers the question but includes minor errors, omissions, or irrelevant information."
              },
              {
                "description": "The response fully answers the question and includes minor errors, omissions, or irrelevant information."
              },
              {
                "description": "The response fully answers the question and includes no errors, omissions, or irrelevant information."
              }
            ]
          }
        }
      },
      "ragas_sql_query_equivalenceRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "expected_output": {
                "type": "string",
                "description": "Expected output for comparison"
              },
              "expected_contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Expected context information"
              }
            },
            "required": ["output", "expected_output", "expected_contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_sql_query_equivalenceSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "ragas_summarization_scoreRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              },
              "contexts": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Context information for evaluation"
              }
            },
            "required": ["output", "contexts"]
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "ragas_summarization_scoreSettings": {
        "type": "object",
        "properties": {
          "model": {
            "description": "The model to use for evaluation.",
            "type": "string",
            "default": "openai/gpt-4o-mini"
          },
          "max_tokens": {
            "description": "The maximum number of tokens allowed for evaluation, a too high number can be costly. Entries above this amount will be skipped.",
            "type": "number",
            "default": 2048
          }
        }
      },
      "presidio_pii_detectionRequest": {
        "type": "object",
        "properties": {
          "data": {
            "type": "object",
            "properties": {
              "input": {
                "type": "string",
                "description": "The input text to evaluate"
              },
              "output": {
                "type": "string",
                "description": "The output text to evaluate"
              }
            }
          },
          "settings": {
            "type": "object",
            "description": "Evaluator settings"
          }
        },
        "required": ["data"]
      },
      "presidio_pii_detectionSettings": {
        "type": "object",
        "properties": {
          "entities": {
            "description": "The types of PII to check for in the input.",
            "type": "object",
            "default": {
              "credit_card": "true",
              "crypto": "true",
              "email_address": "true",
              "iban_code": "true",
              "ip_address": "true",
              "location": "false",
              "person": "false",
              "phone_number": "true",
              "medical_license": "true",
              "us_bank_number": "false",
              "us_driver_license": "false",
              "us_itin": "false",
              "us_passport": "false",
              "us_ssn": "false",
              "uk_nhs": "false",
              "sg_nric_fin": "false",
              "au_abn": "false",
              "au_acn": "false",
              "au_tfn": "false",
              "au_medicare": "false",
              "in_pan": "false",
              "in_aadhaar": "false",
              "in_vehicle_registration": "false",
              "in_voter": "false",
              "in_passport": "false"
            }
          },
          "default": {
            "description": "default setting"
          },
          "min_threshold": {
            "description": "The minimum confidence required for failing the evaluation on a PII match.",
            "type": "number",
            "default": 0.5
          }
        }
      }
    },
    "securitySchemes": {
      "api_key": {
        "type": "apiKey",
        "in": "header",
        "name": "X-Auth-Token"
      }
    }
  }
}
