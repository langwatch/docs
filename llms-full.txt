# LangWatch

# FILE: ./introduction.mdx

---
title: "LangWatch: The Complete LLMOps Platform"
description: "Ship AI agents 8x faster with comprehensive observability, evaluation, and prompt optimization. Open-source platform, with over 2.5k stars on GitHub."
sidebarTitle: Introduction
keywords: langwatch, llm, ai, observability, evaluation, prompt optimization, llmops, open-source, github
---

<Frame>
  <img
    className="block"
    src="/images/langwatch-quick-preview.gif"
    alt="LangWatch Quick Preview"
  />
</Frame>

## Quick Start

Ready to add observability to your LLM application? LangWatch integrates with your existing codebase in just a few lines of code, regardless of your tech stack.

<CardGroup cols={2}>
  <Card
    title="Python SDK"
    description="Integrate with Python applications using our comprehensive SDK with auto-instrumentation."
    icon="python"
    href="/integration/python/guide"
    horizontal
    arrow
  />
  <Card
    title="TypeScript SDK"
    description="Add observability to Node.js and TypeScript applications with minimal code changes."
    icon="square-js"
    href="/integration/typescript/guide"
    horizontal
    arrow
  />
  <Card
    title="Go SDK"
    description="Add observability to Go applications with minimal code changes."
    icon="golang"
    href="/integration/go/guide"
    horizontal
    arrow
  />
  <Card
    title="View All Integrations"
    description="View all integrations with our comprehensive SDKs."
    icon="plug"
    href="/integration/overview"
    horizontal
    arrow
  />
</CardGroup>

<Steps>
  <Step title="Sign up for free">
    Create your account at [app.langwatch.ai](https://app.langwatch.ai) to get started with our free tier.
  </Step>
  <Step title="Install the SDK">
    Install the SDK to your project.

    <CodeGroup>
      ```bash Python
      pip install langwatch
      ```
      ```bash TypeScript
      npm install langwatch
      ```
      ```bash Go
      go get github.com/langwatch/langwatch/sdk-go
      ```
    </CodeGroup>
  </Step>
  <Step title="Setup the SDK">
    Configure the SDK to your project. Choose your preferred language:


      ### Python

        **Quick Setup:**
        ```python
        import langwatch
        langwatch.setup()
        ```

        <Card title="View Python Guide" arrow horizontal icon="link" href="/integration/python/guide" />


      ### TypeScript

        **Quick Setup:**
        ```typescript
        import { setupObservability } from "langwatch/observability/node";
        await setupObservability({ serviceName: "my-app" });
        ```

        <Card title="View TypeScript Guide" arrow horizontal icon="link" href="/integration/typescript/guide" />


      ### Go

        **Quick Setup:**
        ```go
        import "github.com/langwatch/langwatch/sdk-go"

        tracer := langwatch.Tracer("my-app")
        ctx, span := tracer.Start(ctx, "MyOperation")
        defer span.End()
        ```

        <Card title="View Go Guide" arrow horizontal icon="link" href="/integration/go/guide" />


  </Step>
  <Step title="Start tracking">
    Your LLM calls are automatically tracked and visible in the LangWatch dashboard.
  </Step>
</Steps>

## What is LangWatch?

LangWatch is the **open-source** LLMOps platform that helps teams collaboratively debug, analyze, and iterate on their LLM applications. All platform features are natively integrated to accelerate the development workflow.

Building AI applications is hard. Developers spend weeks debugging issues, optimizing prompts, and ensuring quality. Without proper observability, you're flying blind - you don't know why your AI behaves the way it does, where it fails, or how to improve it.

LangWatch provides the missing operations platform for AI applications. Observe every LLM call, tool usage, and user interaction is automatically tracked with detailed traces, spans, and metadata. See the full conversation flow, identify bottlenecks, and understand exactly how your AI applications behave in production.

## Core Features

LangWatch provides everything you need to build, monitor, and optimize LLM applications through four core capabilities:

<CardGroup cols={2}>
  <Card
    title="Real-time Observability"
    description="Track every LLM call, tool usage, and user interaction with detailed traces, spans, and metadata."
    icon="chart-network"
    href="/observability/overview"
    arrow
    horizontal
  />
  <Card
    title="Agent Simulation Testing"
    description="Test and optimize agents with collaborative tools and A/B testing."
    icon="masks-theater"
    href="/agent-simulations/introduction"
    arrow
    horizontal
  />
  <Card
    title="LLM Evaluation"
    description="Measure output quality with built-in evaluators, custom metrics, and human feedback integration."
    icon="square-check"
    href="/llm-evaluation/overview"
    arrow
    horizontal
  />
  <Card
    title="Prompt Management"
    description="Version control, test, and optimize prompts with collaborative tools and A/B testing."
    icon="code"
    href="/prompt-management/overview"
    arrow
    horizontal
  />
  <Card
    title="Cost & Performance Tracking"
    description="Monitor token usage, costs, and performance metrics across all models and providers."
    icon="chart-line"
    href="/integration/python/tutorials/tracking-llm-costs"
    arrow
    horizontal
  />
  <Card
    title="Alerts & Triggersfeatures/triggers"
    description="Set up alerts and triggers for your LLM applications."
    icon="bell"
    href="/features/triggers"
    arrow
    horizontal
  />
</CardGroup>

## For Every Role

LangWatch serves different needs across your organization, providing value to every team member working with AI applications.

### For Developers

Debug faster with detailed traces that show exactly what happened in each LLM call. Build datasets from production data, run batch evaluations, and continuously improve your AI applications with comprehensive debugging tools and performance insights.

### For Domain Experts

Easily sift through conversations, see topics being discussed, and annotate messages for improvement in a collaborative manner with the development team. Provide feedback on AI outputs and help guide quality improvements through intuitive interfaces.

### For Business Teams

Track conversation metrics, user analytics, and cost tracking with custom dashboards and reporting. Monitor AI application performance, understand user behavior, and make data-driven decisions about your AI investments.

## Where to Start?

Setting up the full process of online tracing, prompt management, production evaluations, and offline evaluations requires some time. This guide helps you figure out what's most important for your use case.

<CardGroup cols={2}>
  <Card
    title="Just Getting Started?"
    description="Start with basic tracing to understand what's happening in your LLM applications."
    icon="rocket"
    href="/integration/quick-start"
    arrow
    horizontal
  />
  <Card
    title="Already Instrumented?"
    description="Add prompt management and evaluation to optimize your existing setup."
    icon="wrench"
    href="/prompt-management/overview"
    arrow
    horizontal
  />
  <Card
    title="Production Ready?"
    description="Set up comprehensive monitoring, alerts, and cost tracking for production."
    icon="chart-line"
    href="/observability/overview"
    arrow
    horizontal
  />
  <Card
    title="Research & Development?"
    description="Use datasets, experiments, and evaluation tools for systematic testing."
    icon="flask"
    href="/llm-evaluation/overview"
    arrow
    horizontal
  />
</CardGroup>

Ready to get started? [Sign up for free](https://app.langwatch.ai) and begin building better AI applications today.

---

# FILE: ./concepts.mdx

---
title: Concepts
description: LLM tracing and observability conceptual guide
keywords: LangWatch, concepts, tracing, observability, LLM, AI, travel, blog, user, customer, labels, threads, traces, spans
---

Understanding the core concepts of LangWatch is essential for effective observability in your LLM applications. This guide explains key terms and their relationships using practical examples, like building an AI travel assistant or a text generation service.

### Threads: The Whole Conversation

Field: `thread_id`

Think of a **Thread** as the entire journey a user takes in a single session. It's the complete chat with your AI travel buddy, from "Where should I go?" to booking the flight. For the blog post generator, a `thread_id` bundles up the whole session – from brainstorming headlines to polishing the final SEO-optimized draft. It groups *all* the back-and-forth interactions (Traces) for a specific task or conversation.

### Traces: One Task, End-to-End

Field: `trace_id`

<Note>While previously LangWatch allowed you to pass in a custom `trace_id`, we now generate it for you automatically, and provide no way to pass in your own.</Note>

Zooming in from Threads, a **Trace** represents a single, complete task performed by your AI. It's one round trip.

* **Travel Bot:** A user asking, "What are the cheapest flights to Bali in July?" is one Trace. Asking, "Does the hotel allow llamas?" is another Trace.
* **Blog Tool:** Generating headline options? That's a Trace. Drafting the intro paragraph? Another Trace. Optimizing for keywords? You guessed it – a Trace.

Each `trace_id` captures an entire end-to-end generation, no matter how many internal steps (Spans) it takes.

### Spans: The Building Blocks

Field: `span_id`

<Note>While previously LangWatch allowed you to pass in a custom `span_id`, we now generate it for you automatically, and provide no way to pass in your own.</Note>

Now, let's get granular! **Spans** are the individual steps or operations *within* a single Trace. Think of them as the building blocks of getting the job done.

* **Travel Bot Trace:** Might have a Span for the LLM call figuring out destinations, another Span querying an airline API for prices, and a final Span formatting the response.
* **Blog Tool Trace:** Could involve a Span for the initial text generation, a second Span where the LLM critiques its own work (clever!), and a third Span refining the text based on that critique.

Each `span_id` pinpoints a specific action taken by your system or an LLM call.

### User ID: Who's Using the App?

Field: `user_id`

Simple but crucial: The **User ID** identifies the actual end-user interacting with your product. Whether they're planning trips or writing posts, this `user_id` (usually their account ID in your system) links the activity back to a real person, helping you see how different users experience your AI features.

### Customer ID: For Platform Builders

Field: `customer_id`

Are you building a platform *for other companies* to create *their own* LLM apps? That's where the **Customer ID** shines. If you're providing the tools for others (your customers) to build AI assistants for *their* users, the `customer_id` lets you (and them!) track usage and performance per customer account. It's perfect for offering custom analytics dashboards, showing your customers how *their* AI implementations are doing.

### Labels: Your Organizational Superpowers

Field: `labels`

Think of **Labels** as flexible tags you can slap onto Traces to organize, filter, and compare anything you want! They're your secret weapon for slicing and dicing your data.

* **Categorize Actions:** Use labels like `blogpost_title` or `blogpost_keywords`.
* **Track Versions:** Label traces with `version:v1.0.0`, then deploy an improved prompt and label new traces `version:v1.0.1`.
* **Run Experiments:** Tag traces with `experiment:prompt_a` vs. `experiment:prompt_b`.

Labels make it easy to zoom in on specific features or A/B test different approaches right within the LangWatch dashboard.

---

# FILE: ./integration/overview.mdx

---
title: "Getting Started"
description: "LangWatch integrates with all major LLM providers, frameworks, and tools. See our complete list of integrations below."
---

# LangWatch, anywhere you go

<Tip>
**Pro Tip**: Start with our [Quick Start Guide](/integration/quick-start) to get up and running in minutes, then explore specific integrations based on your tech stack.
</Tip>

LangWatch is designed to be the most open and flexible platform for LLM observability that integrates with all the major LLM providers, frameworks, and tools. See a full list of integrations below.

LangWatch is based on OpenTelemetry. Use our Python SDK, TypeScript SDK, or Go SDK to log traces to LangWatch. Alternatively, you can also directly use our OpenTelemetry Endpoint from any language.

### SDK's

LangWatch provides SDKs for all several programming languages.

<CardGroup cols={2}>
<Card title="Python SDK" icon="python" href="/integration/python/guide" arrow>
  Complete Python SDK with automatic instrumentation for popular frameworks
</Card>

<Card title="TypeScript SDK" icon="square-js" href="/integration/typescript/guide" arrow>
  Full-featured TypeScript/JavaScript SDK with type safety
</Card>

<Card title="Go SDK" icon="golang" href="/integration/go/guide" arrow>
  High-performance Go SDK for server-side applications
</Card>

<Card title="OpenTelemetry" icon="telescope" href="/integration/opentelemetry/guide" arrow>
  Native OpenTelemetry integration for any language
</Card>
</CardGroup>

### Frameworks

Use LangWatch to effortlessly integrate with popular AI frameworks

<CardGroup cols={3}>
<Card title="LangChain" icon="/images/logos/langchain.svg" href="/integration/python/integrations/langchain" horizontal arrow />
<Card title="LangGraph" icon="/images/logos/langchain.svg" href="/integration/python/integrations/langgraph" horizontal arrow />
<Card title="Agno" icon="/images/logos/agno.png" href="/integration/python/integrations/agno" horizontal arrow />
<Card title="DSPy" icon="/images/logos/dspy.webp" href="/integration/python/integrations/dspy" horizontal arrow />
<Card title="LlamaIndex" icon="/images/logos/llamaindex.png" href="/integration/python/integrations/llamaindex" horizontal arrow />
<Card title="Haystack" icon="/images/logos/haystack.png" href="/integration/python/integrations/haystack" horizontal arrow />
<Card title="CrewAI" icon="/images/logos/crewai.svg" href="/integration/python/integrations/crew-ai" horizontal arrow />
<Card title="AutoGen" icon="/images/logos/ag.svg" href="/integration/python/integrations/autogen" horizontal arrow />
<Card title="Semantic Kernel" icon="/images/logos/semantic-kernel.png" href="/integration/python/integrations/semantic-kernel" horizontal arrow />
<Card title="Pydantic AI" icon="/images/logos/pydanticai.svg" href="/integration/python/integrations/pydantic-ai" horizontal arrow />
<Card title="Spring AI" icon="/images/logos/spring-boot.svg" href="/integration/java/integrations/spring-ai" horizontal arrow />
<Card title="Strand Agents" icon="/images/logos/strand-agents.svg" href="/integration/python/integrations/strand-agents" horizontal arrow />
<Card title="Lite LLM" icon="/images/logos/litellm.avif" href="/integration/python/integrations/lite-llm" horizontal arrow />
<Card title="OpenAI Agents" icon="/images/logos/openai.svg" href="/integration/python/integrations/open-ai-agents" horizontal arrow />
<Card title="PromptFlow" icon="/images/logos/promptflow.svg" href="/integration/python/integrations/promptflow" horizontal arrow />
<Card title="Google ADK" icon="/images/logos/google.svg" href="/integration/python/integrations/google-ai" horizontal arrow />
</CardGroup>

### Model Providers

Use LangWatch to effortlessly integrate with popular AI model providers

<CardGroup cols={3}>
<Card title="OpenAI" icon="/images/logos/openai.svg" href="/integration/python/integrations/open-ai" horizontal arrow />
<Card title="Anthropic Claude" icon="/images/logos/anthropic.svg" href="/integration/python/integrations/anthropic" horizontal arrow />
<Card title="Gemini" icon="/images/logos/google.svg" href="/integration/go/integrations/google-gemini" horizontal arrow />
<Card title="Azure AI" icon="/images/logos/azure.svg" href="/integration/python/integrations/azure-ai" horizontal arrow />
<Card title="AWS Bedrock" icon="/images/logos/aws.svg" href="/integration/python/integrations/aws-bedrock" horizontal arrow />
<Card title="Groq" icon="/images/logos/groq.svg" href="/integration/go/integrations/groq" horizontal arrow />
<Card title="Ollama" icon="/images/logos/ollama.png" href="/integration/go/integrations/ollama" horizontal arrow />
<Card title="OpenRouter" icon="/images/logos/openrouter.svg" href="/integration/go/integrations/openrouter" horizontal arrow />
<Card title="Vertex AI" icon="/images/logos/google.svg" href="/integration/python/integrations/vertex-ai" horizontal arrow />
</CardGroup>

### No-Code Platforms

No-code agent builders and tools

<CardGroup cols={3}>
<Card title="n8n" icon="/images/logos/n8n.svg" href="/integration/n8n" horizontal arrow />
<Card title="Flowise" icon="/images/logos/flowise.svg" href="/integration/flowise" horizontal arrow />
<Card title="Langflow" icon="/images/logos/langflow.svg" href="/integration/langflow" horizontal arrow />
</CardGroup>

### Other Official LangWatch Integrations

LangWatch provides several official integrations with other tools and services.

<CardGroup cols={2}>
<Card title="REST API" icon="globe" href="/integration/rest-api" horizontal arrow>
  Direct API integration for custom applications
</Card>

<Card title="MCP" icon="brain-circuit" href="/integration/mcp" horizontal arrow>
  Model Context Protocol integration
</Card>
</CardGroup>

## Request a new integration

We use [GitHub Discussions](https://github.com/orgs/langwatch/discussions/new?category=integration-request) to track interest in new integrations. Please upvote/add to the list below if you'd like to see a new integration.

---

# FILE: ./integration/python/reference.mdx

---
title: Python SDK API Reference
sidebarTitle: Reference
description: LangWatch Python SDK API reference
icon: terminal
---

## Setup

### `langwatch.setup()`

Initializes the LangWatch client, enabling data collection and tracing for your LLM application. This is typically the first function you'll call when integrating LangWatch.

<CodeGroup>
```python Basic Setup
import langwatch
import os

client = langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    endpoint_url=os.getenv("LANGWATCH_ENDPOINT_URL") # Optional, defaults to LangWatch Cloud
)
```

```python Advanced Setup
import langwatch
import os
from opentelemetry.sdk.trace import TracerProvider
from langwatch.domain import SpanProcessingExcludeRule
from langwatch.types import BaseAttributes

# Example: Configure with a custom TracerProvider and exclude rules
my_provider = TracerProvider()
exclude_rules = [
    SpanProcessingExcludeRule(
        field_name="span_name",
        match_value="InternalHealthCheck",
        match_operation="exact_match"
    )
]
custom_attributes = BaseAttributes(
    tags=["my-app", "production"],
    props={"service.version": "1.2.3"}
)

client = langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    tracer_provider=my_provider,
    span_exclude_rules=exclude_rules,
    base_attributes=custom_attributes,
    debug=True
)
```
</CodeGroup>

<ParamField path="api_key" type="Optional[str]" default="None">
  Your LangWatch API key. It's recommended to set this via an environment variable (e.g., `LANGWATCH_API_KEY`) and retrieve it using `os.getenv("LANGWATCH_API_KEY")`.
</ParamField>

<ParamField path="endpoint_url" type="Optional[str]" default="LangWatch Cloud URL">
  The URL of the LangWatch backend where traces will be sent. Defaults to the LangWatch Cloud service. For self-hosted instances, you'll need to provide this.
</ParamField>

<ParamField path="base_attributes" type="Optional[BaseAttributes]" default="None">
  A `BaseAttributes` object allowing you to set default tags and properties that will be attached to all traces and spans. See `BaseAttributes` type for more details.
</ParamField>

<ParamField path="tracer_provider" type="Optional[TracerProvider]" default="None">
  An OpenTelemetry `TracerProvider` instance. If you have an existing OpenTelemetry setup, you can pass your `TracerProvider` here. LangWatch will add its exporter to this provider. If not provided, LangWatch will configure its own.
</ParamField>

<ParamField path="instrumentors" type="Optional[Sequence[Instrumentor]]" default="None">
  A sequence of OpenTelemetry instrumentor instances. LangWatch can automatically apply these instrumentors. (Note: Specific instrumentor types might need to be defined or linked here).
</ParamField>

<ParamField path="span_exclude_rules" type="Optional[List[SpanProcessingExcludeRule]]" default="[]">
  A list of `SpanProcessingExcludeRule` objects. These rules allow you to filter out specific spans from being exported to LangWatch, based on span name or attributes. See `SpanProcessingExcludeRule` for details.
</ParamField>

<ParamField path="debug" type="bool" default="False">
  If `True`, enables debug logging for the LangWatch client, providing more verbose output about its operations.
</ParamField>

**Returns**

<ResponseField name="client" type="Client">
  An instance of the LangWatch `Client`.
</ResponseField>

## Tracing

### `@langwatch.trace()` / `langwatch.trace()`

This is the primary way to define the boundaries of a request or a significant operation in your application. It can be used as a decorator around a function or as a context manager.

When used, it creates a new trace and a root span for that trace. Any `@langwatch.span()` or other instrumented calls made within the decorated function or context manager will be nested under this root span.

<CodeGroup>
```python Decorator Usage
import langwatch

langwatch.setup()

@langwatch.trace(name="MyRequestHandler", metadata={"user_id": "123"})
def handle_request(query: str):
    # ... your logic ...
    # langwatch.get_current_span().update(output=response)
    return f"Processed: {query}"

handle_request("Hello LangWatch!")
```

```python Context Manager Usage
import langwatch

langwatch.setup()

def process_data(data: str):
    with langwatch.trace(name="DataProcessingFlow", input={"data_input": data}) as current_trace:
        # ... your logic ...
        # langwatch.get_current_span().update(output={"status": "success"})
        result = f"Processed data: {data}"
        if langwatch.get_current_span(): # Check if span exists
            langwatch.get_current_span().update(output=result)
        return result

process_data("Sample Data")
```
</CodeGroup>


<ParamField path="name" type="Optional[str]" default="Name of the decorated function or a default name">
  The name for the root span of this trace. If used as a decorator and not provided, it defaults to the name of the decorated function. For context manager usage, a name like "LangWatch Trace" might be used if not specified.
</ParamField>

<ParamField path="trace_id" type="Optional[Union[str, UUID]]" default="Auto-generated">
  (Deprecated) A specific ID to assign to this trace. If not provided, a new UUID will be generated. It's generally recommended to let LangWatch auto-generate trace IDs. This will be mapped to `deprecated.trace_id` in metadata.
</ParamField>

<ParamField path="metadata" type="Optional[TraceMetadata]" default="None">
  A dictionary of metadata to attach to the entire trace. This can include things like user IDs, session IDs, or any other contextual information relevant to the whole operation. `TraceMetadata` is typically `Dict[str, Any]`.
</ParamField>

<ParamField path="expected_output" type="Optional[str]" default="None">
  If you have a known expected output for this trace (e.g., for testing or evaluation), you can provide it here.
</ParamField>

<ParamField path="api_key" type="Optional[str]" default="Global or None">
  Overrides the global API key for this specific trace. Useful if you need to direct traces to different projects or accounts dynamically.
</ParamField>

<ParamField path="disable_sending" type="bool" default="False">
  If `True`, this trace (and its spans) will be processed but not sent to the LangWatch backend. This can be useful for local debugging or conditional tracing.
</ParamField>

<ParamField path="max_string_length" type="Optional[int]" default="5000">
  The maximum length for string values captured in inputs, outputs, and metadata. Longer strings will be truncated.
</ParamField>

<ParamField path="skip_root_span" type="bool" default="False">
  If `True`, a root span will not be automatically created for this trace. This is an advanced option, typically used if you intend to manage the root span's lifecycle manually or if the trace is purely a logical grouping without its own primary operation.
</ParamField>

<Expandable title="Root Span Parameters">
The following parameters are used to configure the root span that is automatically created when the trace starts. Refer to `@langwatch.span()` documentation for more details on these, as they behave similarly.

<ParamField path="span_id" type="Optional[str]">
  A specific ID for the root span.
</ParamField>

<ParamField path="capture_input" type="bool" default="True">
  Whether to capture the input of the decorated function/context block for the root span.
</ParamField>

<ParamField path="capture_output" type="bool" default="True">
  Whether to capture the output/result for the root span.
</ParamField>

<ParamField path="type" type="SpanTypes" default="'span'">
  The type of the root span (e.g., 'llm', 'rag', 'agent', 'tool', 'span').
</ParamField>

<ParamField path="input" type="Optional[Union[SpanInputOutput, str, List[ChatMessage]]]">
  Explicitly sets the input for the root span. If not provided and `capture_input` is `True`, it may be inferred from function arguments.
</ParamField>

<ParamField path="output" type="Optional[Union[SpanInputOutput, str, List[ChatMessage]]]">
  Explicitly sets the output for the root span. If not provided and `capture_output` is `True`, it may be inferred from the function's return value.
</ParamField>

<ParamField path="error" type="Optional[Exception]">
  Records an error for the root span.
</ParamField>

<ParamField path="timestamps" type="Optional[SpanTimestamps]">
  Custom start and end timestamps for the root span.
</ParamField>

<ParamField path="contexts" type="Optional[Union[List[RAGChunk], List[str]]]">
  RAG chunks or string contexts relevant to the root span.
</ParamField>

<ParamField path="model" type="Optional[str]">
  The model used in the operation represented by the root span (e.g., an LLM model name).
</ParamField>

<ParamField path="params" type="Optional[SpanParams]">
  Parameters associated with the operation of the root span (e.g., LLM call parameters).
</ParamField>

<ParamField path="metrics" type="Optional[SpanMetrics]">
  Metrics for the root span (e.g., token counts, cost).
</ParamField>

<ParamField path="evaluations" type="Optional[List[Evaluation]]">
  A list of `Evaluation` objects to associate directly with the root span.
</ParamField>
</Expandable>


**Context Manager Return**

When used as a context manager, `langwatch.trace()` returns a `LangWatchTrace` object.

<ResponseField name="current_trace" type="LangWatchTrace">
  The `LangWatchTrace` instance. You can use this object to call methods like `current_trace.add_evaluation()`.
</ResponseField>

#### `LangWatchTrace` Object Methods

When `langwatch.trace()` is used as a context manager, it yields a `LangWatchTrace` object. This object has several methods to interact with the current trace:

<ParamField path="update" type="Callable">
  Updates attributes of the trace or its root span.
  This method can take many of the same parameters as the `langwatch.trace()` decorator/context manager itself, such as `metadata`, `expected_output`, or any of the root span parameters like `name`, `input`, `output`, `metrics`, etc.

  ```python
  with langwatch.trace(name="Initial Trace") as current_trace:
      # ... some operations ...
      current_trace.update(metadata={"step": "one_complete"})
      # ... more operations ...
      root_span_output = "Final output of root operation"
      current_trace.update(output=root_span_output, metrics={"total_items": 10})
  ```
</ParamField>

<ParamField path="add_evaluation" type="Callable">
  Adds an `Evaluation` object directly to the trace (or a specified span within it).

  ```python
  from langwatch.domain import Evaluation, EvaluationTimestamps

  with langwatch.trace(name="MyEvaluatedProcess") as current_trace:
      # ... operation ...
      eval_result = Evaluation(
          name="Accuracy Check",
          status="processed",
          passed=True,
          score=0.95,
          details="All checks passed.",
          timestamps=EvaluationTimestamps(started_at=..., finished_at=...)
      )
      current_trace.add_evaluation(**eval_result) # Pass fields as keyword arguments
  ```
  (Refer to `Evaluation` type in Core Data Types and `langwatch.evaluations` module for more details on parameters.)
</ParamField>

<ParamField path="evaluate" type="Callable">
  Triggers a remote evaluation for this trace using a pre-configured evaluator slug on the LangWatch platform.

  ```python
  with langwatch.trace(name="ProcessWithRemoteEval") as current_trace:
      output_to_evaluate = "Some generated text."
      current_trace.evaluate(
          slug="sentiment-analyzer",
          output=output_to_evaluate,
          as_guardrail=False
      )
  ```
  Parameters include `slug`, `name`, `input`, `output`, `expected_output`, `contexts`, `conversation`, `settings`, `as_guardrail`, `data`.
  Returns: Result of the evaluation call.
</ParamField>

<ParamField path="async_evaluate" type="Callable">
  An asynchronous version of `evaluate`.
</ParamField>

<ParamField path="autotrack_openai_calls" type="Callable">
  Instruments an OpenAI client instance (e.g., `openai.OpenAI()`) to automatically create spans for any OpenAI API calls made using that client within the current trace.

  ```python
  import openai
  # client = openai.OpenAI() # or AzureOpenAI, etc.

  with langwatch.trace(name="OpenAICallsDemo") as current_trace:
      # current_trace.autotrack_openai_calls(client)
      # response = client.chat.completions.create(...)
      # The above call will now be automatically traced as a span.
      pass
  ```
  Takes the OpenAI client instance as an argument.
</ParamField>

<ParamField path="autotrack_dspy" type="Callable">
  Enables automatic tracing for DSPy operations within the current trace. Requires DSPy to be installed and properly configured.

  ```python
  with langwatch.trace(name="DSPyPipeline") as current_trace:
      # current_trace.autotrack_dspy()
      # ... your DSPy calls ...
      pass
  ```
</ParamField>

<ParamField path="get_langchain_callback" type="Callable">
  Returns a LangChain callback handler (`LangChainTracer`) associated with the current trace. This handler can be passed to LangChain runs to automatically trace LangChain operations.

  ```python
  # from langchain_core.llms import FakeLLM (or any LangChain LLM)
  # from langchain.chains import LLMChain

  with langwatch.trace(name="LangChainFlow") as current_trace:
      # handler = current_trace.get_langchain_callback()
      # llm = FakeLLM()
      # chain = LLMChain(llm=llm, prompt=...)
      # chain.run("some input", callbacks=[handler])
      pass
  ```
  Returns: `LangChainTracer` instance.
</ParamField>

<ParamField path="share" type="Callable">
  (Potentially) Generates a shareable link or identifier for this trace. The exact behavior might depend on backend support and configuration.
  Returns: A string, possibly a URL or an ID.
</ParamField>

<ParamField path="unshare" type="Callable">
  (Potentially) Revokes sharing for this trace if it was previously shared.
</ParamField>

### `@langwatch.span()` / `langwatch.span()`

Use this to instrument specific operations or blocks of code within a trace. Spans can be nested to create a hierarchical view of your application's execution flow.

It can be used as a decorator around a function or as a context manager.

<CodeGroup>
```python Decorator Usage
import langwatch

langwatch.setup()

@langwatch.trace(name="MainProcess")
def main_process():
    do_first_step("data for step 1")
    do_second_step("data for step 2")

@langwatch.span(name="StepOne", type="tool")
def do_first_step(data: str):
    # langwatch.get_current_span().set_attributes({"custom_info": "info1"})
    return f"Step 1 processed: {data}"

@langwatch.span() # Name will be 'do_second_step', type will be 'span'
def do_second_step(data: str):
    # langwatch.get_current_span().update(metrics={"items_processed": 10})
    return f"Step 2 processed: {data}"

main_process()
```

```python Context Manager Usage
import langwatch

langwatch.setup()

@langwatch.trace(name="ComplexOperation")
def complex_op():
    # ... some initial work ...
    with langwatch.span(name="DataRetrieval", type="rag") as rag_span:
        # rag_span.update(contexts=[{"document_id": "doc1", "content": "..."}])
        retrieved_data = "some data from RAG"
        rag_span.update(output=retrieved_data)

    # ... more work ...
    with langwatch.span(name="LLMCall", type="llm", input={"prompt": "Summarize this"}, model="gpt-5") as llm_span:
        summary = "summary from LLM"
        llm_span.update(output=summary, metrics={"input_tokens": 50, "output_tokens": 15})
    return summary

complex_op()
```
</CodeGroup>


<ParamField path="name" type="Optional[str]" default="Name of the decorated function or a default name">
  The name for the span. If used as a decorator and not provided, it defaults to the name of the decorated function. For context manager usage, a default name like "LangWatch Span" might be used if not specified.
</ParamField>

<ParamField path="type" type="Optional[SpanType]" default="'span'">
  The semantic type of the span, which helps categorize the operation. Common types include `'llm'`, `'rag'`, `'agent'`, `'tool'`, `'embedding'`, or a generic `'span'`. `SpanType` is typically a string literal from `langwatch.domain.SpanTypes`.
</ParamField>

<ParamField path="span_id" type="Optional[Union[str, UUID]]" default="Auto-generated">
  (Deprecated) A specific ID to assign to this span. It's generally recommended to let LangWatch auto-generate span IDs. This will be mapped to `deprecated.span_id` in the span's attributes.
</ParamField>

<ParamField path="parent" type="Optional[Union[OtelSpan, LangWatchSpan]]" default="Current span in context">
  Explicitly sets the parent for this span. If not provided, the span will be nested under the currently active `LangWatchSpan` or OpenTelemetry span.
</ParamField>

<ParamField path="capture_input" type="bool" default="True">
  If `True` (and used as a decorator), automatically captures the arguments of the decorated function as the span's input.
</ParamField>

<ParamField path="capture_output" type="bool" default="True">
  If `True` (and used as a decorator), automatically captures the return value of the decorated function as the span's output.
</ParamField>

<ParamField path="input" type="SpanInputType" default="None">
  Explicitly sets the input for this span. `SpanInputType` can be a dictionary, a string, or a list of `ChatMessage` objects. This overrides automatic input capture.
</ParamField>

<ParamField path="output" type="SpanInputType" default="None">
  Explicitly sets the output for this span. `SpanInputType` has the same flexibility as for `input`. This overrides automatic output capture.
</ParamField>

<ParamField path="error" type="Optional[Exception]" default="None">
  Records an error for this span. If an exception occurs within a decorated function or context manager, it's often automatically recorded.
</ParamField>

<ParamField path="timestamps" type="Optional[SpanTimestamps]" default="None">
  A `SpanTimestamps` object to explicitly set the `start_time` and `end_time` for the span. Useful for instrumenting operations where the duration is known or externally managed.
</ParamField>

<ParamField path="contexts" type="ContextsType" default="None">
  Relevant contextual information for this span, especially for RAG operations. `ContextsType` can be a list of `RAGChunk` objects or a list of strings.
</ParamField>

<ParamField path="model" type="Optional[str]" default="None">
  The name or identifier of the model used in this operation (e.g., `'gpt-5'`, `'text-embedding-ada-002'`).
</ParamField>

<ParamField path="params" type="Optional[Union[SpanParams, Dict[str, Any]]]" default="None">
  A dictionary or `SpanParams` object containing parameters relevant to the operation (e.g., temperature for an LLM call, k for a vector search).
</ParamField>

<ParamField path="metrics" type="Optional[SpanMetrics]" default="None">
  A `SpanMetrics` object or dictionary to record quantitative measurements for this span, such as token counts (`input_tokens`, `output_tokens`), cost, or other custom metrics.
</ParamField>

<ParamField path="evaluations" type="Optional[List[Any]]" default="None">
  A list of `Evaluation` objects to directly associate with this span.
</ParamField>

<ParamField path="ignore_missing_trace_warning" type="bool" default="False">
  If `True`, suppresses the warning that is normally emitted if a span is created without an active parent trace.
</ParamField>

**OpenTelemetry Parameters:**
These parameters are passed directly to the underlying OpenTelemetry span creation. Refer to OpenTelemetry documentation for more details.

<ParamField path="kind" type="SpanKind" default="SpanKind.INTERNAL">
  The OpenTelemetry `SpanKind` (e.g., `INTERNAL`, `CLIENT`, `SERVER`, `PRODUCER`, `CONSUMER`).
</ParamField>

<ParamField path="span_context" type="Optional[Context]" default="None">
  An OpenTelemetry `Context` object to use for creating the span.
</ParamField>

<ParamField path="attributes" type="Optional[Dict[str, Any]]" default="None">
  Additional custom attributes (key-value pairs) to attach to the span.
</ParamField>

<ParamField path="links" type="Optional[List[Link]]" default="None">
  A list of OpenTelemetry `Link` objects to associate with this span.
</ParamField>

<ParamField path="start_time" type="Optional[int]" default="None">
  An explicit start time for the span (in nanoseconds since epoch).
</ParamField>

<ParamField path="record_exception" type="bool" default="True">
  Whether OpenTelemetry should automatically record exceptions for this span.
</ParamField>

<ParamField path="set_status_on_exception" type="bool" default="True">
  Whether OpenTelemetry should automatically set the span status to error when an exception is recorded.
</ParamField>


**Context Manager Return**

When used as a context manager, `langwatch.span()` returns a `LangWatchSpan` object.

<ResponseField name="current_span" type="LangWatchSpan">
  The `LangWatchSpan` instance. You can use this object to call methods like `current_span.update()` or `current_span.add_evaluation()`.
</ResponseField>

#### `LangWatchSpan` Object Methods

When `langwatch.span()` is used as a context manager, it yields a `LangWatchSpan` object. This object provides methods to interact with the current span:

<ParamField path="update" type="Callable">
  Updates attributes of the span. This is the primary method for adding or changing information on an active span.
  It accepts most of the same parameters as the `@langwatch.span()` decorator itself, such as `name`, `type`, `input`, `output`, `error`, `timestamps`, `contexts`, `model`, `params`, `metrics`, and arbitrary key-value pairs for custom attributes.

  ```python
  with langwatch.span(name="MySpan", type="tool") as current_span:
      # ... some operation ...
      current_span.update(output={"result": "success"}, metrics={"items_processed": 5})
      # Add a custom attribute
      current_span.update(custom_tool_version="1.2.3")
  ```
</ParamField>

<ParamField path="add_evaluation" type="Callable">
  Adds an `Evaluation` object directly to this span.

  ```python
  from langwatch.domain import Evaluation

  with langwatch.span(name="SubOperation") as current_span:
      # ... operation ...
      eval_data = {
          "name": "Correctness Check",
          "status": "processed",
          "passed": False,
          "score": 0.2,
          "label": "Incorrect"
      } # Example, more fields available
      current_span.add_evaluation(**eval_data)
  ```
  (Refer to `Evaluation` type in Core Data Types and `langwatch.evaluations` module for more details on parameters.)
</ParamField>

<ParamField path="evaluate" type="Callable">
  Triggers a remote evaluation for this span using a pre-configured evaluator slug on the LangWatch platform.

  ```python
  with langwatch.span(name="LLM Generation", type="llm") as llm_span:
      llm_output = "Some text generated by an LLM."
      llm_span.update(output=llm_output)
      llm_span.evaluate(slug="toxicity-check", output=llm_output, as_guardrail=True)
  ```
  Parameters include `slug`, `name`, `input`, `output`, `expected_output`, `contexts`, `conversation`, `settings`, `as_guardrail`, `data`.
  Returns: Result of the evaluation call.
</ParamField>

<ParamField path="async_evaluate" type="Callable">
  An asynchronous version of `evaluate` for spans.
</ParamField>

<ParamField path="end" type="Callable">
  Explicitly ends the span. If you provide arguments (like `output`, `metrics`, etc.), it will call `update()` with those arguments before ending.
  Usually not needed when using the span as a context manager, as `__exit__` handles this.

  ```python
  # Manual span management example
  # current_span = langwatch.span(name="ManualSpan").__enter__() # Start span manually
  # try:
  #     # ... operations ...
  #     result = "some result"
  #     current_span.end(output=result) # Update and end
  # except Exception as e:
  #     current_span.end(error=e) # Record error and end
  #     raise
  ```
</ParamField>

**OpenTelemetry Span Methods:**

The `LangWatchSpan` object also directly exposes standard OpenTelemetry `trace.Span` API methods for more advanced use cases or direct OTel interop. These include:
- `record_error(exception)`: Records an exception against the span.
- `add_event(name, attributes)`: Adds a timed event to the span.
- `set_status(status_code, description)`: Sets the OTel status of the span (e.g., `StatusCode.ERROR`).
- `set_attributes(attributes_dict)`: Sets multiple OTel attributes at once.
- `update_name(new_name)`: Changes the name of the span.
- `is_recording()`: Returns `True` if the span is currently recording information.
- `get_span_context()`: Returns the `SpanContext` of the underlying OTel span.

Refer to the OpenTelemetry Python documentation for details on these methods.

```python
from opentelemetry.trace import Status, StatusCode

with langwatch.span(name="MyDetailedSpan") as current_span:
    current_span.add_event("Checkpoint 1", {"detail": "Reached stage A"})
    # ... some work ...
    try:
        # risky_operation()
        pass
    except Exception as e:
        current_span.record_error(e)
        current_span.set_status(Status(StatusCode.ERROR, description="Risky op failed"))
    current_span.set_attributes({"otel_attribute": "value"})
```

### Context Accessors

These functions allow you to access the currently active LangWatch trace or span from anywhere in your code, provided that a trace/span has been started (e.g., via `@langwatch.trace` or `@langwatch.span`).

#### `langwatch.get_current_trace()`

Retrieves the currently active `LangWatchTrace` object.

This is useful if you need to interact with the trace object directly, for example, to add trace-level metadata or evaluations from a helper function called within an active trace.

```python
import langwatch

langwatch.setup()

def some_utility_function(detail: str):
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(metadata={"utility_info": detail})

@langwatch.trace(name="MainOperation")
def main_operation():
    # ... some work ...
    some_utility_function("Processed step A")
    # ... more work ...

main_operation()
```

<ParamField path="suppress_warning" type="bool" default="False">
  If `True`, suppresses the warning that is normally emitted if this function is called when no LangWatch trace is currently in context.
</ParamField>

<ResponseField name="trace" type="LangWatchTrace">
  The current `LangWatchTrace` object. If no trace is active and `suppress_warning` is `False`, a warning is issued and a new (detached) `LangWatchTrace` instance might be returned.
</ResponseField>

#### `langwatch.get_current_span()`

Retrieves the currently active `LangWatchSpan` object.

This allows you to get a reference to the current span to update its attributes, add events, or record information specific to that span from nested function calls.
If no LangWatch-specific span is in context, it will attempt to wrap the current OpenTelemetry span.

```python
import langwatch

langwatch.setup()

def enrich_span_data(key: str, value: any):
    current_span = langwatch.get_current_span()
    if current_span:
        current_span.update(**{key: value})
        # Or using the older set_attributes for OpenTelemetry compatible attributes
        # current_span.set_attributes({key: value})

@langwatch.trace(name="UserFlow")
def user_flow():
    with langwatch.span(name="Step1") as span1:
        # ... step 1 logic ...
        enrich_span_data("step1_result", "success")

    with langwatch.span(name="Step2") as span2:
        # ... step 2 logic ...
        enrich_span_data("step2_details", {"info": "more data"})

user_flow()
```

<ResponseField name="span" type="LangWatchSpan">
  The current `LangWatchSpan` object. This could be a span initiated by `@langwatch.span`, the root span of a `@langwatch.trace`, or a `LangWatchSpan` wrapping an existing OpenTelemetry span if no LangWatch span is directly in context.
</ResponseField>

## Prompt Management

### `langwatch.prompts.create()`

Creates a new prompt in the LangWatch platform.

```python
# Create a basic prompt
prompt = langwatch.prompts.create(
    handle="customer-support-bot",
    scope="PROJECT",
    prompt="You are a helpful customer support assistant.",
    messages=[{"role": "user", "content": "{{question}}"}],
    inputs=[{"identifier": "question", "type": "str"}],
    outputs=[{"identifier": "answer", "type": "str", "json_schema": {"type": "str"}}]
)
```

<ParamField path="handle" type="str" required>
  The unique identifier (handle) for the prompt.
</ParamField>

<ParamField path="scope" type="str" required>
  The scope of the prompt. Must be either `"ORGANIZATION"` or `"PROJECT"`.
</ParamField>

<ParamField path="prompt" type="Optional[str]">
  The prompt text template. Cannot be used simultaneously with `messages`.
</ParamField>

<ParamField path="messages" type="Optional[List[Dict[str, str]]]">
  A list of message objects defining the conversation structure. Cannot be used simultaneously with `prompt`.
</ParamField>

<ParamField path="inputs" type="Optional[List[Dict[str, Any]]]">
  List of input definitions with identifier, type, and optional JSON schema.
</ParamField>

<ParamField path="outputs" type="Optional[List[Dict[str, Any]]]">
  List of output definitions with identifier, type, and optional JSON schema.
</ParamField>

<ParamField path="author_id" type="Optional[str]">
  The ID of the author creating the prompt.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt">
  The newly created prompt object.
</ResponseField>

### `langwatch.prompts.get()`

Retrieves a prompt from the LangWatch platform using its handle.

```python
# Get a prompt by handle
prompt = langwatch.prompts.get("customer-support-bot")
```

<ParamField path="handle" type="str" required>
  The handle (identifier) of the prompt to retrieve.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt">
  The prompt object.
</ResponseField>

<ResponseField name="error" type="Error">
  Throws an error if the specified prompt is not found.
</ResponseField>

### `langwatch.prompts.update()`

Updates an existing prompt, creating a new version automatically.

```python
# Update prompt content
updated_prompt = langwatch.prompts.update(
    "customer-support-bot",
    prompt="You are a helpful and friendly customer support assistant.",
    scope="PROJECT"
)
```

<ParamField path="handle" type="str" required>
  The handle (identifier) of the prompt to update.
</ParamField>

<ParamField path="scope" type="Optional[str]">
  The new scope for the prompt. Must be either `"ORGANIZATION"` or `"PROJECT"`.
</ParamField>

<ParamField path="prompt" type="Optional[str]">
  The updated prompt text template.
</ParamField>

<ParamField path="messages" type="Optional[List[Dict[str, str]]]">
  Updated list of message objects.
</ParamField>

<ParamField path="inputs" type="Optional[List[Dict[str, Any]]]">
  Updated list of input definitions.
</ParamField>

<ParamField path="outputs" type="Optional[List[Dict[str, Any]]]">
  Updated list of output definitions.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt">
  The updated prompt object (new version).
</ResponseField>

<Warning>
Each update operation creates a new version of the prompt. Previous versions are preserved for version control and rollback purposes.
</Warning>

### `langwatch.prompts.delete()`

Deletes a prompt and all its versions from the LangWatch platform.

```python
# Delete a prompt
result = langwatch.prompts.delete("customer-support-bot")
```

<ParamField path="handle" type="str" required>
  The handle (identifier) of the prompt to delete.
</ParamField>

**Returns**

<ResponseField name="result" type="DeletePromptResult">
  Confirmation of the deletion operation.
</ResponseField>

<Warning>
This action is irreversible and will permanently remove the prompt and all its versions.
</Warning>

### Prompt Compilation

#### `prompt.compile()`

Compiles a prompt template with provided variables, using lenient compilation that handles missing variables gracefully.

```python
# Compile a prompt with variables
compiled_prompt = prompt.compile(
    question="What is the capital of France?",
    subject="geography"
)
```

<ParamField path="variables" type="Dict[str, Any]" required>
  Variables to substitute into the prompt template.
</ParamField>

**Returns**

<ResponseField name="compiled_prompt" type="CompiledPrompt">
  The compiled prompt with resolved variables and messages.
</ResponseField>

<Info>
Lenient compilation will not throw errors for missing variables, making it suitable for dynamic content where some variables may be optional.
</Info>

##### `prompt.compile_strict()`

Compiles a prompt template with strict variable validation, throwing an error if any required variables are missing.

```python
# Strict compilation with all required variables
compiled_prompt = prompt.compile_strict(
    subject="quantum computing",
    question="How does it work in 10 words or less?"
)
```

<ParamField path="variables" type="Dict[str, Any]" required>
  Variables to substitute into the prompt template. All template variables must be provided.
</ParamField>

**Returns**

<ResponseField name="compiled_prompt" type="CompiledPrompt">
  The compiled prompt with resolved variables and messages.
</ResponseField>

<ResponseField name="error" type="PromptCompilationError">
  Throws an error if any template variables are missing or invalid.
</ResponseField>

<Warning>
Strict compilation will throw a `PromptCompilationError` if any template variables are missing, ensuring all required data is provided.
</Warning>

## Core Data Types

This section describes common data structures used throughout the LangWatch SDK, particularly as parameters to functions like `langwatch.setup()`, `@langwatch.trace()`, and `@langwatch.span()`, or as part of the data captured.

### `SpanProcessingExcludeRule`

Defines a rule to filter out spans before they are exported to LangWatch. Used in the `span_exclude_rules` parameter of `langwatch.setup()`.

<ResponseField name="field_name" type="Literal['span_name']" required>
  The field of the span to match against. Currently, only `"span_name"` is supported.
</ResponseField>
<ResponseField name="match_value" type="str" required>
  The value to match for the specified `field_name`.
</ResponseField>
<ResponseField name="match_operation" type="Literal['includes', 'exact_match', 'starts_with', 'ends_with']" required>
  The operation to use for matching (e.g., `"exact_match"`, `"starts_with"`).
</ResponseField>

```python Example
from langwatch.domain import SpanProcessingExcludeRule

exclude_health_checks = SpanProcessingExcludeRule(
    field_name="span_name",
    match_value="/health_check",
    match_operation="exact_match"
)

exclude_internal_utils = SpanProcessingExcludeRule(
    field_name="span_name",
    match_value="utils.",
    match_operation="starts_with"
)
```

### `ChatMessage`

Represents a single message in a chat conversation, typically used for `input` or `output` of LLM spans.

<ResponseField name="role" type="ChatRole" required>
  The role of the message sender. `ChatRole` is a Literal: `"system"`, `"user"`, `"assistant"`, `"function"`, `"tool"`, `"guardrail"`, `"evaluation"`, `"unknown"`.
</ResponseField>
<ResponseField name="content" type="Optional[str]">
  The textual content of the message.
</ResponseField>
<ResponseField name="function_call" type="Optional[FunctionCall]">
  For assistant messages that involve a function call (legacy OpenAI format).
  <Expandable title="FunctionCall Properties">
    <ResponseField name="name" type="str">Name of the function to call.</ResponseField>
    <ResponseField name="arguments" type="str">JSON string of arguments for the function.</ResponseField>
  </Expandable>
</ResponseField>
<ResponseField name="tool_calls" type="Optional[List[ToolCall]]">
  For assistant messages that involve tool calls (current OpenAI format).
  <Expandable title="ToolCall Properties">
    <ResponseField name="id" type="str">ID of the tool call.</ResponseField>
    <ResponseField name="type" type="str">Type of the tool (usually `"function"`).</ResponseField>
    <ResponseField name="function" type="FunctionCall">Details of the function to be called (see `FunctionCall` above).</ResponseField>
  </Expandable>
</ResponseField>
<ResponseField name="tool_call_id" type="Optional[str]">
  For messages that are responses from a tool, this is the ID of the tool call that this message is a response to.
</ResponseField>
<ResponseField name="name" type="Optional[str]">
  The name of the function whose result is in the `content` (if role is `"function"`), or the name of the tool/participant (if role is `"tool"`).
</ResponseField>

```python Example
from langwatch.domain import ChatMessage

user_message = ChatMessage(role="user", content="Hello, world!")
assistant_response = ChatMessage(role="assistant", content="Hi there!")
```

### `RAGChunk`

Represents a chunk of retrieved context, typically used with RAG (Retrieval Augmented Generation) operations in the `contexts` field of a span.

<ResponseField name="document_id" type="Optional[str]">
  An identifier for the source document of this chunk.
</ResponseField>
<ResponseField name="chunk_id" type="Optional[str]">
  An identifier for this specific chunk within the document.
</ResponseField>
<ResponseField name="content" type="Union[str, Dict[str, Any], List[Any]]" required>
  The actual content of the RAG chunk. Can be a simple string or a more complex structured object.
</ResponseField>

```python Example
from langwatch.domain import RAGChunk

retrieved_chunk = RAGChunk(
    document_id="doc_123",
    chunk_id="chunk_005",
    content="LangWatch helps monitor LLM applications."
)
```

### `SpanInputOutput`

This is a flexible type used for the `input` and `output` fields of spans. It's a Union that can take several forms to represent different kinds of data. LangWatch will store it as a typed value.
Common forms include:
- `TypedValueText`: For simple string inputs/outputs. `{"type": "text", "value": "your string"}`
- `TypedValueChatMessages`: For conversational inputs/outputs. `{"type": "chat_messages", "value": [ChatMessage, ...]}`
- `TypedValueJson`: For arbitrary JSON-serializable data. `{"type": "json", "value": {"key": "value"}}`
- `TypedValueRaw`: For data that should be stored as a raw string, escaping any special interpretation. `{"type": "raw", "value": "<xml>data</xml>"}`
- `TypedValueList`: For a list of `SpanInputOutput` objects. `{"type": "list", "value": [SpanInputOutput, ...]}`

When providing `input` or `output` to `@langwatch.span()` or `span.update()`, you can often provide the raw Python object (e.g., a string, a list of `ChatMessage` dicts, a dictionary for JSON), and the SDK will attempt to serialize it correctly. For more control, you can construct the `TypedValue` dictionaries yourself.

### `SpanTimestamps`

A dictionary defining custom start and end times for a span, in nanoseconds since epoch.

<ResponseField name="started_at" type="int" required>
  Timestamp when the span started (nanoseconds since epoch).
</ResponseField>
<ResponseField name="first_token_at" type="Optional[int]">
  For LLM operations, timestamp when the first token was received (nanoseconds since epoch).
</ResponseField>
<ResponseField name="finished_at" type="int" required>
  Timestamp when the span finished (nanoseconds since epoch).
</ResponseField>

### `SpanTypes` (Semantic Span Types)

A string literal defining the semantic type of a span. This helps categorize spans in the LangWatch UI and for analytics. Possible values include:
- `"span"` (generic span)
- `"llm"` (Language Model operation)
- `"chain"` (a sequence of operations, e.g., LangChain chain)
- `"tool"` (execution of a tool or function call)
- `"agent"` (an autonomous agent's operation)
- `"guardrail"` (a guardrail or safety check)
- `"evaluation"` (an evaluation step)
- `"rag"` (Retrieval Augmented Generation operation)
- `"workflow"` (a broader workflow or business process)
- `"component"` (a sub-component within a larger system)
- `"module"` (a logical module of code)
- `"server"` (server-side operation)
- `"client"` (client-side operation)
- `"producer"` (message producer)
- `"consumer"` (message consumer)
- `"task"` (a background task or job)
- `"unknown"`

### `SpanMetrics`

A dictionary for quantitative measurements associated with a span.

<ResponseField name="prompt_tokens" type="Optional[int]">
  Number of tokens in the input/prompt to an LLM.
</ResponseField>
<ResponseField name="completion_tokens" type="Optional[int]">
  Number of tokens in the output/completion from an LLM.
</ResponseField>
<ResponseField name="cost" type="Optional[float]">
  Estimated or actual monetary cost of the operation (e.g., LLM API call cost).
</ResponseField>

### `SpanParams`

A dictionary for parameters related to an operation, especially LLM calls.
Examples include:
<ResponseField name="frequency_penalty" type="Optional[float]">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</ResponseField>
<ResponseField name="logit_bias" type="Optional[Dict[str, float]]">
  Modify the likelihood of specified tokens appearing in the completion. Accepts a dictionary mapping token IDs (or tokens, depending on the model) to a bias value from -100 to 100.
</ResponseField>
<ResponseField name="logprobs" type="Optional[bool]">
  Whether to return log probabilities of the output tokens.
</ResponseField>
<ResponseField name="top_logprobs" type="Optional[int]">
  An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with log probability. `logprobs` must be `True` if this is used.
</ResponseField>
<ResponseField name="max_tokens" type="Optional[int]">
  The maximum number of tokens to generate in the completion.
</ResponseField>
<ResponseField name="n" type="Optional[int]">
  How many completions to generate for each prompt.
</ResponseField>
<ResponseField name="presence_penalty" type="Optional[float]">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
</ResponseField>
<ResponseField name="seed" type="Optional[int]">
  If specified, the system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
</ResponseField>
<ResponseField name="stop" type="Optional[Union[str, List[str]]]">
  Up to 4 sequences where the API will stop generating further tokens.
</ResponseField>
<ResponseField name="stream" type="Optional[bool]">
  If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available.
</ResponseField>
<ResponseField name="temperature" type="Optional[float]">
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
</ResponseField>
<ResponseField name="top_p" type="Optional[float]">
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass.
</ResponseField>
<ResponseField name="tools" type="Optional[List[Dict[str, Any]]]">
  A list of tools the model may call. Currently, only functions are supported as a tool.
</ResponseField>
<ResponseField name="tool_choice" type="Optional[str]">
  Controls which (if any) tool is called by the model. `none` means the model will not call any tool. `auto` means the model can pick between generating a message or calling a tool.
</ResponseField>
<ResponseField name="user" type="Optional[str]">
  A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
</ResponseField>

### `Prompt`

Represents a prompt configuration that can be used with OpenAI's API. Handles formatting messages with variables using Liquid templating.

<ResponseField name="id" type="str">
  Unique identifier for the prompt.
</ResponseField>

<ResponseField name="handle" type="str">
  The handle (identifier) of the prompt.
</ResponseField>

<ResponseField name="version" type="str">
  Version string of the prompt.
</ResponseField>

<ResponseField name="version_id" type="str">
  Unique identifier for this specific version.
</ResponseField>

<ResponseField name="prompt" type="Optional[str]">
  The prompt text template.
</ResponseField>

<ResponseField name="messages" type="Optional[List[MessageDict]]">
  List of message objects defining the conversation structure.
</ResponseField>

<ResponseField name="inputs" type="Optional[List[Dict[str, Any]]]">
  List of input definitions with identifier, type, and optional JSON schema.
</ResponseField>

<ResponseField name="outputs" type="Optional[List[Dict[str, Any]]]">
  List of output definitions with identifier, type, and optional JSON schema.
</ResponseField>

<ResponseField name="scope" type="str">
  The scope of the prompt (`"ORGANIZATION"` or `"PROJECT"`).
</ResponseField>

<ResponseField name="author_id" type="Optional[str]">
  The ID of the author who created the prompt.
</ResponseField>

<ResponseField name="raw" type="Any">
  Get the raw prompt data from the API.
</ResponseField>

<ResponseField name="version_number" type="int">
  Returns the version number of the prompt as an integer.
</ResponseField>

#### Methods

<ParamField path="compile" type="Callable">
  Compiles the prompt template with provided variables using lenient compilation.

  ```python
  compiled = prompt.compile(variables={"name": "Alice"})
  # or
  compiled = prompt.compile(name="Alice")
  ```
</ParamField>

<ParamField path="compile_strict" type="Callable">
  Compiles with strict validation, throwing error if required variables are missing.

  ```python
  compiled = prompt.compile_strict(variables={"name": "Alice", "topic": "weather"})
  ```
</ParamField>

### `CompiledPrompt`

Represents a compiled prompt with resolved variables and compiled content.

<ResponseField name="original" type="Prompt">
  Reference to the original prompt object.
</ResponseField>

<ResponseField name="prompt" type="str">
  The compiled prompt text with variables resolved.
</ResponseField>

<ResponseField name="variables" type="TemplateVariables">
  The original compilation variables used.
</ResponseField>

<ResponseField name="messages" type="List[ChatCompletionMessageParam]">
  Compiled messages as a list of ChatCompletionMessageParam objects, ready for OpenAI API.
</ResponseField>

<ResponseField name="id" type="str">
  Inherited from original prompt - unique identifier.
</ResponseField>

<ResponseField name="version" type="str">
  Inherited from original prompt - version string.
</ResponseField>

<ResponseField name="version_id" type="str">
  Inherited from original prompt - version identifier.
</ResponseField>

### `TemplateVariables`

Type alias for template variables supporting common data types.

```python
TemplateVariables = Dict[
    str, Union[str, int, float, bool, Dict[str, Any], List[Any], None]
]
```

### `MessageDict`

Represents a single message in the conversation structure.

<ResponseField name="role" type="str">
  The role of the message sender (e.g., "user", "assistant", "system").
</ResponseField>

<ResponseField name="content" type="str">
  The textual content of the message.
</ResponseField>

### `PromptCompilationError`

Exception raised when prompt template compilation fails.

<ResponseField name="message" type="str">
  Human-readable error message describing the compilation failure.
</ResponseField>

<ResponseField name="template" type="str">
  The template string that failed to compile.
</ResponseField>

<ResponseField name="original_error" type="Optional[Exception]">
  The underlying exception that caused the compilation failure.
</ResponseField>

<ResponseField name="name" type="str">
  Always set to "PromptCompilationError".
</ResponseField>

---

# FILE: ./integration/python/guide.mdx

---
title: Python Integration Guide
sidebarTitle: Guide
description: LangWatch Python SDK integration guide
keywords: LangWatch, Python, SDK, integration, guide, setup, tracing, spans, traces, OpenTelemetry, OpenAI, Celery, HTTP clients, databases, ORMs
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
    <a href="https://github.com/langwatch/langwatch/tree/main/python-sdk" target="_blank">
      <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch Python Repo" />
    </a>
  </div>

  <div>
    <a href="https://pypi.org/project/langwatch/" target="_blank">
      <img src="https://img.shields.io/pypi/v/langwatch?color=007EC6" noZoom alt="LangWatch Python SDK version" />
    </a>
  </div>
</div>

Integrate LangWatch into your Python application to start observing your LLM interactions. This guide covers the setup and basic usage of the LangWatch Python SDK.

## Get your LangWatch API Key

First, you need a LangWatch API key. Sign up at [app.langwatch.ai](https://app.langwatch.ai) and find your API key in your project settings. The SDK will automatically use the `LANGWATCH_API_KEY` environment variable if it is set.

## Start Instrumenting

First, ensure you have the SDK installed:

```bash
pip install langwatch
```

Initialize LangWatch early in your application, typically where you configure services:

```python
import langwatch
import os

langwatch.setup()

# Your application code...
```

<Note>
  If you have an existing OpenTelemetry setup in your application, please see
  the [Already using OpenTelemetry?](#already-using-opentelemetry) section
  below.
</Note>

## Capturing Messages

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces-one-task-end-to-end).
- A [Trace](/concepts#traces-one-task-end-to-end) contains multiple [Spans](/concepts#spans-the-building-blocks), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans-the-building-blocks) capture different parameters.
  - [Spans](/concepts#spans-the-building-blocks) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces-one-task-end-to-end) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads-the-whole-conversation) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.

### Creating a Trace

To capture an end-to-end operation, like processing a user message, you can wrap the main function or entry point with the `@langwatch.trace()` decorator. This automatically creates a root span for the entire operation.

```python
import langwatch
from openai import OpenAI

client = OpenAI()

@langwatch.trace()
async def handle_message():
    # This whole function execution is now a single trace
    langwatch.get_current_trace().autotrack_openai_calls(client) # Automatically capture OpenAI calls

    # ... rest of your message handling logic ...
    pass

```

You can customize the trace name and add initial metadata if needed:

```python
@langwatch.trace(name="My Custom Trace Name", metadata={"foo": "bar"})
async def handle_message():
    # ...
    pass
```

Within a traced function, you can access the current trace context using `langwatch.get_current_trace()`.

### Capturing a Span

To instrument specific parts of your pipeline within a trace (like an llm operation, rag retrieval, or external api call), use the `@langwatch.span()` decorator.

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.span(type="rag", name="RAG Document Retrieval") # Add type and custom name
def rag_retrieval(query: str):
    # ... logic to retrieve documents ...
    search_results = [
        {"id": "doc-1", "content": "..." },
        {"id": "doc-2", "content": "..." }
    ]

    # Add specific context data to the span
    langwatch.get_current_span().update(
        contexts=[
            RAGChunk(document_id=doc["id"], content=doc["content"])
            for doc in search_results
        ],
        retrieval_strategy="vector_search",
    )

    return search_results

@langwatch.trace()
async def handle_message(message: cl.Message):
    # ...
    retrieved_docs = rag_retrieval(message.content) # This call creates a nested span
    # ...
```

<Note>
  The `@langwatch.span()` decorator automatically captures the decorated
  function's arguments as the span's `input` and its return value as the
  `output`. This behavior can be controlled via the `capture_input` and
  `capture_output` arguments (both default to `True`).
</Note>

Spans created within a function **decorated** with `@langwatch.trace()` will automatically be nested under the main trace span. You can add additional `type`, `name`, `metadata`, and `events`, or override the automatic input/output using decorator arguments or the `update()` method on the span object obtained via `langwatch.get_current_span()`.

For detailed guidance on manually creating traces and spans using context managers or direct start/end calls, see the [Manual Instrumentation Tutorial](/integration/python/tutorials/manual-instrumentation).

## Full Setup

```python
import os

import langwatch
from langwatch.attributes import AttributeKey
from langwatch.domain import SpanProcessingExcludeRule

from community.instrumentors import OpenAIInstrumentor # Example instrumentor

from opentelemetry.sdk.trace import TracerProvider

# Example: Providing an existing TracerProvider
# existing_provider = TracerProvider()

# Example: Defining exclude rules
exclude_rules = [
    SpanProcessingExcludeRule(
      field_name=["span_name"],
      match_value="GET /health_check",
      match_operation="exact_match"
    ),
]

langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    endpoint_url="https://your-langwatch-instance.com", # Optional: Defaults to env var or cloud
    base_attributes={
      AttributeKey.ServiceName: "my-awesome-service",
      AttributeKey.ServiceVersion: "1.2.3",
      # Add other custom attributes here
    },
    instrumentors=[OpenAIInstrumentor()], # Optional: List of instrumentors that conform to the `Instrumentor` protocol
    # tracer_provider=existing_provider, # Optional: Provide your own TracerProvider
    debug=True, # Optional: Enable debug logging
    disable_sending=False, # Optional: Disable sending traces
    flush_on_exit=True, # Optional: Flush traces on exit (default: True)
    span_exclude_rules=exclude_rules, # Optional: Rules to exclude spans
    ignore_global_tracer_provider_override_warning=False # Optional: Silence warning if global provider exists
)

# Your application code...
```

### Options

<ParamField path="api_key" type="str | None">
  Your LangWatch API key. If not provided, it uses the `LANGWATCH_API_KEY`
  environment variable.
</ParamField>

<ParamField path="endpoint_url" type="str | None">
  The LangWatch endpoint URL. Defaults to the `LANGWATCH_ENDPOINT` environment
  variable or `https://app.langwatch.ai`.
</ParamField>

<ParamField path="base_attributes" type="dict[str, Any] | None">
  A dictionary of attributes to add to all spans (e.g., service name, version).
  Automatically includes SDK name, version, and language.
</ParamField>

<ParamField path="instrumentors" type="Sequence[Instrumentor] | None">
  A list of automatic instrumentors (e.g., `OpenAIInstrumentor`,
  `LangChainInstrumentor`) to capture data from supported libraries.
</ParamField>

<ParamField path="tracer_provider" type="TracerProvider | None">
  An existing OpenTelemetry `TracerProvider`. If provided, LangWatch will use it
  (adding its exporter) instead of creating a new one. If not provided,
  LangWatch checks the global provider or creates a new one.
</ParamField>

<ParamField path="debug" type="bool" default="False">
  Enable debug logging for LangWatch. Defaults to `False` or checks if the
  `LANGWATCH_DEBUG` environment variable is set to `"true"`.
</ParamField>

<ParamField path="disable_sending" type="bool" default="False">
  If `True`, disables sending traces to the LangWatch server. Useful for testing
  or development.
</ParamField>

<ParamField path="flush_on_exit" type="bool" default="True">
  If `True` (the default), the tracer provider will attempt to flush all pending
  spans when the program exits via `atexit`.
</ParamField>

<ParamField
  path="span_exclude_rules"
  type="List[SpanProcessingExcludeRule] | None"
>
  If provided, the SDK will exclude spans from being exported to LangWatch based
  on the rules defined in the list (e.g., matching span names).
</ParamField>

<ParamField
  path="ignore_global_tracer_provider_override_warning"
  type="bool"
  default="False"
>
  If `True`, suppresses the warning message logged when an existing global
  `TracerProvider` is detected and LangWatch attaches its exporter to it instead
  of overriding it.
</ParamField>

## Integrations

LangWatch offers seamless integrations with a variety of popular Python libraries and frameworks. These integrations provide automatic instrumentation, capturing relevant data from your LLM applications with minimal setup.

Below is a list of currently supported integrations. Click on each to learn more about specific setup instructions and available features:

- [Agno](/integration/python/integrations/agno)
- [AWS Bedrock](/integration/python/integrations/aws-bedrock)
- [Azure AI](/integration/python/integrations/azure-ai)
- [Crew AI](/integration/python/integrations/crew-ai)
- [DSPy](/integration/python/integrations/dspy)
- [Haystack](/integration/python/integrations/haystack)
- [Langchain](/integration/python/integrations/langchain)
- [LangGraph](/integration/python/integrations/langgraph)
- [LiteLLM](/integration/python/integrations/lite-llm)
- [OpenAI](/integration/python/integrations/open-ai)
- [OpenAI Agents](/integration/python/integrations/open-ai-agents)
- [OpenAI Azure](/integration/python/integrations/open-ai-azure)
- [Pydantic AI](/integration/python/integrations/pydantic-ai)
- [Other Frameworks](/integration/python/integrations/other)

If you are using a library that is not listed here, you can still instrument your application manually. See the [Manual Instrumentation Tutorial](/integration/python/tutorials/manual-instrumentation) for more details. Since LangWatch is built on OpenTelemetry, it also supports any library or framework that integrates with OpenTelemetry. We are also continuously working on adding support for more integrations.

## FAQ: Frequently Asked Questions

<AccordionGroup>
  <Accordion title="How do I track LLM costs and token usage?">
    LangWatch automatically captures cost and token data for most LLM providers. If you're missing costs or token counts, our [cost tracking tutorial](/integration/python/tutorials/tracking-llm-costs) covers troubleshooting steps, model cost configuration, and manual token tracking setup.
  </Accordion>

  <Accordion title="How do I capture RAG (Retrieval Augmented Generation) contexts?">
    To monitor your RAG pipelines and track retrieved documents, see our [RAG capturing guide](/integration/python/tutorials/capturing-rag). This enables specialized RAG evaluators and analytics on document usage patterns.
  </Accordion>

  <Accordion title="How can I make input and output of the trace more human readable to better read the conversation?">
    Our [input/output mapping guide](/integration/python/tutorials/capturing-mapping-input-output) shows how to properly structure chat messages, handle different data formats, and ensure your LLM conversations are captured correctly for analysis.
  </Accordion>

  <Accordion title="How do I add custom metadata and user information to traces?">
    Learn how to enrich your traces with user IDs, session data, and custom attributes in our [metadata and attributes tutorial](/integration/python/tutorials/capturing-metadata). This is essential for user analytics and filtering traces by custom criteria.
  </Accordion>

  <Accordion title="How can I capture a whole conversation?">
    To connect multiple traces into a conversation, you can use the `thread_id` metadata. See the [metadata and attributes tutorial](/integration/python/tutorials/capturing-metadata) for more details.
  </Accordion>

  <Accordion title="How do I capture evaluations and guardrails tracing data?">
    Implement automated quality checks and safety measures with our [evaluations and guardrails tutorial](/integration/python/tutorials/capturing-evaluations-guardrails). Learn to create custom evaluators and integrate safety guardrails into your LLM workflows.
  </Accordion>

  <Accordion title="How can I manually instrument my application for more fine-grained control?">
    For custom frameworks or fine-grained control, our [manual instrumentation guide](/integration/python/tutorials/manual-instrumentation) covers creating traces and spans programmatically using context managers and direct API calls.
  </Accordion>

  <Accordion title="How do I integrate with existing OpenTelemetry setups?">
    LangWatch is OpenTelemetry-based, so it can be integrated seamlessly with any OpenTelemetry-compatible application. If you already use OpenTelemetry in your application, our [OpenTelemetry integration tutorial](/integration/python/tutorials/open-telemetry) explains how to configure LangWatch alongside existing telemetry infrastructure, including custom collectors and exporters.
  </Accordion>
</AccordionGroup>

---

# FILE: ./integration/python/integrations/promptflow.mdx

---
title: PromptFlow Instrumentation
sidebarTitle: PromptFlow
description: Learn how to instrument PromptFlow applications with LangWatch.
keywords: promptflow, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

PromptFlow is a development tool designed to streamline the entire development cycle of AI applications, from ideation, prototyping, testing, evaluation to production deployment and monitoring. For more details on PromptFlow, refer to the [official PromptFlow documentation](https://microsoft.github.io/promptflow/).

LangWatch can capture traces generated by PromptFlow by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install PromptFlow and OpenInference instrumentor**:
    ```bash
    pip install promptflow openinference-instrumentation-promptflow
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for PromptFlow using the [OpenInference PromptFlow instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-promptflow). This approach automatically captures traces from your PromptFlow flows and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from promptflow import PFClient
from openinference.instrumentation.promptflow import PromptFlowInstrumentor
import os

# Initialize LangWatch with the PromptFlow instrumentor
langwatch.setup(
    instrumentors=[PromptFlowInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Initialize PromptFlow client
pf = PFClient()

# Use PromptFlow as usual—traces will be sent to LangWatch automatically
def run_promptflow_flow(flow_path: str, inputs: dict):
    # Run a flow
    result = pf.run(
        flow=flow_path,
        inputs=inputs
    )
    return result

# Example usage
if __name__ == "__main__":
    # Example flow path and inputs
    flow_path = "./my_flow"
    inputs = {
        "question": "What is the capital of France?",
        "context": "Geography information"
    }

    result = run_promptflow_flow(flow_path, inputs)
    print(f"Flow result: {result}")
```

**That's it!** All PromptFlow operations will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from promptflow import PFClient
from openinference.instrumentation.promptflow import PromptFlowInstrumentor
import os

langwatch.setup(
    instrumentors=[PromptFlowInstrumentor()]
)

# ... client setup code ...

@langwatch.trace(name="PromptFlow Flow Execution")
def run_promptflow_flow(flow_path: str, inputs: dict):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "flow_path": flow_path,
                "input_count": len(inputs)
            }
        )

    result = pf.run(
        flow=flow_path,
        inputs=inputs
    )
    return result
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `PromptFlowInstrumentor()`: The OpenInference instrumentor automatically patches PromptFlow components to create OpenTelemetry spans for their operations, including:
    - Flow execution
    - Node execution
    - LLM calls
    - Tool executions
    - Data processing
    - Input/output handling

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all flow executions, node operations, model calls, and data processing will be automatically traced and sent to LangWatch, providing comprehensive visibility into your PromptFlow-powered applications.

## Environment Variables

Make sure to set the following environment variables:

```bash
# For OpenAI
export OPENAI_API_KEY=your-openai-api-key

# For Anthropic
export ANTHROPIC_API_KEY=your-anthropic-api-key

# LangWatch API key
export LANGWATCH_API_KEY=your-langwatch-api-key
```

## Supported Models

PromptFlow supports various LLM providers including:

- OpenAI (GPT-5, GPT-4o, etc.)
- Anthropic (Claude models)
- Local models (via Ollama, etc.)
- Other providers supported by PromptFlow

All model interactions and flow executions will be automatically traced and captured by LangWatch.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine PromptFlow instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all PromptFlow activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your PromptFlow code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from promptflow import PFClient
from openinference.instrumentation.promptflow import PromptFlowInstrumentor

langwatch.setup(
    instrumentors=[PromptFlowInstrumentor()]
)

@langwatch.trace(name="Custom PromptFlow Application")
def my_custom_promptflow_app(flow_path: str, inputs: dict):
    # Your PromptFlow code here
    pf = PFClient()

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "flow_path": flow_path,
                "input_count": len(inputs)
            }
        )

    # Run your flow
    result = pf.run(
        flow=flow_path,
        inputs=inputs
    )

    return result
```

This approach allows you to combine the automatic tracing capabilities of PromptFlow with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/langchain.mdx

---
title: LangChain Instrumentation
sidebarTitle: Python
description: Learn how to instrument Langchain applications with the LangWatch Python SDK.
icon: python
keywords: langchain, instrumentation, callback, opentelemetry, langwatch, python, tracing, openinference, openllmetry
---

Langchain is a powerful framework for building LLM applications. LangWatch integrates with Langchain to provide detailed observability into your chains, agents, LLM calls, and tool usage.

This guide covers the primary approaches to instrumenting Langchain with LangWatch:

1.  **Using LangWatch's Langchain Callback Handler (Recommended)**: The most direct method, using a specific callback provided by LangWatch to capture rich Langchain-specific trace data.
2.  **Using Community OpenTelemetry Instrumentors**: Leveraging dedicated Langchain instrumentors like those from OpenInference or OpenLLMetry.
3.  **Utilizing Langchain's Native OpenTelemetry Export (Advanced)**: Configuring Langchain to send its own OpenTelemetry traces to an endpoint where LangWatch can collect them.

## 1. Using LangWatch's Langchain Callback Handler (Recommended)

This is the preferred and most comprehensive method for instrumenting Langchain with LangWatch. The LangWatch SDK provides a `LangchainCallbackHandler` that deeply integrates with Langchain's event system.

```python
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
import os
import asyncio

# Initialize LangWatch
langwatch.setup()

# Standard Langchain setup
model = ChatOpenAI(streaming=True)
prompt = ChatPromptTemplate.from_messages(
    [("system", "You are a concise assistant."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - QA with Callback")
async def handle_message_with_callback(user_question: str):
    current_trace = langwatch.get_current_trace()
    current_trace.update(metadata={"user_id": "callback-user"})

    langwatch_callback = current_trace.get_langchain_callback()

    final_response = ""
    async for chunk in runnable.astream(
        {"question": user_question},
        config=RunnableConfig(callbacks=[langwatch_callback])
    ):
        final_response += chunk
    return final_response

async def main_callback():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping Langchain callback example.")
        return
    response = await handle_message_with_callback("What is Langchain? Explain briefly.")
    print(f"AI (Callback): {response}")

if __name__ == "__main__":
    asyncio.run(main_callback())
```

**How it Works:**
-   `@langwatch.trace()`: Creates a parent LangWatch trace.
-   `current_trace.get_langchain_callback()`: Retrieves a LangWatch-specific callback handler linked to the current trace.
-   `RunnableConfig(callbacks=[langwatch_callback])`: Injects the handler into Langchain's execution. Langchain emits events (on_llm_start, on_chain_end, etc.), which the handler converts into detailed LangWatch spans, correctly parented under the main trace.

**Key points:**
-   Provides the most detailed Langchain-specific structural information (chains, agents, tools, LLMs as distinct steps).
-   Works for all Langchain execution methods (`astream`, `stream`, `invoke`, `ainvoke`).

## 2. Using Community OpenTelemetry Instrumentors

Dedicated Langchain instrumentors from libraries like OpenInference and OpenLLMetry can also be used to capture Langchain operations as OpenTelemetry traces, which LangWatch can then ingest.

### Instrumenting Langchain with Dedicated Instrumentors

#### i. Via `langwatch.setup()`

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangchainInstrumentor
import os
import asyncio

langwatch.setup(
    instrumentors=[LangchainInstrumentor()] # Add OpenInference LangchainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference via Setup")
async def handle_message_oinference_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (setup) example.")
        return
    response = await handle_message_oinference_setup("Explain Langchain instrumentation with OpenInference.")
    print(f"AI (OInference Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_setup())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangchainInstrumentor # Corrected import
import os
import asyncio

langwatch.setup(
    instrumentors=[LangchainInstrumentor()] # Add OpenLLMetry LangchainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry via Setup")
async def handle_message_openllmetry_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (setup) example.")
        return
    response = await handle_message_openllmetry_setup("Explain Langchain instrumentation with OpenLLMetry.")
    print(f"AI (OpenLLMetry Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_setup())
```
</CodeGroup>

#### ii. Direct Instrumentation

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangchainInstrumentor
import os
import asyncio

langwatch.setup()
LangchainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference Direct")
async def handle_message_oinference_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (direct) example.")
        return
    response = await handle_message_oinference_direct("How does direct Langchain instrumentation work?")
    print(f"AI (OInference Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_direct())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangchainInstrumentor # Corrected import
import os
import asyncio

langwatch.setup()
LangchainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry Direct")
async def handle_message_openllmetry_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (direct) example.")
        return
    response = await handle_message_openllmetry_direct("How does direct Langchain instrumentation work with OpenLLMetry?")
    print(f"AI (OpenLLMetry Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_direct())
```
</CodeGroup>

**Key points for dedicated Langchain instrumentors:**
- Directly instrument Langchain operations, providing traces from Langchain's perspective.
- Requires installing the respective instrumentation package (e.g., `openinference-instrumentation-langchain` or `opentelemetry-instrumentation-langchain` for OpenLLMetry).

## 3. Utilizing Langchain's Native OpenTelemetry Export (Advanced)

Langchain itself can be configured to export OpenTelemetry traces. If you set this up and configure Langchain to send traces to an OpenTelemetry collector endpoint that LangWatch is also configured to receive from (or if LangWatch *is* your OTLP endpoint), then LangWatch can ingest these natively generated Langchain traces.

**Setup (Conceptual):**
1.  Configure Langchain for OpenTelemetry export. This usually involves setting environment variables:
    ```bash
    export LANGCHAIN_TRACING_V2=true
    export LANGCHAIN_ENDPOINT= # Your OTLP gRPC endpoint (e.g., http://localhost:4317)
    # Potentially other OTEL_EXPORTER_OTLP_* variables for more granular control
    ```
2.  Initialize LangWatch: `langwatch.setup()`.

```python
# This example assumes Langchain is configured via environment variables
# to send OTel traces to an endpoint LangWatch can access.
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
import os
import asyncio

langwatch.setup() # LangWatch is ready to receive OTel traces

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - Native OTel Export") # Optional: group Langchain traces
async def handle_message_native_otel(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_native_otel():
    if not os.getenv("OPENAI_API_KEY") or not os.getenv("LANGCHAIN_TRACING_V2") == "true":
        print("Required env vars (OPENAI_API_KEY, LANGCHAIN_TRACING_V2='true') not set. Skipping native OTel.")
        return
    response = await handle_message_native_otel("Tell me about Langchain OTel export itself.")
    print(f"AI (Native OTel): {response}")

if __name__ == "__main__":
    asyncio.run(main_native_otel())
```

**Key points for Langchain's native OTel export:**
-   LangWatch acts as a backend/collector for OpenTelemetry traces generated directly by Langchain.
-   Requires careful configuration of Langchain's environment variables.
-   The level of detail depends on Langchain's native OpenTelemetry instrumentation quality.

<Note>
### Which Approach to Choose?

-   **LangWatch's Langchain Callback Handler (Recommended)**: Provides the richest, most Langchain-aware traces directly integrated with LangWatch's tracing context. Ideal for most users.
-   **Dedicated Langchain Instrumentors (OpenInference, OpenLLMetry)**: Good alternatives if you prefer an explicit instrumentor pattern for Langchain itself or are standardizing on these specific OpenTelemetry ecosystems.
-   **Langchain's Native OTel Export (Advanced)**: Suitable if you have an existing OpenTelemetry collection infrastructure and want Langchain to be another OTel-compliant source.

For the best Langchain-specific observability within LangWatch, the **Langchain Callback Handler** is the generally recommended approach, with dedicated **Langchain Instrumentors** as strong alternatives for instrumentor-based setups.
</Note>

---

# FILE: ./integration/python/integrations/agno.mdx

---
title: Agno Instrumentation
sidebarTitle: Agno
description: Learn how to instrument Agno agents and send traces to LangWatch using the Python SDK.
keywords: agno, openinference, openlit, opentelemetry, LangWatch, Python, tracing, observability
---

LangWatch supports seamless observability for [Agno](https://github.com/agno-agi/agno) agents. You can instrument your Agno-based applications and send traces directly to LangWatch for monitoring and analysis.

This guide shows how to set up tracing for Agno agents using the LangWatch Python SDK. Unlike the default OpenTelemetry/OTLP setup, you only need to call `langwatch.setup()`—no manual exporter or environment variable configuration is required.

## Prerequisites

- Install the required packages:
  ```bash
  pip install agno openai langwatch openinference-instrumentation-agno
  ```
- Get your LangWatch API key from your [project settings](https://app.langwatch.ai/) and set it as an environment variable:
  ```bash
  export LANGWATCH_API_KEY=your-langwatch-api-key
  ```

## Instrumenting Agno with LangWatch

You can use the [OpenInference Agno instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-agno) to automatically capture traces from your Agno agents. Just pass the instrumentor to `langwatch.setup()`.

```python
import langwatch
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor

# Initialize LangWatch and instrument Agno
langwatch.setup(
    instrumentors=[AgnoInstrumentor()]
)

# Create and configure your Agno agent
agent = Agent(
    name="Stock Price Agent",
    model=OpenAIChat(id="gpt-5"),
    tools=[YFinanceTools()],
    instructions="You are a stock price agent. Answer questions in the style of a stock analyst.",
    debug_mode=True,
)

# Use the agent as usual—traces will be sent to LangWatch
agent.print_response("What is the current price of Tesla?")
```

**That's it!** All Agno agent activity will now be traced and sent to your LangWatch dashboard.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Agno instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.

For more details, see the [LangWatch Python SDK reference](/integration/python/reference) and the [Agno documentation](https://docs.agno.com/integrations/observability/introduction#opentelemetry).

---

# FILE: ./integration/python/integrations/haystack.mdx

---
title: Haystack Instrumentation
sidebarTitle: Haystack
description: Learn how to instrument Haystack pipelines with LangWatch using community OpenTelemetry instrumentors.
keywords: haystack, deepset, instrumentation, openinference, openllmetry, LangWatch, Python
---

LangWatch does not have a built-in auto-tracking integration for Haystack from deepset.ai. However, you can leverage community-provided OpenTelemetry instrumentors to integrate your Haystack pipelines with LangWatch.

## Integrating Community Instrumentors with LangWatch

Community-provided OpenTelemetry instrumentors for Haystack allow you to automatically capture detailed trace data from your Haystack pipeline components (Nodes, Pipelines, etc.). LangWatch can seamlessly integrate with these instrumentors.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the Haystack instrumentor to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

<CodeGroup>

```python openinference_setup.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from openinference.instrumentation.haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup(
    instrumentors=[HaystackInstrumentor()]
)

# Initialize Haystack components
document_store = InMemoryDocumentStore(use_bm25=True)
# You can add documents to the store here if needed

prompt_template = PromptTemplate(
    prompt="Answer the question based on the context: {query}",
    output_parser=None # Or specify an output parser
)
prompt_node = PromptNode(
    model_name_or_path="gpt-5", # Replace with your desired model
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)

pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="PromptNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenInference (Setup)")
def run_haystack_pipeline_oi_setup(query: str):
    # The OpenInference instrumentor, when configured via langwatch.setup(),
    # should automatically capture Haystack operations.
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "What is the capital of France?"
        print(f"Running Haystack pipeline with OpenInference (setup) for query: {user_query}")
        output = run_haystack_pipeline_oi_setup(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

```python openllmetry_setup.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from opentelemetry_instrumentation_haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup(
    instrumentors=[HaystackInstrumentor()]
)

# Initialize Haystack components
document_store = InMemoryDocumentStore(use_bm25=True)
prompt_template = PromptTemplate(prompt="Answer the question: {query}")
prompt_node = PromptNode(
    model_name_or_path="gpt-5",
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)
pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="MyPromptNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenLLMetry (Setup)")
def run_haystack_pipeline_ollm_setup(query: str):
    # The OpenLLMetry instrumentor should automatically capture Haystack operations.
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "What is deep learning?"
        print(f"Running Haystack pipeline with OpenLLMetry (setup) for query: {user_query}")
        output = run_haystack_pipeline_ollm_setup(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

</CodeGroup>

<Note>
  Ensure you have the respective community instrumentation library installed:
  - For OpenInference: `pip install openinference-instrumentation-haystack`
  - For OpenLLMetry: `pip install opentelemetry-instrumentation-haystack`
  You'll also need Haystack: `pip install farm-haystack[openai]` (if using OpenAI models).
  Consult the specific library's documentation for the exact package name and instrumentor class if the above assumptions are incorrect.
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

<CodeGroup>

```python openinference_direct.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from openinference.instrumentation.haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup()

# Instrument Haystack directly using OpenInference
HaystackInstrumentor().instrument()

document_store = InMemoryDocumentStore(use_bm25=True)
prompt_template = PromptTemplate(prompt="Summarize this for a 5-year old: {query}")
prompt_node = PromptNode(
    model_name_or_path="gpt-5",
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)
pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="SummarizerNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenInference (Direct)")
def run_haystack_pipeline_oi_direct(query: str):
    # Spans from Haystack operations should be captured by the directly configured instrumentor.
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "The quick brown fox jumps over the lazy dog."
        print(f"Running Haystack pipeline with OpenInference (direct) for query: {user_query}")
        output = run_haystack_pipeline_oi_direct(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

```python openllmetry_direct.py
import langwatch
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PromptNode, PromptTemplate
from haystack.pipelines import Pipeline
from opentelemetry_instrumentation_haystack import HaystackInstrumentor # Assuming this is the correct import

langwatch.setup()

# Instrument Haystack directly using OpenLLMetry
HaystackInstrumentor().instrument()

document_store = InMemoryDocumentStore(use_bm25=True)
prompt_template = PromptTemplate(prompt="Translate to French: {query}")
prompt_node = PromptNode(
    model_name_or_path="gpt-5",
    api_key=os.environ.get("OPENAI_API_KEY"),
    default_prompt_template=prompt_template
)
pipeline = Pipeline()
pipeline.add_node(component=prompt_node, name="TranslatorNode", inputs=["Query"])

@langwatch.trace(name="Haystack Pipeline with OpenLLMetry (Direct)")
def run_haystack_pipeline_ollm_direct(query: str):
    result = pipeline.run(query=query)
    return result

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        user_query = "Hello, how are you?"
        print(f"Running Haystack pipeline with OpenLLMetry (direct) for query: {user_query}")
        output = run_haystack_pipeline_ollm_direct(user_query)
        print("\n\nHaystack Pipeline Output:")
        print(output)
```

</CodeGroup>

### Key points for community instrumentors:
-   These instrumentors typically patch Haystack at a global level or integrate deeply with its execution flow (e.g., via `BaseComponent` or `Pipeline` hooks), meaning most Haystack operations should be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup and lifecycle of the instrumentor.
-   If instrumenting directly (e.g., `HaystackInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.
-   Always refer to the specific documentation of the community instrumentor (OpenInference or OpenLLMetry) for the most accurate and up-to-date installation and usage instructions, including the correct class names for instrumentors and any specific setup requirements for different Haystack versions or components.
-   The examples use a `PromptNode` with an OpenAI model for simplicity. Ensure you have the necessary API keys (e.g., `OPENAI_API_KEY`) set in your environment if you run these examples.

---

# FILE: ./integration/python/integrations/open-ai-azure.mdx

---
title: Azure OpenAI Instrumentation
sidebarTitle: Azure OpenAI
description: Learn how to instrument Azure OpenAI API calls with the LangWatch Python SDK
keywords: azure openai, openai, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Azure OpenAI, allowing you to capture detailed information about your LLM calls automatically. There are two primary approaches to instrumenting your Azure OpenAI interactions:

1.  **Using `autotrack_openai_calls()`**: This method, part of the LangWatch SDK, dynamically patches your `AzureOpenAI` client instance to capture calls made through it within a specific trace.
2.  **Using Community OpenTelemetry Instrumentors**: Leverage existing OpenTelemetry instrumentation libraries like those from OpenInference or OpenLLMetry. These can be integrated with LangWatch by either passing them to the `langwatch.setup()` function or by using their native `instrument()` methods if you're managing your OpenTelemetry setup more directly.

This guide will walk you through both methods.

## Using `autotrack_openai_calls()`

The `autotrack_openai_calls()` function provides a straightforward way to capture all Azure OpenAI calls made with a specific client instance for the duration of the current trace.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`.

```python
import langwatch
from openai import AzureOpenAI
import os

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup()

# Initialize your AzureOpenAI client
# Ensure your Azure environment variables are set:
# AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME
client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2023-05-15"  # Or your preferred API version
)


@langwatch.trace(name="Azure OpenAI Chat Completion")
async def get_azure_openai_chat_response(user_prompt: str):
    # Get the current trace and enable autotracking for the 'client' instance
    langwatch.get_current_trace().autotrack_openai_calls(client)

    # All calls made with 'client' will now be automatically captured as spans
    response = client.chat.completions.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"), # Use your Azure deployment name
        messages=[{"role": "user", "content": user_prompt}],
    )
    completion = response.choices[0].message.content
    return completion

async def main():
    user_query = "Tell me a fact about the Azure cloud."
    response = await get_azure_openai_chat_response(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

Key points for `autotrack_openai_calls()` with Azure OpenAI:
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments a *specific instance* of the `AzureOpenAI` client. If you have multiple clients, you'll need to call it for each one you want to track.
-   Ensure your `AzureOpenAI` client is correctly configured with `azure_endpoint`, `api_key`, `api_version`, and you use the deployment name for the `model` parameter.

## Using Community OpenTelemetry Instrumentors

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from the `openai` library, which `AzureOpenAI` is part of.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `OpenAIInstrumentor` from OpenInference) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from openai import AzureOpenAI
import os

from openinference.instrumentation.openai import OpenAIInstrumentor

# Initialize LangWatch with the OpenAIInstrumentor
langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

# Initialize your AzureOpenAI client
client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2023-05-15"
)

@langwatch.trace(name="Azure OpenAI Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles OpenAI calls globally.
    response = client.chat.completions.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "your-deployment-name"), # Use your Azure deployment name
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    user_query = "Explain Azure Machine Learning in simple terms."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```
<Note>
  Ensure you have the respective community instrumentation library installed (e.g., `pip install openllmetry-instrumentation-openai` or `pip install openinference-instrumentation-openai`). The instrumentor works with `AzureOpenAI` as it's part of the same `openai` Python package.
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from openai import AzureOpenAI
import os

from openinference.instrumentation.openai import OpenAIInstrumentor

langwatch.setup() # LangWatch sets up or uses the global OpenTelemetry provider

# Initialize your AzureOpenAI client
client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2023-05-15"
)

# Instrument OpenAI directly using the community library
# This will patch the openai library, affecting AzureOpenAI instances too.
OpenAIInstrumentor().instrument()

@langwatch.trace(name="Azure OpenAI Call with Direct Community Instrumentation")
def get_creative_idea(topic: str):
    response = client.chat.completions.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "your-deployment-name"), # Use your Azure deployment name
        messages=[
            {"role": "system", "content": "You are an idea generation bot."},
            {"role": "user", "content": f"Generate a creative idea about {topic}."}
        ]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    subject = "sustainable energy"
    idea = get_creative_idea(subject)
    print(f"Topic: {subject}")
    print(f"AI's Idea: {idea}")
```

### Key points for community instrumentors with Azure OpenAI:
-   These instrumentors often patch the `openai` library at a global level, meaning all calls from any `OpenAI` or `AzureOpenAI` client instance will be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the instrumentor's setup.
-   If instrumenting directly (e.g., `OpenAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This typically happens automatically if LangWatch initializes the global provider or if you configure them to use the same explicit provider.

<Note>
### Which Approach to Choose?

-   **`autotrack_openai_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which `AzureOpenAI` client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
-   **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture Azure OpenAI calls globally across your application, or need to instrument other libraries alongside Azure OpenAI with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send Azure OpenAI call data to LangWatch for monitoring and analysis.
</Note>

---

# FILE: ./integration/python/integrations/google-ai.mdx

---
title: Google Agent Development Kit (ADK) Instrumentation
sidebarTitle: Google ADK
description: Learn how to instrument Google Agent Development Kit (ADK) applications with LangWatch.
keywords: google adk, agent development kit, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

The Google Agent Development Kit (ADK) streamlines building, orchestrating, and tracing generative-AI agents out of the box, letting you move from prototype to production far faster than wiring everything yourself. For more details on ADK, refer to the [official Google ADK documentation](https://google.github.io/adk-docs/).

LangWatch can capture traces generated by Google ADK by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Google ADK and OpenInference instrumentor**:
    ```bash
    pip install google-adk openinference-instrumentation-google-adk
    ```

3.  **Set up Google Cloud authentication**:
    You'll need to authenticate with Google Cloud. You can either:
    - Set the `GOOGLE_API_KEY` environment variable for Gemini API access
    - Use Application Default Credentials (ADC) if running on Google Cloud
    - Use service account keys for production deployments

## Instrumentation with OpenInference

LangWatch supports seamless observability for Google ADK agents using the [OpenInference Google ADK instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-google-adk). This approach automatically captures traces from your ADK agents and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from google.adk import Agent, Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from openinference.instrumentation.google_adk import GoogleADKInstrumentor
import os

# Initialize LangWatch with the Google ADK instrumentor
langwatch.setup(
    instrumentors=[GoogleADKInstrumentor()]
)

# Set up environment variables
os.environ["GOOGLE_API_KEY"] = "your-gemini-api-key"

# Define your agent tools
def say_hello():
    return {"greeting": "Hello LangWatch 👋"}

def get_weather(location: str):
    return {"location": location, "temperature": "22°C", "condition": "sunny"}

# Create your agent
agent = Agent(
    name="hello_agent",
    model="gemini-2.0-flash",
    instruction="Always greet using the say_hello tool and provide weather information when asked.",
    tools=[say_hello, get_weather],
)

# Set up session service and runner
session_service = InMemorySessionService()
session_service.create_session(
    app_name="hello_app", user_id="demo-user", session_id="demo-session"
)

runner = Runner(agent=agent, app_name="hello_app", session_service=session_service)

# Use the agent as usual—traces will be sent to LangWatch automatically
def run_agent_interaction(user_message: str):
    user_msg = types.Content(role="user", parts=[types.Part(text=user_message)])

    for event in runner.run(user_id="demo-user", session_id="demo-session", new_message=user_msg):
        if event.is_final_response():
            return event.content.parts[0].text

    return "No response generated"

# Example usage
if __name__ == "__main__":
    user_prompt = "hi"
    response = run_agent_interaction(user_prompt)
    print(f"User: {user_prompt}")
    print(f"Agent: {response}")
```

**That's it!** All Google ADK agent activity will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from google.adk import Agent, Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from openinference.instrumentation.google_adk import GoogleADKInstrumentor
import os

langwatch.setup(
    instrumentors=[GoogleADKInstrumentor()]
)

# ... agent setup code ...

@langwatch.trace(name="Google ADK Agent Run")
def run_agent_interaction(user_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "hello_agent",
                "model": "gemini-2.0-flash"
            }
        )

    user_msg = types.Content(role="user", parts=[types.Part(text=user_message)])

    for event in runner.run(user_id="demo-user", session_id="demo-session", new_message=user_msg):
        if event.is_final_response():
            return event.content.parts[0].text

    return "No response generated"
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `GoogleADKInstrumentor()`: The OpenInference instrumentor automatically patches Google ADK components to create OpenTelemetry spans for their operations, including:
    - Agent initialization
    - Tool calls
    - Model completions
    - Session management

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all agent interactions, tool calls, and model completions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your ADK-powered applications.


## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Google ADK instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all ADK activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.
- Ensure you have the correct Google API key set for Gemini access.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from google.adk import Agent, Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types
from openinference.instrumentation.google_adk import GoogleADKInstrumentor

langwatch.setup(
    instrumentors=[GoogleADKInstrumentor()]
)

@langwatch.trace(name="Custom ADK Agent")
def my_custom_agent(input_message: str):
    # Your ADK agent code here
    agent = Agent(
        name="custom_agent",
        model="gemini-2.0-flash",
        instruction="Your custom instructions",
        tools=[your_custom_tools]
    )

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "custom_agent",
                "model": "gemini-2.0-flash"
            }
        )

    # Run your agent
    # ... agent execution code ...

    return "Agent response"
```

This approach allows you to combine the automatic tracing capabilities of Google ADK with the rich metadata and custom attributes provided by LangWatch.

---

# FILE: ./integration/python/integrations/dspy.mdx

---
title: DSPy Instrumentation
sidebarTitle: DSPy
description: Learn how to instrument DSPy programs with the LangWatch Python SDK
keywords: dspy, instrumentation, autotrack, LangWatch, Python
---

LangWatch provides seamless integration with DSPy, allowing you to automatically capture detailed information about your DSPy program executions, including module calls and language model interactions.

The primary way to instrument your DSPy programs is by using `autotrack_dspy()` on the current LangWatch trace.

## Using `autotrack_dspy()`

The `autotrack_dspy()` function, when called on an active trace object, dynamically patches the DSPy framework to capture calls made during the execution of your DSPy programs within that trace.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`. This ensures that all DSPy operations within that traced function are monitored.

```python
import langwatch
import dspy
import os

# Ensure LANGWATCH_API_KEY is set, or set it in langwatch.setup()
langwatch.setup() # If not done elsewhere or API key not in env

# Initialize your DSPy LM (Language Model)
# This example uses OpenAI, ensure OPENAI_API_KEY is set
lm = dspy.LM("openai/gpt-5", api_key=os.environ.get("OPENAI_API_KEY"))
dspy.settings.configure(lm=lm)

class BasicRAG(dspy.Signature):
    """Answer questions with short factoid answers."""
    question = dspy.InputField()
    answer = dspy.OutputField(desc="often between 1 and 5 words")

class SimpleModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_answer = dspy.Predict(BasicRAG)

    def forward(self, question):
        prediction = self.generate_answer(question=question)
        return dspy.Prediction(answer=prediction.answer)

@langwatch.trace(name="DSPy RAG Execution")
def run_dspy_program(user_query: str):
    # Get the current trace and enable autotracking for DSPy
    current_trace = langwatch.get_current_trace().autotrack_dspy()

    program = SimpleModule()
    prediction = program(question=user_query)
    return prediction.answer

def main():
    user_question = "What is the capital of France?"
    response = run_dspy_program(user_question)
    print(f"Question: {user_question}")
    print(f"Answer: {response}")

if __name__ == "__main__":
    main()
```

### Key points for `autotrack_dspy()`:
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments DSPy operations specifically for the duration and scope of the current trace.
-   LangWatch will capture interactions with DSPy modules (like `dspy.Predict`, `dspy.ChainOfThought`, `dspy.Retrieve`) and the underlying LM calls.

## Using Community OpenTelemetry Instrumentors

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like OpenInference, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM frameworks, including DSPy.

The [OpenInference community provides an instrumentor for DSPy](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-dspy) which can be used with LangWatch.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `OpenInferenceDSPyInstrumentor` from OpenInference) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
import dspy
import os

from openinference.instrumentation.dspy import DSPyInstrumentor

# Initialize LangWatch with the DSPyInstrumentor
langwatch.setup(
    instrumentors=[DSPyInstrumentor()]
)

# Configure DSPy LM
# This example uses OpenAI, ensure OPENAI_API_KEY is set
lm = dspy.LM("openai/gpt-5", api_key=os.environ.get("OPENAI_API_KEY"))
dspy.settings.configure(lm=lm)

class BasicRAG(dspy.Signature):
    question = dspy.InputField()
    answer = dspy.OutputField()

class SimpleModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_answer = dspy.Predict(BasicRAG)

    def forward(self, question):
        prediction = self.generate_answer(question=question)
        return dspy.Prediction(answer=prediction.answer)

@langwatch.trace(name="DSPy Call with Community Instrumentor")
def generate_with_community_instrumentor(prompt: str):
    # No need to call autotrack_dspy explicitly,
    # the community instrumentor handles DSPy calls globally.
    program = SimpleModule()
    prediction = program(question=prompt)
    return prediction.answer

if __name__ == "__main__":
    # from dotenv import load_dotenv # Make sure to load .env if you have one
    # load_dotenv()
    user_query = "What is DSPy?"
    response = generate_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

```
<Note>
  Ensure you have the respective community instrumentation library installed (e.g., `pip install openinference-instrumentation-dspy`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
import dspy
import os

from openinference.instrumentation.dspy import DSPyInstrumentor

langwatch.setup() # Initialize LangWatch

# Configure DSPy LM
lm = dspy.LM("openai/gpt-5", api_key=os.environ.get("OPENAI_API_KEY"))
dspy.settings.configure(lm=lm)

# Instrument DSPy directly using the community library
DSPyInstrumentor().instrument()

class BasicRAG(dspy.Signature):
    question = dspy.InputField()
    answer = dspy.OutputField()

class SimpleModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_answer = dspy.Predict(BasicRAG)

    def forward(self, question):
        prediction = self.generate_answer(question=question)
        return dspy.Prediction(answer=prediction.answer)

@langwatch.trace(name="DSPy Call with Direct Community Instrumentation")
def ask_dspy_directly_instrumented(original_question: str):
    program = SimpleModule()
    prediction = program(question=original_question)
    return prediction.answer

if __name__ == "__main__":
    query = "Explain the concept of few-shot learning in LLMs."
    response = ask_dspy_directly_instrumented(query)
    print(f"Query: {query}")
    print(f"Response: {response}")
```

### Key points for community instrumentors:
-   These instrumentors often patch DSPy at a global level, meaning all DSPy calls will be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
-   If instrumenting directly (e.g., `DSPyInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

<Note>
### Which Approach to Choose?

-   **`autotrack_dspy()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which DSPy program executions are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
-   **Community Instrumentors** (like OpenInference's `DSPyInstrumentor`) are powerful if you're already using OpenTelemetry, want to capture DSPy calls globally across your application, or need to instrument other libraries alongside DSPy with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send DSPy call data to LangWatch for monitoring and analysis.
</Note>

## Example: Chainlit Bot with DSPy and LangWatch

The following is a more complete example demonstrating `autotrack_dspy()` in a Chainlit application, similar to the `dspy_bot.py` found in the SDK examples.

```python
import os

import chainlit as cl
import langwatch
import dspy

langwatch.setup()

# Configure DSPy LM and RM (Retriever Model)
# Ensure OPENAI_API_KEY is in your environment for the LM
lm = dspy.LM("openai/gpt-5", api_key=os.environ["OPENAI_API_KEY"])
# This example uses a public ColBERTv2 instance for retrieval
colbertv2_wiki17_abstracts = dspy.ColBERTv2(
    url="http://20.102.90.50:2017/wiki17_abstracts"
)
dspy.settings.configure(lm=lm, rm=colbertv2_wiki17_abstracts)


class GenerateAnswer(dspy.Signature):
    """Answer questions with short factoid answers."""

    context = dspy.InputField(desc="may contain relevant facts")
    question = dspy.InputField()
    answer = dspy.OutputField(desc="often between 1 and 5 words")


class RAG(dspy.Module):
    def __init__(self, num_passages=3):
        super().__init__()
        self.retrieve = dspy.Retrieve(k=num_passages)
        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

    def forward(self, question):
        context = self.retrieve(question).passages  # type: ignore
        prediction = self.generate_answer(question=question, context=context)
        return dspy.Prediction(answer=prediction.answer)


@cl.on_message # Decorator for Chainlit message handling
@langwatch.trace() # Decorator to trace this function with LangWatch
async def main_chat_handler(message: cl.Message):
    # Get the current LangWatch trace and enable DSPy autotracking
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.autotrack_dspy()

    msg_ui = cl.Message(content="") # Chainlit UI message

    # Initialize and run the DSPy RAG program
    program = RAG()
    prediction = program(question=message.content)

    # Stream the answer to the Chainlit UI
    # Note: For simplicity, this example doesn't stream token by token
    # from the DSPy prediction if it were a streaming response.
    # DSPy's prediction.answer is typically the full string.
    final_answer = prediction.answer
    if isinstance(final_answer, str):
        await msg_ui.stream_token(final_answer)
    else:
        # Handle cases where answer might not be a direct string (e.g. structured output)
        await msg_ui.stream_token(str(final_answer))

    await msg_ui.update()

# To run this example:
# 1. Ensure you have .env file with OPENAI_API_KEY and LANGWATCH_API_KEY.
# 2. Install dependencies: pip install langwatch dspy-ai openai chainlit python-dotenv
# 3. Run with Chainlit: chainlit run your_script_name.py -w
```

By calling `autotrack_dspy()` within your LangWatch-traced functions, you gain valuable insights into your DSPy program's behavior, including latencies, token counts, and the flow of data through your defined modules and signatures. This is essential for debugging, optimizing, and monitoring your DSPy-powered AI applications.

---

# FILE: ./integration/python/integrations/llamaindex.mdx

---
title: LlamaIndex Instrumentation
sidebarTitle: LlamaIndex
description: Learn how to instrument LlamaIndex applications with LangWatch.
keywords: llamaindex, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

LlamaIndex is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. For more details on LlamaIndex, refer to the [official LlamaIndex documentation](https://docs.llamaindex.ai/).

LangWatch can capture traces generated by LlamaIndex by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install LlamaIndex and OpenInference instrumentor**:
    ```bash
    pip install llama-index openinference-instrumentation-llama-index
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for LlamaIndex using the [OpenInference LlamaIndex instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-llama-index). This approach automatically captures traces from your LlamaIndex applications and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
import os

# Initialize LangWatch with the LlamaIndex instrumentor
langwatch.setup(
    instrumentors=[LlamaIndexInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create documents
documents = SimpleDirectoryReader('data').load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Create query engine
query_engine = index.as_query_engine()

# Use the query engine as usual—traces will be sent to LangWatch automatically
def run_query(user_question: str):
    response = query_engine.query(user_question)
    return response

# Example usage
if __name__ == "__main__":
    user_question = "What is the main topic of the documents?"
    response = run_query(user_question)
    print(f"Question: {user_question}")
    print(f"Answer: {response}")
```

**That's it!** All LlamaIndex activity will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
import os

langwatch.setup(
    instrumentors=[LlamaIndexInstrumentor()]
)

# ... index setup code ...

@langwatch.trace(name="LlamaIndex Query")
def run_query(user_question: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "index_name": "my_documents",
                "model": "gpt-5"
            }
        )

    response = query_engine.query(user_question)
    return response
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `LlamaIndexInstrumentor()`: The OpenInference instrumentor automatically patches LlamaIndex components to create OpenTelemetry spans for their operations, including:
    - Document loading and processing
    - Index creation and updates
    - Query execution
    - LLM calls
    - Retrieval operations

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all document processing, indexing, querying, and LLM interactions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your LlamaIndex-powered applications.


## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine LlamaIndex instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all LlamaIndex activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your LlamaIndex code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor

langwatch.setup(
    instrumentors=[LlamaIndexInstrumentor()]
)

@langwatch.trace(name="Custom LlamaIndex Application")
def my_custom_llamaindex_app(user_question: str):
    # Your LlamaIndex code here
    documents = SimpleDirectoryReader('data').load_data()
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "index_name": "custom_index",
                "model": "gpt-5"
            }
        )

    # Run your query
    response = query_engine.query(user_question)

    return response
```

This approach allows you to combine the automatic tracing capabilities of LlamaIndex with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/aws-bedrock.mdx

---
title: AWS Bedrock Instrumentation
sidebarTitle: AWS Bedrock
description: Learn how to instrument AWS Bedrock calls with the LangWatch Python SDK using OpenInference.
keywords: aws, bedrock, boto3, instrumentation, opentelemetry, openinference, langwatch, python, tracing
---

AWS Bedrock, accessed via the `boto3` library, allows you to leverage powerful foundation models. By using the OpenInference Bedrock instrumentor, you can automatically capture OpenTelemetry traces for your Bedrock API calls. LangWatch, being an OpenTelemetry-compatible observability platform, can seamlessly ingest these traces, providing insights into your LLM interactions.

This guide explains how to configure your Python application to send Bedrock traces to LangWatch.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Bedrock Instrumentation and Dependencies**:
    You'll need `boto3` to interact with AWS Bedrock, and the OpenInference instrumentation library for Bedrock.
    ```bash
    pip install boto3 openinference-instrumentation-bedrock
    ```
    Note: `openinference-instrumentation-bedrock` will install necessary OpenTelemetry packages. Ensure your `boto3` and `botocore` versions are compatible with the Bedrock features you intend to use (e.g., `botocore >= 1.34.116` for the `converse` API).

## Instrumenting AWS Bedrock with LangWatch

The integration involves initializing LangWatch to set up the OpenTelemetry environment and then applying the Bedrock instrumentor.

### Steps:

1.  **Initialize LangWatch**: Call `langwatch.setup()` at the beginning of your application. This configures the global OpenTelemetry SDK to export traces to LangWatch.
2.  **Instrument Bedrock**: Import `BedrockInstrumentor` and call its `instrument()` method. This will patch `boto3` to automatically create spans for Bedrock client calls.

```python
import langwatch
import boto3
import json
import os
import asyncio

# 1. Initialize LangWatch for OpenTelemetry trace export
langwatch.setup()

# 2. Instrument Boto3 for Bedrock
from openinference.instrumentation.bedrock import BedrockInstrumentor
BedrockInstrumentor().instrument()

# Global Bedrock client (initialize after instrumentation)
bedrock_runtime = None
try:
    aws_session = boto3.session.Session(
        region_name=os.environ.get("AWS_REGION_NAME") # Ensure region is set
    )
    bedrock_runtime = aws_session.client("bedrock-runtime")
except Exception as e:
    print(f"Error creating Bedrock client: {e}. Ensure AWS credentials and region are configured.")

@langwatch.span(name="Bedrock - Invoke Claude")
async def invoke_claude(prompt_text: str):
    if not bedrock_runtime:
        print("Bedrock client not initialized. Skipping API call.")
        return None

    current_span = langwatch.get_current_span()
    current_span.update(model_id="anthropic.claude-v2", action="invoke_model")

    try:
        body = json.dumps({
            "prompt": f"Human: {prompt_text} Assistant:",
            "max_tokens_to_sample": 200
        })
        response = bedrock_runtime.invoke_model(modelId="anthropic.claude-v2", body=body)
        response_body = json.loads(response.get("body").read())
        completion = response_body.get("completion")
        current_span.update(outputs={"completion_preview": completion[:50] + "..." if completion else "N/A"})
        return completion
    except Exception as e:
        print(f"Error invoking model: {e}")
        if current_span:
            current_span.record_exception(e)
            current_span.set_status("error", str(e))
        raise

@langwatch.trace(name="Bedrock - Example Usage")
async def main():
    try:
        prompt = "Explain the concept of OpenTelemetry in one sentence."
        print(f"Invoking model with prompt: '{prompt}'")
        response = await invoke_claude(prompt)
        if response:
            print(f"Response from Claude: {response}")
    except Exception as e:
        print(f"An error occurred in main: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

**Key points for this approach:**
-   `langwatch.setup()`: Initializes the global OpenTelemetry environment configured for LangWatch. This must be called before any instrumented code is run.
-   `BedrockInstrumentor().instrument()`: This call patches the `boto3` library. Any subsequent Bedrock calls made using a `boto3.client("bedrock-runtime")` will automatically generate OpenTelemetry spans.
-   `@langwatch.trace()`: Creates a parent trace in LangWatch. The automated Bedrock spans generated by OpenInference will be nested under this parent trace if the Bedrock calls are made within the decorated function. This provides a clear hierarchy for your operations.
-   **API Versions**: The example shows both `invoke_model` and `converse` APIs. The `converse` API requires `botocore` version `1.34.116` or newer.

By following these steps, your application's interactions with AWS Bedrock will be traced, and the data will be sent to LangWatch for monitoring and analysis. This allows you to observe latencies, errors, and other metadata associated with your foundation model calls.
For more details on the specific attributes captured by the OpenInference Bedrock instrumentor, please refer to the [OpenInference Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/). (Note: Link to general OTel AI/OpenInference conventions, specific Bedrock attributes might be detailed in OpenInference's own docs).

Remember to replace placeholder values for AWS credentials and adapt the model IDs and prompts to your specific use case.

---

# FILE: ./integration/python/integrations/instructor.mdx

---
title: Instructor AI Instrumentation
sidebarTitle: Instructor AI
description: Learn how to instrument Instructor AI applications with LangWatch using OpenInference.
keywords: instructor, python, sdk, instrumentation, opentelemetry, langwatch, tracing, openinference, structured output
---

Instructor AI is a library that provides structured output capabilities for LLMs, making it easier to extract structured data from language models. For more details on Instructor AI, refer to the [official Instructor documentation](https://github.com/567-labs/instructor/tree/main/docs).

LangWatch can capture traces generated by Instructor AI by leveraging OpenInference's OpenAI instrumentation, since Instructor AI is built on top of OpenAI's client. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Instructor AI and OpenInference instrumentor**:
    ```bash
    pip install instructor openinference-instrumentation-instructor
    ```

3.  **Set up your OpenAI API key**:
    You'll need to configure your OpenAI API key in your environment.

## Instrumentation with OpenInference

LangWatch supports seamless observability for Instructor AI using the [OpenInference Instructor AI instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-instructor). This dedicated instrumentor automatically captures traces from your Instructor AI calls and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
import instructor
from openinference.instrumentation.instructor import InstructorInstrumentor
from openai import OpenAI
import os
from pydantic import BaseModel
from typing import List

# Initialize LangWatch with the Instructor AI instrumentor
langwatch.setup(
    instrumentors=[InstructorInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create an OpenAI client
client = OpenAI()

# Patch the client with Instructor
client = instructor.patch(client)

# Define your Pydantic models for structured output
class User(BaseModel):
    name: str
    age: int
    email: str

class UserList(BaseModel):
    users: List[User]

# Use the client as usual—traces will be sent to LangWatch automatically
def extract_user_info(text: str) -> User:
    return client.chat.completions.create(
        model="gpt-5",
        response_model=User,
        messages=[
            {"role": "user", "content": f"Extract user information from: {text}"}
        ]
    )

def extract_multiple_users(text: str) -> UserList:
    return client.chat.completions.create(
        model="gpt-5",
        response_model=UserList,
        messages=[
            {"role": "user", "content": f"Extract all users from: {text}"}
        ]
    )

# Example usage
if __name__ == "__main__":
    text = "John is 25 years old and his email is john@example.com"
    user = extract_user_info(text)
    print(f"Extracted user: {user}")

    multiple_text = "Alice is 30 (alice@example.com) and Bob is 28 (bob@example.com)"
    users = extract_multiple_users(multiple_text)
    print(f"Extracted users: {users}")
```

**That's it!** All Instructor AI calls will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
import instructor
from openinference.instrumentation.instructor import InstructorInstrumentor
from openai import OpenAI
import os
from pydantic import BaseModel

langwatch.setup(
    instrumentors=[InstructorInstrumentor()]
)

client = OpenAI()
client = instructor.patch(client)

class Product(BaseModel):
    name: str
    price: float
    category: str

@langwatch.trace(name="Product Information Extraction")
def extract_product_info(text: str) -> Product:
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "extraction_type": "product_info",
                "model": "gpt-5",
                "source_text_length": len(text)
            }
        )

    return client.chat.completions.create(
        model="gpt-5",
        response_model=Product,
        messages=[
            {"role": "user", "content": f"Extract product information from: {text}"}
        ]
    )
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `InstructorInstrumentor()`: The OpenInference instrumentor automatically patches Instructor AI operations to create OpenTelemetry spans for their operations, including:
    - Structured output generation
    - Model calls with response models
    - Validation and parsing
    - Error handling

3.  **Instructor AI Integration**: The dedicated Instructor AI instrumentor captures all Instructor AI operations (structured output generation, validation, etc.) as spans.

4.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all Instructor AI operations, including structured output generation, validation, and error handling, will be automatically traced and sent to LangWatch, providing comprehensive visibility into your structured data extraction applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Instructor AI instrumentation with other instrumentors (e.g., LangChain, DSPy) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all Instructor AI activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your Instructor AI code is being executed.
- Ensure you have the correct OpenAI API key set.
- Verify that your Pydantic models are properly defined and compatible with Instructor AI.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
import instructor
from openinference.instrumentation.instructor import InstructorInstrumentor
from openai import OpenAI
import os
from pydantic import BaseModel

langwatch.setup(
    instrumentors=[InstructorInstrumentor()]
)

client = OpenAI()
client = instructor.patch(client)

class Task(BaseModel):
    title: str
    priority: str
    due_date: str

@langwatch.trace(name="Task Extraction Pipeline")
def extract_tasks_from_text(text: str) -> List[Task]:
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "pipeline_type": "task_extraction",
                "model": "gpt-5",
                "input_length": len(text)
            }
        )

    # Your Instructor AI code here
    return client.chat.completions.create(
        model="gpt-5",
        response_model=List[Task],
        messages=[
            {"role": "user", "content": f"Extract tasks from: {text}"}
        ]
    )
```

This approach allows you to combine the automatic tracing capabilities of Instructor AI with the rich metadata and custom attributes provided by LangWatch.

---

# FILE: ./integration/python/integrations/semantic-kernel.mdx

---
title: Semantic Kernel Instrumentation
sidebarTitle: Semantic Kernel
description: Learn how to instrument Semantic Kernel applications with LangWatch.
keywords: semantic-kernel, python, sdk, instrumentation, opentelemetry, langwatch, tracing, openinference
---

Semantic Kernel is a lightweight SDK that enables you to easily build AI agents that can combine the power of LLMs with external data sources and APIs. For more details on Semantic Kernel, refer to the [official Semantic Kernel documentation](https://learn.microsoft.com/en-us/semantic-kernel/).

LangWatch can capture traces generated by Semantic Kernel using OpenInference's OpenAI instrumentation. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Semantic Kernel and OpenInference instrumentor**:
    ```bash
    pip install semantic-kernel openinference-instrumentation-openai
    ```

3.  **Set up your OpenAI API key**:
    You'll need to configure your OpenAI API key in your environment.

## Instrumentation with OpenInference

LangWatch supports observability for Semantic Kernel using the [OpenInference OpenAI instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-openai). This approach captures traces from your Semantic Kernel calls and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from openinference.instrumentation.openai import OpenAIInstrumentor
import os

# Initialize LangWatch with the OpenAI instrumentor
langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create a kernel
kernel = sk.Kernel()

# Add OpenAI chat completion service
kernel.add_service(
    OpenAIChatCompletion(
        service_id="chat-gpt",
        ai_model_id="gpt-5",
        api_key=os.environ["OPENAI_API_KEY"]
    )
)

# Use the kernel as usual—traces will be sent to LangWatch automatically
async def run_semantic_kernel_example(user_input: str):
    # Create a prompt template
    prompt = """You are a helpful assistant.
    User: {{$input}}
    Assistant: Let me help you with that."""

    # Create a function from the prompt
    kernel.add_function(
        plugin_name="chat_plugin",
        prompt=prompt,
        function_name="chat",
        description="A helpful chat function"
    )

    # Invoke the function
    result = await kernel.invoke(
        function_name="chat",
        plugin_name="chat_plugin",
        input=user_input
    )
    return result

# Example usage
async def main():
    user_query = "What's the weather like in New York?"
    response = await run_semantic_kernel_example(user_query)
    print(f"Response: {response}")

if __name__ == "__main__":
    asyncio.run(main())
```

**That's it!** All Semantic Kernel calls will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from openinference.instrumentation.openai import OpenAIInstrumentor
import os

langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

kernel = sk.Kernel()
kernel.add_service(
    OpenAIChatCompletion(
        service_id="chat-gpt",
        ai_model_id="gpt-5",
        api_key=os.environ["OPENAI_API_KEY"]
    )
)

@langwatch.trace(name="Semantic Kernel Chat Function")
async def chat_with_context(user_input: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "kernel_function": "chat",
                "model": "gpt-5",
                "input_length": len(user_input)
            }
        )

    prompt = """You are a helpful assistant.
    User: {{$input}}
    Assistant: Let me help you with that."""

    kernel.add_function(
        plugin_name="chat_plugin",
        prompt=prompt,
        function_name="chat",
        description="A helpful chat function"
    )

    result = await kernel.invoke(
        function_name="chat",
        plugin_name="chat_plugin",
        input=user_input
    )
    return result
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `OpenAIInstrumentor()`: The OpenInference instrumentor automatically patches OpenAI client operations to create OpenTelemetry spans for their operations, including:
    - Chat completions
    - Model calls
    - Response parsing
    - Error handling

3.  **Semantic Kernel Integration**: The OpenAI instrumentor captures Semantic Kernel operations (function invocations, prompt processing, etc.) as spans.

4.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, Semantic Kernel operations, including function invocations, prompt processing, and model calls, will be traced and sent to LangWatch, providing visibility into your Semantic Kernel-powered applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine Semantic Kernel instrumentation with other instrumentors (e.g., LangChain, DSPy) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture Semantic Kernel activity.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your Semantic Kernel code is being executed.
- Ensure you have the correct OpenAI API key set.
- Verify that your Semantic Kernel functions are properly defined and invoked.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from openinference.instrumentation.openai import OpenAIInstrumentor
import os

langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

kernel = sk.Kernel()
kernel.add_service(
    OpenAIChatCompletion(
        service_id="chat-gpt",
        ai_model_id="gpt-5",
        api_key=os.environ["OPENAI_API_KEY"]
    )
)

@langwatch.trace(name="Semantic Kernel Pipeline")
async def run_kernel_pipeline(user_input: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "pipeline_type": "semantic_kernel",
                "model": "gpt-5",
                "input_length": len(user_input)
            }
        )

    # Your Semantic Kernel code here
    prompt = """You are a helpful assistant.
    User: {{$input}}
    Assistant: Let me help you with that."""

    kernel.add_function(
        plugin_name="chat_plugin",
        prompt=prompt,
        function_name="chat",
        description="A helpful chat function"
    )

    result = await kernel.invoke(
        function_name="chat",
        plugin_name="chat_plugin",
        input=user_input
    )
    return result
```

This approach allows you to combine the tracing capabilities of Semantic Kernel with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/smolagents.mdx

---
title: SmolAgents Instrumentation
sidebarTitle: SmolAgents
description: Learn how to instrument SmolAgents applications with LangWatch.
keywords: smolagents, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

SmolAgents is a lightweight framework for building AI agents with minimal boilerplate. For more details on SmolAgents, refer to the [official SmolAgents documentation](https://github.com/huggingface/smolagents/tree/main/docs).

LangWatch can capture traces generated by SmolAgents by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install SmolAgents and OpenInference instrumentor**:
    ```bash
    pip install smolagents openinference-instrumentation-smolagents
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for SmolAgents using the [OpenInference SmolAgents instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-smolagents). This approach automatically captures traces from your SmolAgents and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
from smolagents import Agent
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
import os

# Initialize LangWatch with the SmolAgents instrumentor
langwatch.setup(
    instrumentors=[SmolagentsInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Create your agent
agent = Agent(
    name="hello_agent",
    model="gpt-5",
    instruction="You are a helpful assistant. Always be friendly and concise.",
)

# Use the agent as usual—traces will be sent to LangWatch automatically
def run_agent_interaction(user_message: str):
    response = agent.run(user_message)
    return response

# Example usage
if __name__ == "__main__":
    user_prompt = "Hello! How are you today?"
    response = run_agent_interaction(user_prompt)
    print(f"User: {user_prompt}")
    print(f"Agent: {response}")
```

**That's it!** All SmolAgents activity will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
from smolagents import Agent
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
import os

langwatch.setup(
    instrumentors=[SmolagentsInstrumentor()]
)

# ... agent setup code ...

@langwatch.trace(name="SmolAgents Run")
def run_agent_interaction(user_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "hello_agent",
                "model": "gpt-5"
            }
        )

    response = agent.run(user_message)
    return response
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `SmolagentsInstrumentor()`: The OpenInference instrumentor automatically patches SmolAgents components to create OpenTelemetry spans for their operations, including:
    - Agent initialization
    - Model calls
    - Tool executions
    - Response generation

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all agent interactions, model calls, and tool executions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your SmolAgents-powered applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine SmolAgents instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all SmolAgents activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
from smolagents import Agent
from openinference.instrumentation.smolagents import SmolagentsInstrumentor

langwatch.setup(
    instrumentors=[SmolagentsInstrumentor()]
)

@langwatch.trace(name="Custom SmolAgents Agent")
def my_custom_agent(input_message: str):
    # Your SmolAgents code here
    agent = Agent(
        name="custom_agent",
        model="gpt-5",
        instruction="Your custom instructions",
    )

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_name": "custom_agent",
                "model": "gpt-5"
            }
        )

    # Run your agent
    response = agent.run(input_message)

    return response
```

This approach allows you to combine the automatic tracing capabilities of SmolAgents with the rich metadata and custom attributes provided by LangWatch.

---

# FILE: ./integration/python/integrations/crew-ai.mdx

---
title: CrewAI
description: Learn how to instrument the CrewAI Python SDK with LangWatch.
keywords: crewai, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

LangWatch does not have a built-in auto-tracking integration for CrewAI. However, you can use community-provided instrumentors to integrate CrewAI with LangWatch.

## Community Instrumentors

There are two main community instrumentors available for CrewAI:

<CodeGroup>
<CodeGroupItem title="OpenLLMetry">
OpenLLMetry provides an OpenTelemetry-based instrumentation package for CrewAI.

You can find more details and installation instructions on their GitHub repository:
[traceloop/openllmetry/packages/opentelemetry-instrumentation-crewai](https://github.com/traceloop/openllmetry/tree/main/packages/opentelemetry-instrumentation-crewai)
</CodeGroupItem>

<CodeGroupItem title="OpenInference">
OpenInference, by Arize AI, also offers an instrumentation solution for CrewAI, compatible with OpenTelemetry.

For more information and setup guides, please visit their GitHub repository:
[Arize-ai/openinference/python/instrumentation/openinference-instrumentation-crewai](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-crewai)
</CodeGroupItem>
</CodeGroup>

To use these instrumentors with LangWatch, you would typically configure them to export telemetry data via OpenTelemetry, which LangWatch can then ingest.

## Integrating Community Instrumentors with LangWatch

Community-provided OpenTelemetry instrumentors for CrewAI, like those from OpenLLMetry or OpenInference, allow you to automatically capture detailed trace data from your CrewAI agents and tasks. LangWatch can seamlessly integrate with these instrumentors.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the CrewAI instrumentor to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

<CodeGroup>

```python openinference_setup.py
import langwatch
from crewai import Agent, Task, Crew
import os
from openinference.instrumentation.crewai import CrewAIInstrumentor # Assuming this is the correct import

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup(
    instrumentors=[CrewAIInstrumentor()]
)

# Define your CrewAI agents and tasks
researcher = Agent(
  role='Senior Researcher',
  goal='Discover new insights on AI',
  backstory='A seasoned researcher with a knack for uncovering hidden gems.'
)
writer = Agent(
  role='Expert Writer',
  goal='Craft compelling content on AI discoveries',
  backstory='A wordsmith who can make complex AI topics accessible and engaging.'
)

task1 = Task(description='Investigate the latest advancements in LLM prompting techniques.', agent=researcher)
task2 = Task(description='Write a blog post summarizing the findings.', agent=writer)

# Create and run the crew
crew = Crew(
  agents=[researcher, writer],
  tasks=[task1, task2],
  verbose=2
)

@langwatch.trace(name="CrewAI Execution with OpenInference")
def run_crewai_process_oi():
    result = crew.kickoff()
    return result

if __name__ == "__main__":
    print("Running CrewAI process with OpenInference...")
    output = run_crewai_process_oi()
    print("\n\nCrewAI Process Output:")
    print(output)
```

```python openllmetry_setup.py
import langwatch
from crewai import Agent, Task, Crew
import os
from opentelemetry_instrumentation_crewai import CrewAIInstrumentor # Assuming this is the correct import

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup(
    instrumentors=[CrewAIInstrumentor()]
)

# Define your CrewAI agents and tasks
researcher = Agent(
  role='Senior Researcher',
  goal='Discover new insights on AI',
  backstory='A seasoned researcher with a knack for uncovering hidden gems.'
)
writer = Agent(
  role='Expert Writer',
  goal='Craft compelling content on AI discoveries',
  backstory='A wordsmith who can make complex AI topics accessible and engaging.'
)

task1 = Task(description='Investigate the latest advancements in LLM prompting techniques.', agent=researcher)
task2 = Task(description='Write a blog post summarizing the findings.', agent=writer)

# Create and run the crew
crew = Crew(
  agents=[researcher, writer],
  tasks=[task1, task2],
  verbose=2
)

@langwatch.trace(name="CrewAI Execution with OpenLLMetry")
def run_crewai_process_ollm():
    result = crew.kickoff()
    return result

if __name__ == "__main__":
    print("Running CrewAI process with OpenLLMetry...")
    output = run_crewai_process_ollm()
    print("\n\nCrewAI Process Output:")
    print(output)
```

</CodeGroup>

<Note>
  Ensure you have the respective community instrumentation library installed:
  - For OpenLLMetry: `pip install opentelemetry-instrumentation-crewai`
  - For OpenInference: `pip install openinference-instrumentation-crewai`
  Consult the specific library's documentation for the exact package name and instrumentor class if the above assumptions are incorrect.
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

<CodeGroup>

```python openinference_direct.py
import langwatch
from crewai import Agent, Task, Crew
import os
from openinference.instrumentation.crewai import CrewAIInstrumentor # Assuming this is the correct import
# from opentelemetry.sdk.trace import TracerProvider # If managing your own provider
# from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter # If managing your own provider

langwatch.setup()

# Instrument CrewAI directly using OpenInference
CrewAIInstrumentor().instrument()

planner = Agent(
  role='Event Planner',
  goal='Plan an engaging tech conference',
  backstory='An experienced planner with a passion for technology events.'
)
task_planner = Task(description='Outline the agenda for a 3-day AI conference.', agent=planner)
conference_crew = Crew(agents=[planner], tasks=[task_planner])

@langwatch.trace(name="CrewAI Direct Instrumentation with OpenInference")
def plan_conference_oi():
    agenda = conference_crew.kickoff()
    return agenda

if __name__ == "__main__":
    print("Planning conference with OpenInference (direct)...")
    conference_agenda = plan_conference_oi()
    print("\n\nConference Agenda:")
    print(conference_agenda)
```

```python openllmetry_direct.py
import langwatch
from crewai import Agent, Task, Crew
import os
from opentelemetry_instrumentation_crewai import CrewAIInstrumentor # Assuming this is the correct import
# from opentelemetry.sdk.trace import TracerProvider # If managing your own provider
# from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter # If managing your own provider

langwatch.setup()

# Instrument CrewAI directly using OpenLLMetry
CrewAIInstrumentor().instrument()

planner = Agent(
  role='Event Planner',
  goal='Plan an engaging tech conference',
  backstory='An experienced planner with a passion for technology events.'
)
task_planner = Task(description='Outline the agenda for a 3-day AI conference.', agent=planner)
conference_crew = Crew(agents=[planner], tasks=[task_planner])

@langwatch.trace(name="CrewAI Direct Instrumentation with OpenLLMetry")
def plan_conference_ollm():
    agenda = conference_crew.kickoff()
    return agenda

if __name__ == "__main__":
    print("Planning conference with OpenLLMetry (direct)...")
    conference_agenda = plan_conference_ollm()
    print("\n\nConference Agenda:")
    print(conference_agenda)
```

</CodeGroup>

### Key points for community instrumentors:
-   These instrumentors typically patch CrewAI at a global level or integrate deeply with its execution flow, meaning all CrewAI operations (agents, tasks, tools) should be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup and lifecycle of the instrumentor.
-   If instrumenting directly (e.g., `CrewAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.
-   Always refer to the specific documentation of the community instrumentor (OpenLLMetry or OpenInference) for the most accurate and up-to-date installation and usage instructions, including the correct class names for instrumentors and any specific setup requirements.

---

# FILE: ./integration/python/integrations/strand-agents.mdx

---
title: Strands Agents Instrumentation
sidebarTitle: Strands Agents
description: Learn how to instrument Strands Agents applications with LangWatch.
keywords: strands agents, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

Strands Agents is a framework for building AI agents with a focus on simplicity and performance. For more details on Strands Agents, refer to the [official Strands Agents documentation](https://strandsagents.com).

LangWatch can capture traces generated by Strands Agents by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK and Strands Agents**:
    ```bash
    pip install langwatch strands-agents[otel] strands-agents-tools
    ```

2.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, AWS Bedrock, etc.) with the appropriate API keys.

## OpenTelemetry Setup Options

LangWatch supports three approaches for instrumenting Strands Agents with OpenTelemetry:

### Option 1: LangWatch SDK Only (Recommended)

This is the simplest approach where LangWatch handles all OpenTelemetry setup:

```python
import langwatch

# Initialize LangWatch - API key is set from environment variable automatically
langwatch.setup()
```

### Option 2: StrandsTelemetry with Custom Configuration

For more control over OpenTelemetry configuration, you can use StrandsTelemetry:

```python
import os
from strands import Agent
from strands.telemetry import StrandsTelemetry
import langwatch

# Configure StrandsTelemetry with LangWatch endpoint
strands_telemetry = StrandsTelemetry()
strands_telemetry.setup_otlp_exporter(
    endpoint=f"{os.environ.get('LANGWATCH_ENDPOINT', 'https://app.langwatch.ai')}/api/otel/v1/traces",
    headers={"Authorization": "Bearer " + os.environ["LANGWATCH_API_KEY"]},
)

# Skip LangWatch OpenTelemetry setup since StrandsTelemetry handles it
langwatch.setup(skip_open_telemetry_setup=True)
```

### Option 3: Skip OpenTelemetry Setup (When Already Configured)

If OpenTelemetry is already configured by another component in your application (like FastAPI, Django, or another framework), you can skip LangWatch's OpenTelemetry setup:

```python
import langwatch

# Skip OpenTelemetry setup since it's already configured elsewhere
langwatch.setup(skip_open_telemetry_setup=True)
```

This is useful when:
- Your backend or infrastructure framework already sets up OpenTelemetry
- You have a custom OpenTelemetry configuration
- Multiple components in your stack configure OpenTelemetry

## Basic Agent Setup

Here's a complete example showing how to create and instrument a Strands Agent:

```python
from strands import Agent
from strands.models.litellm import LiteLLMModel
import langwatch

# Initialize LangWatch
langwatch.setup()

class MyAgent:
    def __init__(self):
        # Configure the model using LiteLLM for provider flexibility
        self.model = LiteLLMModel(
            client_args={
                "api_key": os.getenv("OPENAI_API_KEY"),
            },
            model_id="openai/gpt-5-mini",
        )

        # Create the agent with tracing attributes
        self.agent = Agent(
            name="my-agent",
            model=self.model,
            system_prompt="You are a helpful AI assistant.",
            tools=[],  # Add your tools here
            trace_attributes={
                "custom.model_id": "openai/gpt-5-mini",
                "custom.example.attribute": "swift",
            },
        )

    def run(self, prompt: str):
        return self.agent(prompt)

# Use the agent
agent = MyAgent()
response = agent.run("Hello, how can you help me?")
print(response)
```

## Integration with Web Frameworks

### Chainlit Integration

Here's how to integrate Strands Agents with Chainlit while maintaining full observability:

```python
import os
from strands import Agent
from strands.models.litellm import LiteLLMModel
import langwatch
import chainlit.config as cl_config
import chainlit as cl
from dotenv import load_dotenv

load_dotenv()

# Chainlit has broken telemetry, so we need to disable it
cl_config.config.project.enable_telemetry = False

# Initialize LangWatch
langwatch.setup()

class KiteAgent:
    def __init__(self):
        self.model = LiteLLMModel(
            client_args={
                "api_key": os.getenv("OPENAI_API_KEY"),
            },
            model_id="openai/gpt-5-mini",
        )
        self.agent = Agent(
            name="kite-agent",
            model=self.model,
            system_prompt="You are a helpful AI assistant.",
            tools=[],
            trace_attributes={
                "custom.model_id": "openai/gpt-5-mini",
                "custom.example.attribute": "swift",
            },
        )

    def run(self, prompt: str):
        return self.agent(prompt)

@cl.on_message
@langwatch.trace()
async def main(message: cl.Message):
    msg = cl.Message(content="")

    # Update the current trace with additional metadata
    langwatch.get_current_trace().update(
        metadata={
            "custom.example.attribute": "swift",
        }
    )

    agent = KiteAgent()
    response = agent.run(message.content)

    await msg.stream_token(str(response))
    await msg.update()
```

## Adding Custom Attributes and Metadata

You can add custom attributes to your traces in several ways:

### Agent-Level Attributes

```python
agent = Agent(
    name="my-agent",
    model=model,
    system_prompt="You are a helpful AI assistant.",
    trace_attributes={
        "custom.model_id": "openai/gpt-5-mini",
        "custom.environment": "production",
        "custom.service": "customer-support",
    },
)
```

### Function-Level Metadata

```python
@langwatch.trace(name="Handle Request")
def handle_request(input_message: str):
    langwatch.get_current_trace().update(
        metadata={
            "user_id": "user_001",
            "thread_id": "thead_001"
        }
    )

    return my_custom_strands_agents_app(input_message)

@langwatch.span(name="Handle")
def my_custom_strands_agents_app(input_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace

    response = agent(input_message)
    return response
```

## How it Works

1.  **LangWatch Setup**: `langwatch.setup()` initializes the LangWatch SDK and sets up OpenTelemetry tracing.

2.  **Model Configuration**: Use `LiteLLMModel` for flexible provider support or specific model classes like `BedrockModel` for AWS Bedrock.

3.  **Agent Creation**: The `Agent` constructor accepts `trace_attributes` for consistent metadata across all traces.

4.  **Automatic Tracing**: All agent interactions, model calls, and tool executions are automatically traced and sent to LangWatch.

5.  **Custom Metadata**: Use `@langwatch.trace()` decorators and `langwatch.get_current_trace().update()` to add context-specific metadata.

## Environment Variables

Set up your environment variables in a `.env` file:

```bash
LANGWATCH_API_KEY=your-langwatch-api-key
OPENAI_API_KEY=your-openai-api-key
LANGWATCH_ENDPOINT=https://app.langwatch.ai  # Optional, defaults to this value
```

## Notes

- The `strands-agents[otel]` package includes OpenTelemetry support out of the box.
- The `trace_attributes` parameter allows you to add consistent metadata to all traces from a specific agent instance.
- For advanced configuration, see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the telemetry is properly configured and that your agent code is being executed.
- If you aren't using the LangWatch SDK's automatic OpenTelemetry setup and traces are not showing, double check the url and path given to OpenTelemetry.

## Next Steps

Once you have instrumented your code, you can manage, evaluate and debug your application:

- **View traces** in the LangWatch dashboard
- **Add evaluation scores** to your traces
- **Create custom dashboards** for monitoring
- **Set up alerts** for performance issues
- **Export data** for further analysis

## Learn More

For more detailed information, refer to the official documentation and other examples:

- [Strands Agents Documentation](https://strandsagents.com)
- [Strands Agents GitHub Cookbook](https://github.com/strands-agents/samples/blob/main/01-tutorials/01-fundamentals/08-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb)
- [LangWatch Python Integration Guide](/integration/python/guide)

---

# FILE: ./integration/python/integrations/google-genai.mdx

---
title: Google GenAI Instrumentation
sidebarTitle: Google GenAI
description: Learn how to instrument Google GenAI API calls with the LangWatch Python SDK
keywords: google genai, gemini, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Google GenAI, allowing you to capture detailed information about your Gemini API calls automatically. The recommended approach is to use OpenInference instrumentation, which provides comprehensive tracing for Google GenAI API calls and integrates seamlessly with LangWatch.

## Using OpenInference Instrumentation

The recommended approach for instrumenting Google GenAI calls with LangWatch is to use the OpenInference instrumentation library, which provides comprehensive tracing for Google GenAI API calls.

## Installation and Setup

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM providers, including Google GenAI.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `GoogleGenAIInstrumentor` from OpenInference or OpenLLMetry) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
import google.generativeai as genai
import os

# Example using OpenInference's GoogleGenAIInstrumentor
from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor

# Initialize LangWatch with the GoogleGenAIInstrumentor
langwatch.setup(
    instrumentors=[GoogleGenAIInstrumentor()]
)

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

@langwatch.trace(name="Google GenAI Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles Google GenAI calls globally.
    response = model.generate_content(prompt)
    return response.text

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```

<Note>
Ensure you have the respective community instrumentation library installed (e.g., `pip install openinference-instrumentation-google-genai` or `pip install opentelemetry-instrumentation-google-genai`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
import google.generativeai as genai
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor

langwatch.setup()

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

# Instrument Google GenAI directly using the community library
GoogleGenAIInstrumentor().instrument()

@langwatch.trace(name="Google GenAI Call with Direct Community Instrumentation")
def get_story_ending(beginning: str):
    response = model.generate_content(
        f"You are a creative writer. Complete the story: {beginning}"
    )
    return response.text

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

### Key points for community instrumentors:
- These instrumentors often patch Google GenAI at a global level, meaning all Google GenAI calls from any client instance will be captured once instrumented.
- If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
- If instrumenting directly (e.g., `GoogleGenAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

## Advanced Usage Examples

### Using Google GenAI with Function Calling

When using Google GenAI's function calling capabilities, the instrumentation will capture both the initial request and the function execution:

```python
import langwatch
import google.generativeai as genai
import os

langwatch.setup()

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

@langwatch.trace(name="Google GenAI Function Call")
def get_weather_with_functions(city: str):
    # Define the function schema
    function_declarations = [
        {
            "name": "get_weather",
            "description": "Get the current weather for a city",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "The city name"}
                },
                "required": ["city"]
            }
        }
    ]

    response = model.generate_content(
        f"What's the weather like in {city}?",
        generation_config=genai.types.GenerationConfig(
            function_calling_config=genai.types.FunctionCallingConfig(
                function_declarations=function_declarations
            )
        )
    )
    return response

if __name__ == "__main__":
    result = get_weather_with_functions("San Francisco")
    print(f"Response: {result}")
```

<Note>
### Which Approach to Choose?

- **`autotrack_google_genai_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which Google GenAI client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
- **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture Google GenAI calls globally across your application, or need to instrument other libraries alongside Google GenAI with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send Google GenAI call data to LangWatch for monitoring and analysis.
</Note>
---

# FILE: ./integration/python/integrations/pydantic-ai.mdx

---
title: PydanticAI Instrumentation
sidebarTitle: PydanticAI
description: Learn how to instrument PydanticAI applications with the LangWatch Python SDK.
keywords: pydantic-ai, pydanticai, instrumentation, opentelemetry, langwatch, python, tracing
---

PydanticAI is a library for building AI applications with Pydantic models. It features built-in, optional support for OpenTelemetry, allowing detailed tracing of agent runs and model interactions. LangWatch, being an OpenTelemetry-compatible observability platform, can seamlessly ingest these traces.

This guide explains how to configure PydanticAI and LangWatch to capture this observability data. For more background on PydanticAI's observability features, refer to their debugging and monitoring documentation.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install PydanticAI**:
    ```bash
    pip install pydantic-ai
    ```

## Instrumenting PydanticAI with LangWatch

The primary way to integrate PydanticAI with LangWatch is by leveraging PydanticAI's native OpenTelemetry emission in an environment configured by `langwatch.setup()`.

### Using PydanticAI's Built-in OpenTelemetry with LangWatch Global Setup

When `langwatch.setup()` is called, it initializes a global OpenTelemetry environment, including a trace exporter configured for LangWatch. If PydanticAI's instrumentation is enabled (via `Agent(instrument=True)` or `Agent.instrument_all()`), it will emit OpenTelemetry traces that are automatically captured by LangWatch.

```python
import langwatch
from pydantic_ai import Agent
from pydantic_ai.agent import InstrumentationSettings # Optional, for event_mode
import os
import asyncio

# 1. Initialize LangWatch
# This sets up the global OpenTelemetry environment for LangWatch.
langwatch.setup()

# 2. Enable PydanticAI Instrumentation
# Option A: Instrument all agents globally
Agent.instrument_all()

# Option B: Instrument a specific agent instance
# For this example, we'll instrument a specific agent.
# If targeting a generic OTel collector like LangWatch, event_mode='logs' is recommended
# as it aligns with OTel semantic conventions for capturing message events.
# The default mode might use a custom attribute format for events.
instrumentation_settings_for_langwatch = InstrumentationSettings(event_mode='logs')
agent = Agent(model_name='openai:gpt-5', instrument=instrumentation_settings_for_langwatch)

@langwatch.trace(name="PydanticAI - City Capital Query")
async def get_capital_city(country: str):
    langwatch.get_current_trace().update(metadata={"country_queried": country})

    try:
        # PydanticAI agent calls will now generate OpenTelemetry spans
        # that LangWatch captures under the "PydanticAI - City Capital Query" trace.
        result = await agent.run(f'What is the capital of {country}?')
        return result.output
    except Exception as e:
        if current_trace:
            current_trace.record_exception(e)
            current_trace.set_status("error", str(e))
        raise

async def main():
    try:
        capital = await get_capital_city("France")
        print(f"The capital of France is: {capital}")
    except Exception as e:
        print(f"Error getting capital of France: {e}")

    try:
        capital_error = await get_capital_city("NonExistentCountry") # Example to show error tracing
        print(f"Query for NonExistentCountry returned: {capital_error}")
    except Exception as e:
        print(f"Correctly caught error for NonExistentCountry: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

**Key points for this approach:**
-   `langwatch.setup()`: Essential for initializing the OpenTelemetry environment that LangWatch uses.
-   `Agent(instrument=True)` or `Agent.instrument_all()`: Activates PydanticAI's OpenTelemetry signal emission.
-   `InstrumentationSettings(event_mode='logs')`: As per PydanticAI documentation, using `event_mode='logs'` aligns message capture with OpenTelemetry Semantic Conventions for Generative AI, which might be better for generic OTel collectors. The default mode (`json_array`) uses a custom attribute format for events.
-   `@langwatch.trace()`: Creates a parent trace in LangWatch, under which PydanticAI's operation spans will be nested.

By following these steps, you can effectively monitor your PydanticAI applications using LangWatch, gaining insights into agent behavior, model performance, and overall application flow.

---

# FILE: ./integration/python/integrations/anthropic.mdx

---
title: Anthropic Instrumentation
sidebarTitle: Python
description: Learn how to instrument Anthropic API calls with the LangWatch Python SDK
icon: python
keywords: anthropic, claude, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Anthropic, allowing you to capture detailed information about your Claude API calls automatically. The recommended approach is to use OpenInference instrumentation, which provides comprehensive tracing for Anthropic API calls and integrates seamlessly with LangWatch.

## Using OpenInference Instrumentation

The recommended approach for instrumenting Anthropic calls with LangWatch is to use the OpenInference instrumentation library, which provides comprehensive tracing for Anthropic API calls.

## Installation and Setup

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM providers, including Anthropic.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `AnthropicInstrumentor` from OpenInference or OpenLLMetry) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from anthropic import Anthropic
import os

# Example using OpenInference's AnthropicInstrumentor
from openinference.instrumentation.anthropic import AnthropicInstrumentor

# Initialize LangWatch with the AnthropicInstrumentor
langwatch.setup(
    instrumentors=[AnthropicInstrumentor()]
)

client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

@langwatch.trace(name="Anthropic Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles Anthropic calls globally.
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```

<Note>
Ensure you have the respective community instrumentation library installed (e.g., `pip install openinference-instrumentation-anthropic` or `pip install opentelemetry-instrumentation-anthropic`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from anthropic import Anthropic
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.anthropic import AnthropicInstrumentor

langwatch.setup()
client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Instrument Anthropic directly using the community library
AnthropicInstrumentor().instrument()

@langwatch.trace(name="Anthropic Call with Direct Community Instrumentation")
def get_story_ending(beginning: str):
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[
            {"role": "system", "content": "You are a creative writer. Complete the story."},
            {"role": "user", "content": beginning}
        ]
    )
    return response.content[0].text

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

### Key points for community instrumentors:
- These instrumentors often patch Anthropic at a global level, meaning all Anthropic calls from any client instance will be captured once instrumented.
- If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
- If instrumenting directly (e.g., `AnthropicInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

## Advanced Usage Examples

### Using Anthropic with Tools

When using Anthropic's tool calling capabilities, the instrumentation will capture both the initial request and the tool execution:

```python
import langwatch
from anthropic import Anthropic
import os

langwatch.setup()
client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

@langwatch.trace(name="Anthropic Tool Call")
def get_weather_with_tools(city: str):
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        tools=[{
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get the current weather for a city",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {"type": "string", "description": "The city name"}
                    },
                    "required": ["city"]
                }
            }
        }],
        messages=[{"role": "user", "content": f"What's the weather like in {city}?"}]
    )
    return response

if __name__ == "__main__":
    result = get_weather_with_tools("San Francisco")
    print(f"Response: {result}")
```

### Streaming Responses

For streaming responses, the instrumentation captures the entire streaming session:

```python
import langwatch
from anthropic import Anthropic
import os

langwatch.setup()
client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

@langwatch.trace(name="Anthropic Streaming")
def stream_response(prompt: str):
    with client.messages.stream(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    ) as stream:
        for text in stream.text_stream:
            print(text, end="", flush=True)
        print()  # New line after streaming

if __name__ == "__main__":
    stream_response("Write a short story about a robot learning to paint.")
```

<Note>
### Which Approach to Choose?

- **`autotrack_anthropic_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which Anthropic client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
- **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture Anthropic calls globally across your application, or need to instrument other libraries alongside Anthropic with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send Anthropic call data to LangWatch for monitoring and analysis.
</Note>

---

# FILE: ./integration/python/integrations/langgraph.mdx

---
title: LangGraph Instrumentation
sidebarTitle: LangGraph
description: Learn how to instrument LangGraph applications with the LangWatch Python SDK.
icon: python
keywords: langgraph, instrumentation, callback, opentelemetry, langwatch, python, tracing, openinference, openllmetry
---

LangGraph is a powerful framework for building LLM applications. LangWatch integrates with LangGraph to provide detailed observability into your chains, agents, LLM calls, and tool usage.

<Note>
The community instrumentors below are for LangChain, but LangGraph is compatible with them.
</Note>

## 2. Using Community OpenTelemetry Instrumentors

Dedicated LangChain instrumentors from libraries like OpenInference and OpenLLMetry can also be used to capture LangGraph operations as OpenTelemetry traces, which LangWatch can then ingest.

### Instrumenting LangGraph with Dedicated Instrumentors

#### i. Via `langwatch.setup()`

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangChainInstrumentor
import os
import asyncio

langwatch.setup(
    instrumentors=[LangChainInstrumentor()] # Add OpenInference LangChainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference via Setup")
async def handle_message_oinference_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (setup) example.")
        return
    response = await handle_message_oinference_setup("Explain Langchain instrumentation with OpenInference.")
    print(f"AI (OInference Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_setup())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangChainInstrumentor
import os
import asyncio

langwatch.setup(
    instrumentors=[LangChainInstrumentor()] # Add OpenLLMetry LangChainInstrumentor
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry via Setup")
async def handle_message_openllmetry_setup(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_setup():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (setup) example.")
        return
    response = await handle_message_openllmetry_setup("Explain Langchain instrumentation with OpenLLMetry.")
    print(f"AI (OpenLLMetry Setup): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_setup())
```
</CodeGroup>

#### ii. Direct Instrumentation

<CodeGroup>
```python OpenInference
# OpenInference Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from openinference.instrumentation.langchain import LangChainInstrumentor
import os
import asyncio

langwatch.setup()
LangChainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenInference Direct")
async def handle_message_oinference_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_oinference_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenInference Langchain (direct) example.")
        return
    response = await handle_message_oinference_direct("How does direct Langchain instrumentation work?")
    print(f"AI (OInference Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_oinference_direct())
```
```python OpenLLMetry
# OpenLLMetry Example
import langwatch
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from opentelemetry.instrumentation.langchain import LangChainInstrumentor
import os
import asyncio

langwatch.setup()
LangChainInstrumentor().instrument() # Instrument Langchain directly

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are very brief."), ("human", "{question}")]
)
runnable = prompt | model | StrOutputParser()

@langwatch.trace(name="Langchain - OpenLLMetry Direct")
async def handle_message_openllmetry_direct(user_question: str):
    response = await runnable.ainvoke({"question": user_question})
    return response

async def main_community_openllmetry_direct():
    if not os.getenv("OPENAI_API_KEY"):
        print("OPENAI_API_KEY not set. Skipping OpenLLMetry Langchain (direct) example.")
        return
    response = await handle_message_openllmetry_direct("How does direct Langchain instrumentation work with OpenLLMetry?")
    print(f"AI (OpenLLMetry Direct): {response}")

if __name__ == "__main__":
    asyncio.run(main_community_openllmetry_direct())
```
</CodeGroup>

---

# FILE: ./integration/python/integrations/open-ai-agents.mdx

---
title: OpenAI Agents SDK Instrumentation
sidebarTitle: OpenAI Agents
description: Learn how to instrument OpenAI Agents with the LangWatch Python SDK
keywords: openai-agents, instrumentation, openinference, LangWatch, Python, tracing
---

LangWatch allows you to monitor your OpenAI Agents by integrating with their tracing capabilities. Since OpenAI Agents manage their own execution flow, including LLM calls and tool usage, the direct `autotrack_openai_calls()` method used for the standard OpenAI client is not applicable here.

Instead, you can integrate LangWatch in one of two ways:

1.  **Using OpenInference Instrumentation (Recommended)**: Leverage the `openinference-instrumentation-openai-agents` library, which provides OpenTelemetry-based instrumentation for OpenAI Agents. This is generally the simplest and most straightforward method.
2.  **Alternative: Using OpenAI Agents' Built-in Tracing with a Custom Processor**: If you choose not to use OpenInference or have highly specific requirements, you can adapt the built-in tracing mechanism of the `openai-agents` SDK to forward trace data to LangWatch by implementing your own custom `TracingProcessor`.

This guide will walk you through both methods.

## 1. Using OpenInference Instrumentation for OpenAI Agents (Recommended)

The most straightforward way to integrate LangWatch with OpenAI Agents is by using the OpenInference instrumentation library specifically designed for it: `openinference-instrumentation-openai-agents`. This library is currently in an Alpha stage, so while ready for experimentation, it may undergo breaking changes.

This approach uses OpenTelemetry-based instrumentation and is generally recommended for ease of setup.

### Installation

First, ensure you have the necessary packages installed:

```bash
pip install langwatch openai-agents openinference-instrumentation-openai-agents
```

### Integration via `langwatch.setup()`

You can pass an instance of the `OpenAIAgentsInstrumentor` from `openinference-instrumentation-openai-agents` to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from openai_agents.agents import Agent, Runner # Using openai_agents SDK
from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor
import os
import asyncio

# Ensure LANGWATCH_API_KEY is set in your environment, or pass it to setup
# e.g., langwatch.setup(api_key="your_api_key", instrumentors=[OpenAIAgentsInstrumentor()])
# If LANGWATCH_API_KEY is in env, this is sufficient:
langwatch.setup(
    instrumentors=[OpenAIAgentsInstrumentor()]
)

# Initialize your agent
agent = Agent(name="ExampleAgent", instructions="You are a helpful assistant.")

@langwatch.trace(name="OpenAI Agent Run with OpenInference")
async def run_agent_with_openinference(prompt: str):
    # The OpenAIAgentsInstrumentor will automatically capture agent activities.
    result = await Runner.run(agent, prompt)
    return result.final_output

async def main():
    user_query = "Tell me a fun fact."
    response = await run_agent_with_openinference(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    asyncio.run(main())
```

<Note>
  The `OpenAIAgentsInstrumentor` is part of the `openinference-instrumentation-openai-agents` package. Always refer to its [official documentation](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-openai-agents) for the latest updates, especially as it's in Alpha.
</Note>

### Direct Instrumentation

Alternatively, if you manage your OpenTelemetry `TracerProvider` more directly (e.g., if LangWatch is configured to use an existing global provider), you can use the instrumentor's `instrument()` method. LangWatch will pick up the spans if its exporter is part of the active `TracerProvider`.

```python
import langwatch
from openai_agents.agents import Agent, Runner
from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor
import os
import asyncio

# Initialize LangWatch (it will set up its OTel exporter)
langwatch.setup() # Ensure API key is available via env or parameters

# Instrument OpenAI Agents directly
OpenAIAgentsInstrumentor().instrument()

agent = Agent(name="ExampleAgentDirect", instructions="You are a helpful assistant.")

@langwatch.trace(name="OpenAI Agent Run with Direct OpenInference")
async def run_agent_direct_instrumentation(prompt: str):
    result = await Runner.run(agent, prompt)
    return result.final_output

async def main():
    user_query = "What's the weather like?"
    response = await run_agent_direct_instrumentation(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    asyncio.run(main())
```

Key points for OpenInference instrumentation:
-   It patches `openai-agents` activities globally once instrumented.
-   Ensure `langwatch.setup()` is called so LangWatch's OpenTelemetry exporter is active and configured.
-   The `@langwatch.trace()` decorator on your calling function helps create a parent span under which the agent's detailed operations will be nested.

## 2. Alternative: Using OpenAI Agents' Built-in Tracing with a Custom Processor

If you prefer not to use the OpenInference instrumentor, or if you have highly specific tracing requirements not met by it, you can leverage the `openai-agents` SDK's own [built-in tracing system](https://openai.github.io/openai-agents-python/tracing/).

This involves creating a custom `TracingProcessor` that intercepts trace data from the `openai-agents` SDK and then uses the standard OpenTelemetry Python API to create OpenTelemetry spans. LangWatch will then ingest these OpenTelemetry spans, provided `langwatch.setup()` has been called.

**Conceptual Outline for Your Custom Processor:**

1.  **Initialize LangWatch**: Ensure `langwatch.setup()` is called in your application. This sets up LangWatch to receive OpenTelemetry data.
2.  **Implement Your Custom `TracingProcessor`**:
    -   Following the `openai-agents` SDK documentation, create a class that implements their `TracingProcessor` interface (see their docs on [Custom Tracing Processors](https://openai.github.io/openai-agents-python/tracing/#custom-tracing-processors) and the API reference for [`TracingProcessor`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.TracingProcessor)).
    -   In your processor's methods (e.g., `on_span_start`, `on_span_end`), you will receive `Trace` and `Span` objects from the `openai-agents` SDK.
    -   You will then use the `opentelemetry-api` and `opentelemetry-sdk` (e.g., `opentelemetry.trace.get_tracer(__name__).start_span()`) to translate this information into OpenTelemetry spans, including their names, attributes, timings, and status. Consult the `openai-agents` documentation on [Traces and spans](https://openai.github.io/openai-agents-python/tracing/#traces-and-spans) for details on their data structures.
3.  **Register Your Custom Processor**: Use `openai_agents.tracing.add_trace_processor(your_custom_processor)` or `openai_agents.tracing.set_trace_processors([your_custom_processor])` as per the `openai-agents` SDK documentation.

**Implementation Guidance:**

LangWatch does not provide a pre-built custom `TracingProcessor` for this purpose. The implementation of such a processor is your responsibility and should be based on the official `openai-agents` SDK documentation. This ensures your processor correctly interprets the agent's trace data and remains compatible with `openai-agents` SDK updates.

-   Key `openai-agents` documentation:
    -   [OpenAI Agents Tracing Documentation](https://openai.github.io/openai-agents-python/tracing/)
    -   [API Reference for Tracing](https://openai.github.io/openai-agents-python/ref/tracing/)

<Warning>
Implementing a custom `TracingProcessor` is an advanced task that requires:
- A thorough understanding of both the `openai-agents` tracing internals and OpenTelemetry concepts and semantic conventions.
- Careful mapping of `openai-agents` `SpanData` types to OpenTelemetry attributes.
- Robust handling of span parenting, context propagation, and error states.
- Diligent maintenance to keep your processor aligned with any changes in the `openai-agents` SDK.
This approach offers maximum flexibility but comes with significant development and maintenance overhead.
</Warning>

## Which Approach to Choose?

-   **OpenInference Instrumentation (Recommended)**:
    -   **Pros**: Significantly simpler to set up and maintain. Relies on a community-supported library (`openinference-instrumentation-openai-agents`) designed for OpenTelemetry integration. Aligns well with standard OpenTelemetry practices.
    -   **Cons**: As the `openinference-instrumentation-openai-agents` library is in Alpha, it may have breaking changes. You have less direct control over the exact span data compared to a fully custom processor.

-   **Custom `TracingProcessor` (Alternative for advanced needs)**:
    -   **Pros**: Offers complete control over the transformation of trace data from `openai-agents` to OpenTelemetry. Allows for highly customized span data and behaviors.
    -   **Cons**: Far more complex to implement correctly and maintain. Requires deep expertise in both `openai-agents` tracing and OpenTelemetry. You are responsible for adapting your processor to any changes in the `openai-agents` SDK.

For most users, the **OpenInference instrumentation is the recommended path** due to its simplicity and lower maintenance burden.

The **custom `TracingProcessor`** approach should generally be reserved for situations where the OpenInference instrumentor is unsuitable, or when you have highly specialized tracing requirements that demand direct manipulation of the agent's trace data before converting it to OpenTelemetry spans.

---
Always refer to the latest documentation for `langwatch`, `openai-agents`, and `openinference-instrumentation-openai-agents` for the most up-to-date instructions and API details.

---

# FILE: ./integration/python/integrations/lite-llm.mdx

---
title: LiteLLM Instrumentation
sidebarTitle: LiteLLM
description: Learn how to instrument LiteLLM calls with the LangWatch Python SDK.
keywords: litellm, instrumentation, autotrack, opentelemetry, langwatch, python, tracing, openinference, openllmetry
---

LiteLLM provides a unified interface to various Large Language Models. LangWatch integrates with LiteLLM by capturing OpenTelemetry traces, enabling detailed observability into your LLM calls made through LiteLLM.

This guide outlines three primary approaches for instrumenting LiteLLM with LangWatch:

1.  **Using `autotrack_litellm_calls()`**: This method, part of the LangWatch SDK, dynamically patches your LiteLLM module instance for the current trace to capture its calls.
2.  **Using LiteLLM's Native OpenTelemetry Tracing with Global Setup**: LiteLLM can automatically generate OpenTelemetry traces for its operations when a global OpenTelemetry environment (established by `langwatch.setup()`) is active.
3.  **Using Community OpenTelemetry Instrumentors (for Underlying SDKs)**: If LiteLLM internally uses other instrumented SDKs (like the `openai` SDK for OpenAI models), you can leverage community instrumentors for those specific underlying SDKs.

## Using `autotrack_litellm_calls()`

The `autotrack_litellm_calls()` function, called on a trace object, provides a straightforward way to capture all LiteLLM calls for the duration of the current trace. This is often the most direct way to ensure LiteLLM operations are captured by LangWatch within a specific traced function.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`.

```python
import langwatch
import litellm
import os
import asyncio
from typing import cast
from litellm import CustomStreamWrapper # For streaming example
from litellm.types.utils import StreamingChoices # For streaming example

langwatch.setup()

@langwatch.trace(name="LiteLLM Autotrack Example")
async def get_litellm_response_autotrack(user_prompt: str):
    # Get the current trace and enable autotracking for the litellm module
    langwatch.get_current_trace().autotrack_litellm_calls(litellm) # Pass the imported litellm module

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_prompt}
    ]

    # This call will now be automatically captured as a span by LangWatch
    response = await litellm.acompletion(
        model="groq/llama3-70b-8192",
        messages=messages,
    )
    return response.choices[0].message.content

async def main():
    reply = await get_litellm_response_autotrack("Tell me about LiteLLM.")
    print("AI Response:", reply)

if __name__ == "__main__":
    asyncio.run(main())
```

**Key points for `autotrack_litellm_calls()`:**
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments the passed `litellm` module instance specifically for the current trace.

## Using Community OpenTelemetry Instrumentors

If you are already using a dedicated community instrumentor for LiteLLM, such as the one provided by OpenInference, you can pass an instance of `LiteLLMInstrumentor` from `openinference.instrumentation.litellm` to the `instrumentors` list in `langwatch.setup()`.

### 1. Via `langwatch.setup()`

You can pass an instance of `LiteLLMInstrumentor` from `openinference.instrumentation.litellm` to the `instrumentors` list in `langwatch.setup()`.

```python
import langwatch
import litellm
import os
import asyncio

# Example using OpenInference's LiteLLMInstrumentor
from openinference.instrumentation.litellm import LiteLLMInstrumentor

# 1. Initialize LangWatch with the LiteLLMInstrumentor
langwatch.setup(
    instrumentors=[LiteLLMInstrumentor()] # Instruments LiteLLM directly
)

@langwatch.trace(name="LiteLLM via OpenInference Instrumentor Setup")
async def get_response_via_litellm_instrumentor_setup(user_prompt: str):
    messages = [
        {"role": "user", "content": user_prompt}
    ]

    # This LiteLLM call will be captured by the LiteLLMInstrumentor
    response = await litellm.acompletion(
        model="groq/llama3-70b-8192", # Example model
        messages=messages
    )
    return response.choices[0].message.content

async def main_community_litellm_instrumentor_setup():
    reply = await get_response_via_litellm_instrumentor_setup("Explain OpenInference for LiteLLM.")
    print(f"AI Response (OpenInference via setup): {reply}")

if __name__ == "__main__":
    asyncio.run(main_community_litellm_instrumentor_setup())
```

<Note>
  Ensure you have the `openinference-instrumentation-litellm` library installed.
</Note>

### 2. Direct Instrumentation with `LiteLLMInstrumentor`

If you are managing your OpenTelemetry setup more directly, you can call `instrument()` on an instance of `LiteLLMInstrumentor`.

```python
import langwatch
import litellm
import os
import asyncio

from openinference.instrumentation.litellm import LiteLLMInstrumentor

# 1. Initialize LangWatch (sets up global OTel provider for LangWatch exporter)
langwatch.setup()

# 2. Instrument LiteLLM directly using its OpenInference instrumentor
# This should be done once, early in your application lifecycle.
LiteLLMInstrumentor().instrument()

@langwatch.trace(name="LiteLLM via Directly Instrumented OpenInference")
async def get_response_direct_litellm_instrumentation(user_prompt: str):
    messages = [
        {"role": "user", "content": user_prompt}
    ]
    response = await litellm.acompletion(model="groq/llama3-70b-8192", messages=messages)
    return response.choices[0].message.content

async def main_direct_litellm_instrumentation():
    reply = await get_response_direct_litellm_instrumentation("How does direct OTel instrumentation work for LiteLLM?")
    print(f"AI Response (direct OpenInference): {reply}")

if __name__ == "__main__":
    asyncio.run(main_direct_litellm_instrumentation())

```

**Key points for using OpenInference `LiteLLMInstrumentor`:**
-   This instrumentor specifically targets LiteLLM calls.
-   It provides an alternative to `autotrack_litellm_calls` if you prefer an explicit instrumentor pattern or are using OpenInference across your stack.

<Note>
### Which Approach to Choose?

-   **`autotrack_litellm_calls()`**: Best for explicit, trace-specific instrumentation of LiteLLM. Offers clear control over when LiteLLM calls are tracked by LangWatch within a given trace.
-   **OpenInference `LiteLLMInstrumentor`**: Use if you are standardizing on OpenInference instrumentors or prefer this explicit way of instrumenting LiteLLM itself (rather than its underlying SDKs). It provides traces directly from LiteLLM's perspective.

Choose the method that best fits your instrumentation strategy and the level of detail required.
</Note>

---

# FILE: ./integration/python/integrations/other.mdx

---
title: Other OpenTelemetry Instrumentors
sidebarTitle: Other
description: Learn how to use any OpenTelemetry-compatible instrumentor with LangWatch.
keywords: opentelemetry, instrumentation, custom, other, generic, BaseInstrumentor, LangWatch, Python
---

LangWatch is designed to be compatible with the broader OpenTelemetry ecosystem. Beyond the specifically documented integrations, you can use LangWatch with any Python library that has an OpenTelemetry instrumentor, provided that the instrumentor adheres to the standard OpenTelemetry Python `BaseInstrumentor` interface.

## Using Custom/Third-Party OpenTelemetry Instrumentors

If you have a specific library you want to trace, and there's an OpenTelemetry instrumentor available for it (either a community-provided one not yet listed in our specific integrations, or one you've developed yourself), you can integrate it with LangWatch.

The key is that the instrumentor should be an instance of a class that inherits from `opentelemetry.instrumentation.instrumentor.BaseInstrumentor`. You can find the official documentation for this base class here:

- [OpenTelemetry BaseInstrumentor Documentation](https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/base/instrumentor.html#opentelemetry.instrumentation.instrumentor.BaseInstrumentor)

### Integration via `langwatch.setup()`

To use such an instrumentor, you simply pass an instance of it to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage its lifecycle (calling its `instrument()` and `uninstrument()` methods appropriately).

Here's a conceptual example using the OpenTelemetry `LoggingInstrumentor`:

```python
import langwatch
import os
import logging # Standard Python logging

# Import an off-the-shelf OpenTelemetry instrumentor
# Ensure you have this package installed: pip install opentelemetry-instrumentation-logging
from opentelemetry.instrumentation.logging import LoggingInstrumentor

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup(
    instrumentors=[
        LoggingInstrumentor() # Pass an instance of the instrumentor
    ]
)

# Configure standard Python logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# You might want to add a handler if you also want to see logs in the console
# handler = logging.StreamHandler()
# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# handler.setFormatter(formatter)
# logger.addHandler(handler)

@langwatch.trace(name="Task with Instrumented Logging")
def perform_task_with_logging():
    logger.info("Starting the task.")
    # ... some work ...
    logger.warning("Something to be aware of happened during the task.")
    # ... more work ...
    logger.info("Task completed.")
    return "Task finished successfully"

if __name__ == "__main__":
    print("Running example with LoggingInstrumentor...")
    result = perform_task_with_logging()
    print(f"Result: {result}")
    # Spans for the log messages (e.g., logger.info, logger.warning)
    # would be generated by LoggingInstrumentor and captured by LangWatch.
```

When this code runs, the `LoggingInstrumentor` (managed by `langwatch.setup()`) will automatically create OpenTelemetry spans for any log messages emitted by the standard Python `logging` module. LangWatch will then capture these spans.

## Discovering More Community Instrumentors

Many Python libraries, especially in the AI/ML space, are instrumented by community-driven OpenTelemetry projects. If you're looking for pre-built instrumentors, these are excellent places to start:

*   **OpenInference (by Arize AI):** [https://github.com/Arize-ai/openinference](https://github.com/Arize-ai/openinference)
    *   This project provides instrumentors for a wide range of AI/ML libraries and frameworks. Examples include:
        *   OpenAI
        *   Anthropic
        *   LiteLLM
        *   Haystack
        *   LlamaIndex
        *   LangChain
        *   Groq
        *   Google Gemini
        *   And more (check their repository for the full list).

*   **OpenLLMetry (by Traceloop):** [https://github.com/traceloop/openllmetry](https://github.com/traceloop/openllmetry)
    *   This project also offers a comprehensive suite of instrumentors for LLM applications and related tools. Examples include:
        *   OpenAI
        *   CrewAI
        *   Haystack
        *   LangChain
        *   LlamaIndex
        *   Pinecone
        *   ChromaDB
        *   And more (explore their repository for details).

You can browse these repositories to find instrumentors for other libraries you might be using. If an instrumentor from these projects (or any other source) adheres to the `BaseInstrumentor` interface, you can integrate it with LangWatch using the `langwatch.setup(instrumentors=[...])` method described above.

### Key Considerations:

1.  **`BaseInstrumentor` Compliance:** Ensure the instrumentor correctly implements the `BaseInstrumentor` interface, particularly the `instrument()` and `uninstrument()` methods, and `instrumentation_dependencies()`.
2.  **Installation:** You'll need to have the custom instrumentor package installed in your Python environment, along with the library it instruments.
3.  **TracerProvider:** LangWatch configures an OpenTelemetry `TracerProvider`. The instrumentor, when activated by LangWatch, will use this provider to create spans. If you are managing your OpenTelemetry setup more directly (e.g., providing your own `TracerProvider` to `langwatch.setup()`), the instrumentor will use that instead.
4.  **Data Quality:** The quality and detail of the telemetry data captured will depend on how well the custom instrumentor is written.

By leveraging the `BaseInstrumentor` interface, LangWatch remains flexible and extensible, allowing you to bring telemetry from a wide array of Python libraries into your observability dashboard.

---

# FILE: ./integration/python/integrations/azure-ai.mdx

---
title: Azure AI Inference SDK Instrumentation
sidebarTitle: Python
description: Learn how to instrument the Azure AI Inference Python SDK with LangWatch.
icon: python
keywords: azure ai inference, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

The `azure-ai-inference` Python SDK provides a unified way to interact with various AI models deployed on Azure, including those on Azure OpenAI Service, GitHub Models, and Azure AI Foundry Serverless/Managed Compute endpoints. For more details on the SDK, refer to the [official Azure AI Inference client library documentation](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-inference-readme?view=azure-python-preview).

LangWatch can capture traces generated by the `azure-ai-inference` SDK by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install Azure AI Inference SDK with OpenTelemetry support**:
    The `azure-ai-inference` SDK can be installed with OpenTelemetry capabilities. You might also need the core Azure OpenTelemetry tracing package.
    ```bash
    pip install azure-ai-inference[opentelemetry] azure-core-tracing-opentelemetry
    ```
    Refer to the [Azure SDK documentation](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-inference-readme?view=azure-python-preview#install-the-package) for the most up-to-date installation instructions.

## Instrumentation with `AIInferenceInstrumentor`

The `azure-ai-inference` SDK provides an `AIInferenceInstrumentor` that automatically captures traces for its operations when enabled. LangWatch, when set up, will include an OpenTelemetry exporter that can collect these traces.

Here's how to instrument your application:

```python
import langwatch
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.tracing import AIInferenceInstrumentor
from azure.core.credentials import AzureKeyCredential
import os
import asyncio

# 1. Initialize LangWatch
langwatch.setup(
    instrumentors=[AIInferenceInstrumentor()]
)

# 2. Configure your Azure AI Inference client
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
azure_openai_api_key = os.getenv("AZURE_OPENAI_API_KEY")
azure_openai_api_version = "2024-06-01"

chat_client = ChatCompletionsClient(
    endpoint=azure_openai_endpoint,
    credential=AzureKeyCredential(azure_openai_api_key),
    api_version=azure_openai_api_version
)

@langwatch.trace(name="Azure AI Inference Chat")
async def get_ai_response(prompt: str):
    # This call will now be automatically traced by the AIInferenceInstrumentor and
    # captured by LangWatch as a span within the "Azure AI Inference Chat" trace.
    response = await chat_client.complete(
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

async def main():
    user_prompt = "What is the Azure AI Inference SDK?"

    try:
        ai_reply = await get_ai_response(user_prompt)
        print(f"User: {user_prompt}")
        print(f"AI: {ai_reply}")
    except Exception as e:
        print(f"An error occurred: {e}")


if __name__ == "__main__":
    asyncio.run(main())

```

<Note>
The example uses the synchronous `ChatCompletionsClient` for simplicity in demonstrating instrumentation. The `azure-ai-inference` SDK also provides asynchronous clients under the `azure.ai.inference.aio` namespace (e.g., `azure.ai.inference.aio.ChatCompletionsClient`). If you are using `async/await` in your application, you should use these asynchronous clients. The `AIInferenceInstrumentor` will work with both synchronous and asynchronous clients.
</Note>

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.
2.  `AIInferenceInstrumentor().instrument()`: This command, provided by the `azure-ai-inference` SDK, patches the relevant Azure AI clients (like `ChatCompletionsClient` or `EmbeddingsClient`) to automatically create OpenTelemetry spans for their operations (e.g., a `complete` or `embed` call).
3.  `@langwatch.trace()`: By decorating your own functions (like `get_ai_response` in the example), you create a parent trace in LangWatch. The spans automatically generated by the `AIInferenceInstrumentor` for calls made within this decorated function will then be nested under this parent trace. This provides a full end-to-end view of your operation.

With this setup, calls made using the `azure-ai-inference` clients will be automatically traced and sent to LangWatch, providing visibility into the performance and behavior of your AI model interactions.

---

# FILE: ./integration/python/integrations/vertex-ai.mdx

---
title: Google Vertex AI Instrumentation
sidebarTitle: Vertex AI
description: Learn how to instrument Google Vertex AI API calls with the LangWatch Python SDK using OpenInference
keywords: google vertex ai, gemini, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Google Vertex AI, allowing you to capture detailed information about your Vertex AI API calls automatically. The recommended approach is to use OpenInference instrumentation, which provides comprehensive tracing for Google Vertex AI API calls and integrates seamlessly with LangWatch.

## Using OpenInference Instrumentation

The recommended approach for instrumenting Google Vertex AI calls with LangWatch is to use the [OpenInference instrumentation library](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-vertexai), which provides comprehensive tracing for Google Vertex AI API calls.

### What OpenInference Captures

The OpenInference Vertex AI instrumentation automatically captures:

- **LLM Calls**: All text generation, chat completion, and embedding requests
- **Model Information**: Model name, version, and configuration parameters
- **Input/Output**: Prompts, responses, and token usage
- **Performance Metrics**: Latency, token counts, and cost information
- **Error Handling**: Failed requests and error details
- **Context Information**: Session IDs, user IDs, and custom metadata

## Installation and Setup

### Prerequisites

1. **Install the OpenInference Vertex AI instrumentor**:
   ```bash
   pip install openinference-instrumentation-vertexai
   ```

2. **Install LangWatch SDK**:
   ```bash
   pip install langwatch
   ```

3. **Set up your Google Cloud credentials**:
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/service-account-key.json"
   export GOOGLE_CLOUD_PROJECT="your-project-id"
   export GOOGLE_CLOUD_LOCATION="us-central1"
   ```

### Basic Setup

There are two main ways to integrate OpenInference Vertex AI instrumentation with LangWatch:

#### 1. Via `langwatch.setup()` (Recommended)

You can pass an instance of the `VertexAIInstrumentor` to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from vertexai.language_models import TextGenerationModel
import os

# Example using OpenInference's VertexAIInstrumentor
from openinference.instrumentation.vertexai import VertexAIInstrumentor

# Initialize LangWatch with the VertexAIInstrumentor
langwatch.setup(
    instrumentors=[VertexAIInstrumentor()]
)

# Initialize Vertex AI
from vertexai import init
init(project=os.getenv("GOOGLE_CLOUD_PROJECT"), location=os.getenv("GOOGLE_CLOUD_LOCATION"))

model = TextGenerationModel.from_pretrained("text-bison@001")

@langwatch.trace(name="Vertex AI Call with OpenInference")
def generate_text_with_openinference(prompt: str):
    # No need to call autotrack explicitly, the OpenInference instrumentor handles Vertex AI calls globally.
    response = model.predict(prompt)
    return response.text

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_openinference(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```

#### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application, you can use the instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from vertexai.language_models import TextGenerationModel
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.vertexai import VertexAIInstrumentor

langwatch.setup()

# Initialize Vertex AI
from vertexai import init
init(project=os.getenv("GOOGLE_CLOUD_PROJECT"), location=os.getenv("GOOGLE_CLOUD_LOCATION"))

model = TextGenerationModel.from_pretrained("text-bison@001")

# Instrument Vertex AI directly using the OpenInference library
VertexAIInstrumentor().instrument()

@langwatch.trace(name="Vertex AI Call with Direct OpenInference Instrumentation")
def get_story_ending(beginning: str):
    response = model.predict(
        f"You are a creative writer. Complete the story: {beginning}"
    )
    return response.text

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

<Note>
### Which Approach to Choose?

- **OpenInference Instrumentation** is recommended for most use cases as it provides comprehensive, automatic instrumentation with minimal setup
- **Direct OpenTelemetry Setup** is useful when you need fine-grained control over the tracing configuration or are already using OpenTelemetry extensively

Both approaches effectively send Vertex AI call data to LangWatch for monitoring and analysis.
</Note>
---

# FILE: ./integration/python/integrations/autogen.mdx

---
title: AutoGen Instrumentation
sidebarTitle: AutoGen
description: Learn how to instrument AutoGen applications with LangWatch.
keywords: autogen, python, sdk, instrumentation, opentelemetry, langwatch, tracing
---

AutoGen is a framework for building multi-agent systems with conversational AI. For more details on AutoGen, refer to the [official AutoGen documentation](https://microsoft.github.io/autogen/).

LangWatch can capture traces generated by AutoGen by leveraging its built-in OpenTelemetry support. This guide will show you how to set it up.

## Prerequisites

1.  **Install LangWatch SDK**:
    ```bash
    pip install langwatch
    ```

2.  **Install AutoGen and OpenInference instrumentor**:
    ```bash
    pip install pyautogen openinference-instrumentation-autogen
    ```

3.  **Set up your LLM provider**:
    You'll need to configure your preferred LLM provider (OpenAI, Anthropic, etc.) with the appropriate API keys.

## Instrumentation with OpenInference

LangWatch supports seamless observability for AutoGen using the [OpenInference AutoGen instrumentor](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-autogen). This approach automatically captures traces from your AutoGen agents and sends them to LangWatch.

### Basic Setup (Automatic Tracing)

Here's the simplest way to instrument your application:

```python
import langwatch
import autogen
from openinference.instrumentation.autogen import AutoGenInstrumentor
import os

# Initialize LangWatch with the AutoGen instrumentor
langwatch.setup(
    instrumentors=[AutoGenInstrumentor()]
)

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# Configure your agents
config_list = [
    {
        "model": "gpt-5",
        "api_key": os.environ["OPENAI_API_KEY"],
    }
]

# Create your agents
assistant = autogen.AssistantAgent(
    name="assistant",
    llm_config={"config_list": config_list},
    system_message="You are a helpful AI assistant."
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={"work_dir": "workspace"},
    llm_config={"config_list": config_list},
)

# Use the agents as usual—traces will be sent to LangWatch automatically
def run_agent_conversation(user_message: str):
    user_proxy.initiate_chat(
        assistant,
        message=user_message
    )
    return "Conversation completed"

# Example usage
if __name__ == "__main__":
    user_prompt = "Write a Python function to calculate fibonacci numbers"
    result = run_agent_conversation(user_prompt)
    print(f"Result: {result}")
```

**That's it!** All AutoGen agent interactions will now be traced and sent to your LangWatch dashboard automatically.

### Optional: Using Decorators for Additional Context

If you want to add additional context or metadata to your traces, you can optionally use the `@langwatch.trace()` decorator:

```python
import langwatch
import autogen
from openinference.instrumentation.autogen import AutoGenInstrumentor
import os

langwatch.setup(
    instrumentors=[AutoGenInstrumentor()]
)

# ... agent setup code ...

@langwatch.trace(name="AutoGen Multi-Agent Conversation")
def run_agent_conversation(user_message: str):
    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_count": 2,
                "model": "gpt-5"
            }
        )

    user_proxy.initiate_chat(
        assistant,
        message=user_message
    )
    return "Conversation completed"
```

## How it Works

1.  `langwatch.setup()`: Initializes the LangWatch SDK, which includes setting up an OpenTelemetry trace exporter. This exporter is ready to receive spans from any OpenTelemetry-instrumented library in your application.

2.  `AutoGenInstrumentor()`: The OpenInference instrumentor automatically patches AutoGen components to create OpenTelemetry spans for their operations, including:
    - Agent initialization
    - Multi-agent conversations
    - LLM calls
    - Tool executions
    - Code execution
    - Message passing between agents

3.  **Optional Decorators**: You can optionally use `@langwatch.trace()` to add additional context and metadata to your traces, but it's not required for basic functionality.

With this setup, all agent interactions, conversations, model calls, and tool executions will be automatically traced and sent to LangWatch, providing comprehensive visibility into your AutoGen-powered applications.

## Notes

- You do **not** need to set any OpenTelemetry environment variables or configure exporters manually—`langwatch.setup()` handles everything.
- You can combine AutoGen instrumentation with other instrumentors (e.g., OpenAI, LangChain) by adding them to the `instrumentors` list.
- The `@langwatch.trace()` decorator is **optional** - the OpenInference instrumentor will capture all AutoGen activity automatically.
- For advanced configuration (custom attributes, endpoint, etc.), see the [Python integration guide](/integration/python/guide).

## Troubleshooting

- Make sure your `LANGWATCH_API_KEY` is set in the environment.
- If you see no traces in LangWatch, check that the instrumentor is included in `langwatch.setup()` and that your agent code is being executed.
- Ensure you have the correct API keys set for your chosen LLM provider.

## Interoperability with LangWatch SDK

You can use this integration together with the LangWatch Python SDK to add additional attributes to the trace:

```python
import langwatch
import autogen
from openinference.instrumentation.autogen import AutoGenInstrumentor

langwatch.setup(
    instrumentors=[AutoGenInstrumentor()]
)

@langwatch.trace(name="Custom AutoGen Application")
def my_custom_autogen_app(input_message: str):
    # Your AutoGen code here
    config_list = [
        {
            "model": "gpt-5",
            "api_key": os.environ["OPENAI_API_KEY"],
        }
    ]

    assistant = autogen.AssistantAgent(
        name="assistant",
        llm_config={"config_list": config_list},
        system_message="You are a helpful AI assistant."
    )

    user_proxy = autogen.UserProxyAgent(
        name="user_proxy",
        human_input_mode="NEVER",
        max_consecutive_auto_reply=10,
        llm_config={"config_list": config_list},
    )

    # Update the current trace with additional metadata
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(
            metadata={
                "user_id": "user_123",
                "session_id": "session_abc",
                "agent_count": 2,
                "model": "gpt-5"
            }
        )

    # Run your agents
    user_proxy.initiate_chat(
        assistant,
        message=input_message
    )

    return "Conversation completed"
```

This approach allows you to combine the automatic tracing capabilities of AutoGen with the rich metadata and custom attributes provided by LangWatch.
---

# FILE: ./integration/python/integrations/open-ai.mdx

---
title: OpenAI Instrumentation
sidebarTitle: Python
description: Learn how to instrument OpenAI API calls with the LangWatch Python SDK
icon: python
keywords: openai, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with OpenAI, allowing you to capture detailed information about your LLM calls automatically. There are two primary approaches to instrumenting your OpenAI interactions:

1.  **Using `autotrack_openai_calls()`**: This method, part of the LangWatch SDK, dynamically patches your OpenAI client instance to capture calls made through it within a specific trace.
2.  **Using Community OpenTelemetry Instrumentors**: Leverage existing OpenTelemetry instrumentation libraries like those from OpenInference or OpenLLMetry. These can be integrated with LangWatch by either passing them to the `langwatch.setup()` function or by using their native `instrument()` methods if you're managing your OpenTelemetry setup more directly.

This guide will walk you through both methods.

## Using `autotrack_openai_calls()`

The `autotrack_openai_calls()` function provides a straightforward way to capture all OpenAI calls made with a specific client instance for the duration of the current trace.

You typically call this method on the trace object obtained via `langwatch.get_current_trace()` inside a function decorated with `@langwatch.trace()`.

```python
import langwatch
from openai import OpenAI

# Ensure LANGWATCH_API_KEY is set in your environment, or set it in `setup`
langwatch.setup()

# Initialize your OpenAI client
client = OpenAI()

@langwatch.trace(name="OpenAI Chat Completion")
async def get_openai_chat_response(user_prompt: str):
    # Get the current trace and enable autotracking for the 'client' instance
    langwatch.get_current_trace().autotrack_openai_calls(client)

    # All calls made with 'client' will now be automatically captured as spans
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[{"role": "user", "content": user_prompt}],
    )
    completion = response.choices[0].message.content
    return completion

async def main():
    user_query = "Tell me a joke about Python programming."
    response = await get_openai_chat_response(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

Key points for `autotrack_openai_calls()`:
-   It must be called on an active trace object (e.g., obtained via `langwatch.get_current_trace()`).
-   It instruments a *specific instance* of the OpenAI client. If you have multiple clients, you'll need to call it for each one you want to track.

## Using Community OpenTelemetry Instrumentors

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM providers, including OpenAI.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `OpenAIInstrumentor` from OpenInference or OpenLLMetry) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
from openai import OpenAI
import os

# Example using OpenInference's OpenAIInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

# Initialize LangWatch with the OpenAIInstrumentor
langwatch.setup(
    instrumentors=[OpenAIInstrumentor()]
)

client = OpenAI()

@langwatch.trace(name="OpenAI Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles OpenAI calls globally.
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```
<Note>
  Ensure you have the respective community instrumentation library installed (e.g., `pip install openllmetry-instrumentation-openai` or `pip install openinference-instrumentation-openai`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
from openai import OpenAI
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.openai import OpenAIInstrumentor

langwatch.setup()
client = OpenAI()

# Instrument OpenAI directly using the community library
OpenAIInstrumentor().instrument()

@langwatch.trace(name="OpenAI Call with Direct Community Instrumentation")
def get_story_ending(beginning: str):
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[
            {"role": "system", "content": "You are a creative writer. Complete the story."},
            {"role": "user", "content": beginning}
        ]
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

### Key points for community instrumentors:
-   These instrumentors often patch OpenAI at a global level, meaning all OpenAI calls from any client instance will be captured once instrumented.
-   If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
-   If instrumenting directly (e.g., `OpenAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

<Note>
### Which Approach to Choose?

-   **`autotrack_openai_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which OpenAI client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
-   **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture OpenAI calls globally across your application, or need to instrument other libraries alongside OpenAI with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send OpenAI call data to LangWatch for monitoring and analysis.
</Note>

---

# FILE: ./integration/python/tutorials/capturing-metadata.mdx

---
title: Capturing Metadata and Attributes
sidebarTitle: Python
description: Learn how to enrich your traces and spans with custom metadata and attributes using the LangWatch Python SDK.
icon: python
keywords: langwatch, python, metadata, attributes, tracing, spans, traces
---

Metadata and attributes are key-value pairs that allow you to add custom contextual information to your traces and spans. This enrichment is invaluable for debugging, analysis, filtering, and gaining deeper insights into your LLM application's behavior.

LangWatch distinguishes between two main types of custom data:

*   **Trace Metadata**: Information that applies to the entire lifecycle of a request or a complete operation.
*   **Span Attributes**: Information specific to a particular unit of work or step within a trace.

This tutorial will guide you through capturing both types using the Python SDK.

## Trace Metadata

Trace metadata provides context for the entire trace. It's ideal for information that remains constant throughout the execution of a traced operation, such as:

*   User identifiers (`user_id`)
*   Session or conversation identifiers (`session_id`, `thread_id`)
*   Application version (`app_version`)
*   Environment (`env: "production"`)
*   A/B testing flags or variant names

You can set trace metadata when a trace is initiated or update it at any point while the trace is active.

### Setting Trace Metadata at Initialization

The easiest way to add metadata to a trace is by passing a `metadata` dictionary to the `langwatch.trace()` decorator or context manager.

```python
import langwatch
import os

# Initialize LangWatch (ensure this is done once in your application)
langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

@langwatch.trace(name="UserQueryHandler", metadata={"user_id": "user_123", "session_id": "session_abc"})
def handle_user_query(query: str):
    # Your application logic here
    # For example, process the query and interact with an LLM
    processed_query = f"Query processed: {query}"

    # You can also update trace metadata from within the trace
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(metadata={"query_language": "en"})

    return processed_query

handle_user_query("Hello, LangWatch!")
```

In this example, `user_id` and `session_id` are attached to the "UserQueryHandler" trace from the start. Later, `query_language` is added.

<Note>
  Refer to the [`langwatch.trace()` API reference](/integration/python/reference#langwatchtrace) for more details on its parameters.
</Note>

### Updating Trace Metadata Dynamically

If you need to add or modify trace metadata after the trace has started (e.g., based on some intermediate result), you can use the `update()` method on the `LangWatchTrace` object.

You can get the current trace object using `langwatch.get_current_trace()` or from the `langwatch.trace()` context manager.

```python
import langwatch
import os
import uuid

langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

def process_customer_request(customer_id: str, request_details: dict):
    trace_metadata = {
        "customer_id": customer_id,
        "initial_request_type": request_details.get("type")
    }
    with langwatch.trace(name="CustomerRequestFlow", metadata=trace_metadata) as current_trace:
        # Simulate some processing
        print(f"Processing request for customer {customer_id}")

        # Update metadata based on some condition or result
        is_priority_customer = customer_id.startswith("vip_")
        current_trace.update(metadata={"priority_customer": is_priority_customer})

        # ... further processing ...

        if request_details.get("type") == "complaint":
            current_trace.update(metadata={"escalation_needed": True})

        print(f"Trace ID: {current_trace.id}") # Example of accessing trace properties

process_customer_request(f"vip_{uuid.uuid4()}", {"type": "complaint", "content": "Service issue"})
process_customer_request(f"user_{uuid.uuid4()}", {"type": "inquiry", "content": "Product question"})
```

## Span Attributes

Span attributes (often referred to simply as "attributes") provide context for a specific operation or unit of work *within* a trace. They are useful for details that are relevant only to that particular step. Examples include:

*   For an LLM call span: `model_name`, `prompt_template_version`, `temperature`
*   For a tool call span: `tool_name`, `api_endpoint`, specific input parameters
*   For a RAG span: `retrieved_document_ids`, `chunk_count`
*   Custom business logic flags or intermediate results specific to that span.

### Setting Span Attributes

You can set attributes on a span when it's created using the `attributes` parameter (less common for dynamic values) or, more typically, by calling the `update()` method on the `LangWatchSpan` object.

The `update()` method is flexible and allows you to pass attributes as keyword arguments.

<CodeGroup>
```python Using update() method
import langwatch
import os

langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

@langwatch.trace(name="ArticleGenerator")
def generate_article(topic: str):
    with langwatch.span(name="FetchResearchData", type="tool") as research_span:
        # Simulate fetching data
        research_data = f"Data about {topic}"
        research_span.update(
            source="internal_db",
            query_complexity="medium",
            items_retrieved=10
        )
        # research_span.set_attributes({"source": "internal_db"}) # Also works

    with langwatch.span(name="GenerateText", type="llm") as llm_span:
        llm_span.update(model="gpt-5", prompt_length=len(topic))
        # Simulate LLM call
        article_text = f"Here's an article about {topic} based on {research_data}."
        llm_span.update(output_length=len(article_text), tokens_used=150)

    return article_text

generate_article("AI in Healthcare")
```
```python Using attributes parameter (for static attributes)
import langwatch
import os

langwatch.setup(api_key=os.getenv("LANGWATCH_API_KEY"))

@langwatch.trace(name="StaticAttributeDemo")
def process_with_static_info():
    # 'version' is a static attribute for this span's definition
    with langwatch.span(name="ComponentX", attributes={"version": "1.0.2", "region": "us-east-1"}):
        # ... logic for ComponentX ...
        print("ComponentX with static attributes executed.")
        # Dynamic attributes can still be added via update()
        langwatch.get_current_span().update(status_code=200)

process_with_static_info()
```
</CodeGroup>

In the first example, `source`, `query_complexity`, and `items_retrieved` are added to the "FetchResearchData" span. Similarly, `model`, `prompt_length`, `output_length`, and `tokens_used` contextualize the "GenerateText" LLM span.

The `LangWatchSpan` object also has a `set_attributes()` method which takes a dictionary, similar to OpenTelemetry's underlying span API.

<Note>
  For more details on span parameters and methods, see the [`langwatch.span()` API reference](/integration/python/reference#langwatchspan) and the [`LangWatchSpan` object methods](/integration/python/reference#langwatchspan-object-methods).
</Note>

## Key Differences: Trace Metadata vs. Span Attributes

| Feature         | Trace Metadata                                  | Span Attributes                                        |
|-----------------|-------------------------------------------------|--------------------------------------------------------|
| **Scope**       | Entire trace (e.g., a whole user request)       | Specific span (e.g., one LLM call, one tool use)       |
| **Granularity** | Coarse-grained, applies to the overall operation | Fine-grained, applies to a specific part of the operation |
| **Purpose**     | General context for the entire operation        | Specific details about a particular step or action     |
| **Examples**    | `user_id`, `session_id`, `app_version`          | `model_name`, `tool_parameters`, `retrieved_chunk_id`    |
| **SDK Access**  | `langwatch.trace(metadata={...})` <br/> `trace.update(metadata={...})` | `langwatch.span(attributes={...})` <br/> `span.update(key=value, ...)` <br/> `span.set_attributes({...})` |

**When to use which:**

*   Use **Trace Metadata** for information that you'd want to associate with every single span within that trace, or that defines the overarching context of the request (e.g., who initiated it, what version of the service is running).
*   Use **Span Attributes** for details specific to the execution of that particular span. This helps in understanding the parameters, behavior, and outcome of individual components within your trace.

## Viewing in LangWatch

All captured trace metadata and span attributes will be visible in the LangWatch UI.
- **Trace Metadata** is typically displayed in the trace details view, providing an overview of the entire operation.
- **Span Attributes** are shown when you inspect individual spans within a trace.

This rich contextual data allows you to:
- **Filter and search** for traces and spans based on specific metadata or attribute values.
- **Analyze performance** by correlating metrics with different metadata/attributes (e.g., comparing latencies for different `user_id`s or `model_name`s).
- **Debug issues** by quickly understanding the context and parameters of a failed or slow operation.

## Conclusion

Effectively using trace metadata and span attributes is crucial for maximizing the observability of your LLM applications. By enriching your traces with relevant contextual information, you empower yourself to better understand, debug, and optimize your systems with LangWatch.

Remember to instrument your code thoughtfully, adding data that provides meaningful insights without being overly verbose.

---

# FILE: ./integration/python/tutorials/capturing-evaluations-guardrails.mdx

---
title: Capturing Evaluations & Guardrails
sidebarTitle: Evaluations & Guardrails
description: Learn how to log custom evaluations, trigger managed evaluations, and implement guardrails with LangWatch.
keywords: Custom Evaluations, Managed Evaluations, Guardrails, LangWatch Evaluations, add_evaluation, evaluate, async_evaluate, Evaluation Metric, Evaluation Score, Evaluation Pass/Fail, Evaluation Label, Evaluation Details, Evaluation Cost, Evaluation Status, Evaluation Error, Evaluation Timestamps, Evaluation Type, Guardrail
---

LangWatch provides a flexible system for capturing various types of evaluations and implementing guardrails within your LLM applications. This allows you to track performance, ensure quality, and control application flow based on defined criteria.

There are three main ways to work with evaluations and guardrails:

1.  **Client-Side Custom Evaluations (`add_evaluation`)**: Log any custom evaluation metric, human feedback, or external system score directly from your Python code. These are primarily for observational purposes.
2.  **Server-Side Managed Evaluations (`evaluate`, `async_evaluate`)**: Trigger predefined or custom evaluation logic that runs on the LangWatch backend. These can return scores, pass/fail results, and other details.
3.  **Guardrails**: A special application of evaluations (either client-side or server-side) used to make decisions or enforce policies within your application flow.

## 1. Client-Side Custom Evaluations (`add_evaluation`)

You can log custom evaluation data directly from your application code using the `add_evaluation()` method on a `LangWatchSpan` or `LangWatchTrace` object. This is useful for recording metrics specific to your domain, results from external systems, or human feedback.

When you call `add_evaluation()`, LangWatch typically creates a new child span of type `evaluation` (or `guardrail` if `is_guardrail=True`) under the target span. This child span, named after your custom evaluation, stores its details, primarily in its `output` attribute.

Here's an example:

```python
import langwatch

# Assume langwatch.setup() has been called

@langwatch.span(name="Generate Response")
def process_request(user_query: str):
    response_text = f"Response to: {user_query}"
    langwatch.get_current_span().update(output=response_text)

    # Example 1: A simple pass/fail custom evaluation
    contains_keyword = "LangWatch" in response_text
    langwatch.get_current_span().add_evaluation(
        name="Keyword Check: LangWatch",
        passed=contains_keyword,
        details=f"Checked for 'LangWatch'. Found: {contains_keyword}"
    )

    # Example 2: A custom score for response quality
    human_score = 4.5
    langwatch.get_current_span().add_evaluation(
        name="Human Review: Quality Score",
        score=human_score,
        label="Good",
        details="Reviewed by Jane Doe. Response is clear and relevant."
    )

    # Example 3: A client-side guardrail check
    is_safe = not ("unsafe_word" in response_text)
    langwatch.get_current_span().add_evaluation(
        name="Safety Check (Client-Side)",
        passed=is_safe,
        is_guardrail=True, # Mark this as a guardrail
        details=f"Content safety check. Passed: {is_safe}"
    )
    if not is_safe:
        # Potentially alter flow or log a critical warning
        print("Warning: Client-side safety check failed!")


    return response_text

@langwatch.trace(name="Process User Request")
def main():
    user_question = "Tell me about LangWatch."
    generated_response = process_request(user_question)
    print(f"Query: {user_question}")
    print(f"Response: {generated_response}")

if __name__ == "__main__":
    main()
```

### `add_evaluation()` Parameters

The `add_evaluation()` method is available on both `LangWatchSpan` and `LangWatchTrace` objects (when using on a trace, you must specify the target `span`). For detailed parameter descriptions, please refer to the API reference:

- [`LangWatchSpan.add_evaluation()`](/integration/python/reference#add_evaluation-1)
- [`LangWatchTrace.add_evaluation()`](/integration/python/reference#add_evaluation)

## 2. Server-Side Managed Evaluations (`evaluate` & `async_evaluate`)

LangWatch allows you to trigger evaluations that are performed by the LangWatch backend. These can be [built-in evaluators](/llm-evaluation/list) (e.g., for faithfulness, relevance) or [custom evaluators you define](/evaluations/custom-evaluator-integration) in your LangWatch project settings.

You use the `evaluate()` (synchronous) or `async_evaluate()` (asynchronous) functions for this. These functions send the necessary data to the LangWatch API, which then processes the evaluation. These server-side evaluations are a core part of setting up [real-time monitoring and evaluations in production](/llm-evaluation/realtime/setup).

```python
import langwatch
from langwatch.evaluations import BasicEvaluateData
# from langwatch.types import RAGChunk # For RAG contexts

# Assume langwatch.setup() has been called

@langwatch.span()
def handle_rag_query(user_query: str):
    retrieved_contexts_str = [
        "LangWatch helps monitor LLM applications.",
        "Evaluations can be run on the server."
    ]
    # For richer context, use RAGChunk
    # retrieved_contexts_rag = [
    #     RAGChunk(content="LangWatch helps monitor LLM applications.", document_id="doc1"),
    #     RAGChunk(content="Evaluations can be run on the server.", document_id="doc2")
    # ]

    # Add the RAG contexts to the current span
    langwatch.get_current_span().update(contexts=retrieved_contexts_str)

    # Simulate LLM call
    llm_output = f"Based on the context, LangWatch is for monitoring and server-side evals."

    # Prepare data for server-side evaluation
    eval_data = BasicEvaluateData(
        input=user_query,
        output=llm_output,
        contexts=retrieved_contexts_str
    )

    # Trigger a server-side "faithfulness" evaluation
    # The 'faithfulness-evaluator' slug must be configured in your LangWatch project
    try:
        faithfulness_result = langwatch.evaluate(
            slug="faithfulness-evaluator", # Slug of the evaluator in LangWatch
            name="Faithfulness Check (Server)",
            data=eval_data,
        )

        print(f"Faithfulness Evaluation Result: {faithfulness_result}")
        # faithfulness_result is an EvaluationResultModel(status, passed, score, details, etc.)

        # Example: Using it as a guardrail
        if faithfulness_result.passed is False:
            print("Warning: Faithfulness check failed!")

    except Exception as e:
        print(f"Error during server-side evaluation: {e}")

    return llm_output

@langwatch.trace()
def main():
    query = "What can LangWatch do with contexts?"
    response = handle_rag_query(query)
    print(f"Query: {query}")
    print(f"Response: {response}")

if __name__ == "__main__":
    main()
```

### `evaluate()` / `async_evaluate()` Key Parameters

The `evaluate()` and `async_evaluate()` methods are available on both `LangWatchSpan` and `LangWatchTrace` objects. They can also be imported from `langwatch.evaluations` and called as `langwatch.evaluate()` or `langwatch.async_evaluate()`, where you would then explicitly pass the `span` or `trace` argument. For detailed parameter descriptions, refer to the API reference:

- [`LangWatchSpan.evaluate()`](/integration/python/reference#evaluate-1) and [`LangWatchSpan.async_evaluate()`](/integration/python/reference#async_evaluate-1)
- [`LangWatchTrace.evaluate()`](/integration/python/reference#evaluate) and [`LangWatchTrace.async_evaluate()`](/integration/python/reference#async_evaluate)

<Tip>
  **Understanding the `data` Parameter:**

  The core parameters like `slug`, `data`, `settings`, `as_guardrail`, `span`, and `trace` are generally consistent.
  For the `data` parameter specifically: while `BasicEvaluateData` is commonly used to provide a standardized structure for `input`, `output`, and `contexts` (which many built-in or common evaluators expect), it's important to know that `data` can be **any dictionary**. This flexibility allows you to pass arbitrary data structures tailored to custom server-side evaluators you might define. Using `BasicEvaluateData` with fields like `expected_output` is particularly useful when [evaluating if the LLM is generating the right answers](/llm-evaluation/offline/platform/answer-correctness) against a set of expected outputs. For scenarios where a golden answer isn't available, LangWatch also supports more open-ended evaluations, such as using an [LLM-as-a-judge](/llm-evaluation/offline/platform/llm-as-a-judge).
</Tip>

The `slug` parameter refers to the unique identifier of the evaluator configured in your LangWatch project settings. You can find a list of available evaluator types and learn how to configure them in our [LLM Evaluation documentation](/llm-evaluation/list).

The functions return an `EvaluationResultModel` containing `status`, `passed`, `score`, `details`, `label`, and `cost`.

## 3. Guardrails

Guardrails are evaluations used to make decisions or enforce policies within your application. They typically result in a boolean `passed` status that your code can act upon.

**Using Server-Side Evaluations as Guardrails:**
Set `as_guardrail=True` when calling `evaluate` or `async_evaluate`.

```python
# ... (inside a function with a current span)
eval_data = BasicEvaluateData(output=llm_response)
pii_check_result = langwatch.evaluate(
    slug="pii-detection-guardrail",
    data=eval_data,
    as_guardrail=True,
    span=langwatch.get_current_span()
)

if pii_check_result.passed is False:
    # Take action: sanitize response, return a canned message, etc.
    return "Response redacted due to PII."
```
A key behavior of `as_guardrail=True` for server-side evaluations is that if the *evaluation process itself* encounters an error (e.g., the evaluator service is down), the result will have `status="error"` but `passed` will default to `True`. This is a fail-safe to prevent your application from breaking due to an issue in the guardrail execution itself, assuming a "pass by default on error" stance is desired. For more on setting up safety-focused real-time evaluations like PII detection or prompt injection monitors, see our guide on [Setting up Real-Time Evaluations](/llm-evaluation/realtime/setup).

**Using Client-Side `add_evaluation` as Guardrails:**
Set `is_guardrail=True` when calling `add_evaluation`.

```python
# ... (inside a function with a current span)
is_too_long = len(llm_response) > 1000
response_span.add_evaluation(
    name="Length Guardrail",
    passed=(not is_too_long),
    is_guardrail=True,
    details=f"Length: {len(llm_response)}. Max: 1000"
)
if is_too_long:
    # Take action: truncate response, ask for shorter output, etc.
    return llm_response[:1000] + "..."
```
For client-side guardrails added with `add_evaluation`, your code is fully responsible for interpreting the `passed` status and handling any errors during the local check.

## How Evaluations and Guardrails Appear in LangWatch

Both client-side and server-side evaluations (including those marked as guardrails) are logged as spans in LangWatch.
- `add_evaluation`: Creates a child span of type `evaluation` (or `guardrail` if `is_guardrail=True`).
- `evaluate`/`async_evaluate`: Also create a child span of type `evaluation` (or `guardrail` if `as_guardrail=True`).

These spans will contain the evaluation's name, result (score, passed, label), details, cost, and any associated metadata, typically within their `output` attribute. This allows you to:
- See a history of all evaluation outcomes.
- Filter traces by evaluation results.
- Analyze the performance of different evaluators or guardrails.
- Correlate evaluation outcomes with other trace data (e.g., LLM inputs/outputs, latencies).

## Use Cases

- **Quality Assurance**:
    - **Client-Side**: Log scores from a custom heuristic checking for politeness in responses.
    - **Server-Side**: Trigger a managed ["Toxicity" evaluator](/llm-evaluation/list) on LLM outputs, or use more open-ended approaches like an [LLM-as-a-judge](/llm-evaluation/offline/platform/llm-as-a-judge) for tasks without predefined correct answers.
- **Compliance & Safety**:
    - **Client-Side Guardrail**: Perform a regex check for forbidden words and log it with `is_guardrail=True`.
    - **Server-Side Guardrail**: Use a managed ["PII Detection" evaluator](/llm-evaluation/list) with `as_guardrail=True` to decide if a response can be shown.
- **Performance Monitoring**:
    - **Client-Side**: Log human feedback scores (`add_evaluation`) for helpfulness.
    - **Server-Side**: Evaluate RAG system outputs for ["Context Relevancy" and "Faithfulness"](/llm-evaluation/list) using managed evaluators.
- **A/B Testing**: Log custom metrics or trigger standard evaluations for different model versions or prompts to compare their performance.
- **Feedback Integration**: `add_evaluation` can be used to pipe scores from an external human review platform directly into the relevant trace.

By combining these methods, you can build a robust evaluation and guardrailing strategy tailored to your application's needs, all observable within LangWatch.

---

# FILE: ./integration/python/tutorials/capturing-rag.mdx

---
title: Capturing RAG
sidebarTitle: Capturing RAG
description: Learn how to capture Retrieval Augmented Generation (RAG) data with LangWatch.
icon: python
keywords: RAG, Retrieval Augmented Generation, LangChain, LangWatch, LangChain RAG, RAG Span, RAG Chunk, RAG Tool
---

Retrieval Augmented Generation (RAG) is a common pattern in LLM applications where you first retrieve relevant context from a knowledge base and then use that context to generate a response. LangWatch provides specific ways to capture RAG data, enabling better observability and evaluation of your RAG pipelines.

By capturing the `contexts` (retrieved documents) used by the LLM, you unlock several benefits in LangWatch:
- Specialized RAG evaluators (e.g., Faithfulness, Context Relevancy).
- Analytics on document usage (e.g., which documents are retrieved most often, which ones lead to better responses).
- Deeper insights into the retrieval step of your pipeline.

There are two main ways to capture RAG spans: manually creating a RAG span or using framework-specific integrations like the one for LangChain.

## Manual RAG Span Creation

You can manually create a RAG span by decorating a function with `@langwatch.span(type="rag")`. Inside this function, you should perform the retrieval and then update the span with the retrieved contexts.

The `contexts` should be a list of strings or `RAGChunk` objects. The `RAGChunk` object allows you to provide more metadata about each retrieved chunk, such as `document_id` and `source`.

Here's an example:

```python
import langwatch
import time # For simulating work

# Assume langwatch.setup() has been called elsewhere

@langwatch.span(type="llm")
def generate_answer_from_context(contexts: list[str], user_query: str):
    # Simulate LLM call using the contexts
    time.sleep(0.5)
    response = f"Based on the context, the answer to '{user_query}' is..."
    # You can update the LLM span with model details, token counts, etc.
    langwatch.get_current_span().update(
        model="gpt-5",
        prompt=f"Contexts: {contexts}\nQuery: {user_query}",
        completion=response
    )
    return response

@langwatch.span(type="rag", name="My Custom RAG Process")
def perform_rag(user_query: str):
    # 1. Retrieve contexts
    # Simulate retrieval from a vector store or other source
    time.sleep(0.3)
    retrieved_docs = [
        "LangWatch helps monitor LLM applications.",
        "RAG combines retrieval with generation for better answers.",
        "Python is a popular language for AI development."
    ]

    # Update the current RAG span with the retrieved contexts
    # You can pass a list of strings directly
    langwatch.get_current_span().update(contexts=retrieved_docs)

    # Alternatively, for richer context information:
    # from langwatch.types import RAGChunk
    # rag_chunks = [
    #     RAGChunk(content="LangWatch helps monitor LLM applications.", document_id="doc1", source="internal_wiki/langwatch"),
    #     RAGChunk(content="RAG combines retrieval with generation for better answers.", document_id="doc2", source="blog/rag_explained")
    # ]
    # langwatch.get_current_span().update(contexts=rag_chunks)

    # 2. Generate answer using the contexts
    final_answer = generate_answer_from_context(contexts=retrieved_docs, user_query=user_query)

    # The RAG span automatically captures its input (user_query) and output (final_answer)
    # if capture_input and capture_output are not set to False.
    return final_answer

@langwatch.trace(name="User Question Handler")
def handle_user_question(question: str):
    langwatch.get_current_trace().update(
        input=question,
        metadata={"user_id": "example_user_123"}
    )

    answer = perform_rag(user_query=question)

    langwatch.get_current_trace().update(output=answer)
    return answer

if __name__ == "__main__":
    user_question = "What is LangWatch used for?"
    response = handle_user_question(user_question)
    print(f"Question: {user_question}")
    print(f"Answer: {response}")

```

In this example:
1.  `perform_rag` is decorated with `@langwatch.span(type="rag")`.
2.  Inside `perform_rag`, we simulate a retrieval step.
3.  `langwatch.get_current_span().update(contexts=retrieved_docs)` is called to explicitly log the retrieved documents.
4.  The generation step (`generate_answer_from_context`) is called, which itself can be another span (e.g., an LLM span).

## LangChain RAG Integration

If you are using LangChain, LangWatch provides utilities to simplify capturing RAG data from retrievers and tools.

### Capturing RAG from a Retriever

You can wrap your LangChain retriever with `langwatch.langchain.capture_rag_from_retriever`. This function takes your retriever and a lambda function to transform the retrieved `Document` objects into `RAGChunk` objects.

```python
import langwatch
from langwatch.types import RAGChunk

from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores.faiss import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable.config import RunnableConfig

# 1. Setup LangWatch (if not done globally)
# langwatch.setup()

# 2. Prepare your retriever
loader = WebBaseLoader("https://docs.langwatch.ai/introduction") # Example source
docs = loader.load()
documents = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=200
).split_documents(docs)
vector = FAISS.from_documents(documents, OpenAIEmbeddings())
retriever = vector.as_retriever()

# 3. Wrap the retriever for LangWatch RAG capture
# This lambda tells LangWatch how to extract data for RAGChunk from LangChain's Document
langwatch_retriever_tool = create_retriever_tool(
    langwatch.langchain.capture_rag_from_retriever(
        retriever,
        lambda document: RAGChunk(
            document_id=document.metadata.get("source", "unknown_source"), # Use a fallback for source
            content=document.page_content,
            # You can add other fields like 'score' if available in document.metadata
        ),
    ),
    "langwatch_docs_search", # Tool name
    "Search for information about LangWatch.", # Tool description
)

# 4. Use the wrapped retriever in your agent/chain
tools = [langwatch_retriever_tool]
model = ChatOpenAI(model="gpt-5", streaming=True)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant. Answer questions based on the retrieved context.\n{agent_scratchpad}"),
        ("human", "{question}"),
    ]
)
agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # type: ignore

@langwatch.trace(name="LangChain RAG Agent Execution")
def run_langchain_rag(user_input: str):
    current_trace = langwatch.get_current_trace()
    current_trace.update(metadata={"user_id": "lc_rag_user"})

    # Ensure the LangChain callback is used to capture all LangChain steps
    response = agent_executor.invoke(
        {"question": user_input},
        config=RunnableConfig(
            callbacks=[current_trace.get_langchain_callback()]
        ),
    )

    output = response.get("output", "No output found.")=
    return output

if __name__ == "__main__":
    question = "What is LangWatch?"
    answer = run_langchain_rag(question)
    print(f"Question: {question}")
    print(f"Answer: {answer}")
```

#### Key elements
- `langwatch.langchain.capture_rag_from_retriever(retriever, lambda document: ...)`: This wraps your existing retriever.
- The lambda function `lambda document: RAGChunk(...)` defines how to map fields from LangChain's `Document` to LangWatch's `RAGChunk`. This is crucial for providing detailed context information.
- The wrapped retriever is then used to create a tool, which is subsequently used in an agent or chain.
- Remember to include `langwatch.get_current_trace().get_langchain_callback()` in your `RunnableConfig` when invoking the chain/agent to capture all LangChain operations.

### Capturing RAG from a Tool

Alternatively, if your RAG mechanism is encapsulated within a generic LangChain `BaseTool`, you can use `langwatch.langchain.capture_rag_from_tool`.

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.trace()
def main():
    my_custom_tool = ...
    wrapped_tool = langwatch.langchain.capture_rag_from_tool(
        my_custom_tool, lambda response: [
          RAGChunk(
            document_id=response["id"], # optional
            chunk_id=response["chunk_id"], # optional
            content=response["content"]
          )
        ]
    )

    tools = [wrapped_tool] # use the new wrapped tool in your agent instead of the original one
    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful assistant that only reply in short tweet-like responses, using lots of emojis and use tools only once.\n\n{agent_scratchpad}",
            ),
            ("human", "{question}"),
        ]
    )
    agent = create_tool_calling_agent(model, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return executor.invoke(user_input, config=RunnableConfig(
        callbacks=[langWatchCallback]
    ))
```
The `capture_rag_from_tool` approach is generally less direct for RAG from retrievers because you have to parse the tool's output (which is usually a string) to extract structured context information. `capture_rag_from_retriever` is preferred when dealing directly with LangChain retrievers.

By effectively capturing RAG spans, you gain much richer data in LangWatch, enabling more powerful analysis and evaluation of your RAG systems. Refer to the SDK examples for more detailed implementations.

---

# FILE: ./integration/python/tutorials/capturing-mapping-input-output.mdx

---
title: Capturing and Mapping Inputs & Outputs
sidebarTitle: Python
icon: python
description: Learn how to control the capture and structure of input and output data for traces and spans with the LangWatch Python SDK.
keywords: langwatch, python, input, output, capture, mapping, data, tracing, spans, observability
---

Effectively capturing the inputs and outputs of your LLM application's operations is crucial for observability. LangWatch provides flexible ways to manage this data, whether you prefer automatic capture or explicit control to map complex objects, format data, or redact sensitive information.

This tutorial covers how to:
*   Understand automatic input/output capture.
*   Explicitly set inputs and outputs for traces and spans.
*   Dynamically update this data on active traces/spans.
*   Handle different data formats, especially for chat messages.

## Automatic Input and Output Capture

By default, when you use `@langwatch.trace()` or `@langwatch.span()` as decorators on functions, the SDK attempts to automatically capture:

*   **Inputs**: The arguments passed to the decorated function.
*   **Outputs**: The value returned by the decorated function.

This behavior can be controlled using the `capture_input` and `capture_output` boolean parameters.

```python
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="GreetUser", capture_input=True, capture_output=True)
def greet_user(name: str, greeting: str = "Hello"):
    # 'name' and 'greeting' will be captured as input.
    # The returned string will be captured as output.
    return f"{greeting}, {name}!"

greet_user("Alice")

@langwatch.span(name="SensitiveOperation", capture_input=False, capture_output=False)
def process_sensitive_data(data: dict):
    # Inputs and outputs for this span will not be automatically captured.
    # You might explicitly set a sanitized version if needed.
    print("Processing sensitive data...")
    return {"status": "processed"}

@langwatch.trace(name="MainFlow")
def main_flow():
    greet_user("Bob", greeting="Hi")
    process_sensitive_data({"secret": "data"})

main_flow()
```

<Note>
  Refer to the API reference for [`@langwatch.trace()`](/integration/python/reference#%40langwatch-trace-%2F-langwatch-trace) and [`@langwatch.span()`](/integration/python/reference#%40langwatch-span-%2F-langwatch-span) for more details on `capture_input` and `capture_output` parameters.
</Note>

## Explicitly Setting Inputs and Outputs

You often need more control over what data is recorded. You can explicitly set inputs and outputs using the `input` and `output` parameters when initiating a trace or span, or by using the `update()` method on the respective objects.

This is useful for:
*   Capturing only specific parts of complex objects.
*   Formatting data in a more readable or structured way (e.g., as a list of `ChatMessage` objects).
*   Redacting sensitive information before it's sent to LangWatch.
*   Providing inputs/outputs when not using decorators (e.g., with context managers for parts of a function).

### At Initialization

When using `@langwatch.trace()` or `@langwatch.span()` (either as decorators or context managers), you can pass `input` and `output` arguments.

<CodeGroup>
```python Trace with explicit input/output
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(
    name="UserIntentProcessing",
    input={"user_query": "Book a flight to London"},
    # Output can be set later via update() if determined by function logic
)
def process_user_intent(raw_query_data: dict):
    # raw_query_data might be large or contain sensitive info
    # The 'input' parameter above provides a clean version.
    intent = "book_flight"
    entities = {"destination": "London"}

    # Explicitly set the output for the root span of the trace
    current_trace = langwatch.get_current_trace()
    if current_trace:
        current_trace.update(output={"intent": intent, "entities": entities})

    return {"status": "success", "intent": intent} # Actual function return

process_user_intent({"query": "Book a flight to London", "user_id": "123"})
```

```python Span with explicit input/output
import langwatch
import os
from langwatch.domain import ChatMessage

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="ChatbotInteraction")
def handle_chat():
    user_message = ChatMessage(role="user", content="What is LangWatch?")

    with langwatch.span(
        name="LLMCall",
        type="llm",
        input=[user_message],
        model="gpt-5"
    ) as llm_span:
        # Simulate LLM call
        assistant_response_content = "LangWatch helps you monitor your LLM applications."
        assistant_message = ChatMessage(role="assistant", content=assistant_response_content)

        # Set output on the span object
        llm_span.update(output=[assistant_message])

    print("Chat finished.")

handle_chat()
```
</CodeGroup>

If you provide `input` or `output` directly, it overrides what might have been automatically captured for that field.

### Dynamically Updating Inputs and Outputs

You can modify the input or output of an active trace or span using its `update()` method. This is particularly useful when the input/output data is determined or refined during the operation.

```python
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="DataTransformationPipeline")
def run_pipeline(initial_data: dict):
    # Initial input is automatically captured if capture_input=True (default)

    with langwatch.span(name="Step1_CleanData") as step1_span:
        # Suppose initial_data is complex, we want to record a summary as input
        step1_span.update(input={"data_keys": list(initial_data.keys())})
        cleaned_data = {k: v for k, v in initial_data.items() if v is not None}
        step1_span.update(output={"cleaned_item_count": len(cleaned_data)})

    # ... further steps ...

    # Update the root span's output for the entire trace
    final_result = {"status": "completed", "items_processed": len(cleaned_data)}
    langwatch.get_current_trace().update(output=final_result)

    return final_result

run_pipeline({"a": 1, "b": None, "c": 3})
```

<Note>
  The `update()` method on `LangWatchTrace` and `LangWatchSpan` objects is versatile. See the reference for [`LangWatchTrace` methods](/integration/python/reference#%40langwatch-trace-%2F-langwatch-trace) and [`LangWatchSpan` methods](/integration/python/reference#%40langwatch-span-%2F-langwatch-span).
</Note>

## Handling Different Data Formats

LangWatch can store various types of input and output data:

*   **Strings**: Simple text.
*   **Dictionaries**: Automatically serialized as JSON. This is useful for structured data.
*   **Lists of `ChatMessage` objects**: The standard way to represent conversations for LLM interactions. This ensures proper display and analysis in the LangWatch UI.

### Capturing Chat Messages

For LLM interactions, structure your inputs and outputs as a list of `ChatMessage` objects.

```python
import langwatch
import os
from langwatch.domain import ChatMessage, ToolCall, FunctionCall # For more complex messages

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="AdvancedChat")
def advanced_chat_example():
    messages = [
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="What is the weather in London?")
    ]

    with langwatch.span(name="GetWeatherToolCall", type="llm", input=messages, model="gpt-5") as llm_span:
        # Simulate model deciding to call a tool
        tool_call_id = "call_abc123"
        assistant_response_with_tool = ChatMessage(
            role="assistant",
            tool_calls=[
                ToolCall(
                    id=tool_call_id,
                    type="function",
                    function=FunctionCall(name="get_weather", arguments='''{"location": "London"}''')
                )
            ]
        )
        llm_span.update(output=[assistant_response_with_tool])

    # Simulate tool execution
    with langwatch.span(name="RunGetWeatherTool", type="tool") as tool_span:
        tool_input = {"tool_name": "get_weather", "arguments": {"location": "London"}}
        tool_span.update(input=tool_input)

        tool_result_content = '''{"temperature": "15C", "condition": "Cloudy"}'''
        tool_span.update(output=tool_result_content)

        # Prepare message for next LLM call
        tool_response_message = ChatMessage(
            role="tool",
            tool_call_id=tool_call_id,
            name="get_weather",
            content=tool_result_content
        )
        messages.append(assistant_response_with_tool) # Assistant's decision to call tool
        messages.append(tool_response_message)      # Tool's response

    with langwatch.span(name="FinalLLMResponse", type="llm", input=messages, model="gpt-5") as final_llm_span:
        final_assistant_content = "The weather in London is 15°C and cloudy."
        final_assistant_message = ChatMessage(role="assistant", content=final_assistant_content)
        final_llm_span.update(output=[final_assistant_message])

advanced_chat_example()
```

<Note>
  For the detailed structure of `ChatMessage`, `ToolCall`, and other related types, please refer to the [Core Data Types section in the API Reference](/integration/python/reference#core-data-types).
</Note>

## Use Cases and Best Practices

*   **Redacting Sensitive Information**: If your function arguments or return values contain sensitive data (PII, API keys), disable automatic capture (`capture_input=False`, `capture_output=False`) and explicitly set sanitized versions using `input`/`output` parameters or `update()`.
*   **Mapping Complex Objects**: If your inputs/outputs are complex Python objects, map them to a dictionary or a simplified string representation for clearer display in LangWatch.
*   **Improving Readability**: For long text inputs/outputs (e.g., full documents), consider capturing a summary or metadata instead of the entire content to reduce noise, unless the full content is essential for debugging or evaluating.
*   **Clearing Captured Data**: You can set `input=None` or `output=None` via the `update()` method to remove previously captured (or auto-captured) data if it's no longer relevant or was captured in error.

```python
import langwatch
import os

# Assume we have already setup LangWatch
# langwatch.setup()

@langwatch.trace(name="DataRedactionExample")
def handle_user_data(user_profile: dict):
    # user_profile might contain PII
    # Automatic capture is on by default.
    # Let's update the input to a redacted version for the root span.

    redacted_input = {
        "user_id": user_profile.get("id"),
        "has_email": "email" in user_profile
    }
    langwatch.get_current_trace().update(input=redacted_input)

    # Process data...
    result = {"status": "processed", "user_id": user_profile.get("id")}
    langwatch.get_current_trace().update(output=result)
    return result # Actual function return can still be the full data

handle_user_data({"id": "user_xyz", "email": "test@example.com", "name": "Sensitive Name"})
```

## Conclusion

Controlling how inputs and outputs are captured in LangWatch allows you to tailor the observability data to your specific needs. By using automatic capture flags, explicit parameters, dynamic updates, and appropriate data formatting (especially `ChatMessage` for conversations), you can ensure that your traces provide clear, relevant, and secure insights into your LLM application's behavior.

---

# FILE: ./integration/python/tutorials/manual-instrumentation.mdx

---
title: Manual Instrumentation
description: Learn how to manually instrument your code with the LangWatch Python SDK
keywords: manual instrumentation, context managers, span, trace, async, synchronous, LangWatch, Python
---

While decorators offer a concise way to instrument functions, you might prefer or need to manually manage trace and span lifecycles. This is useful in asynchronous contexts, for finer control, or when decorators are inconvenient. The LangWatch Python SDK provides two primary ways to do this manually:

### Using Context Managers (`with`/`async with`)

The `langwatch.trace()` and `langwatch.span()` functions can be used directly as asynchronous (`async with`) or synchronous (`with`) context managers. This is the recommended approach for manual instrumentation as it automatically handles ending the trace/span, even if errors occur.

Here's how you can achieve the same instrumentation as the decorator examples, but using context managers:

```python
import langwatch
from langwatch.types import RAGChunk
from langwatch.attributes import AttributeKey # For semantic attribute keys
import asyncio # Assuming async operation

langwatch.setup()

async def rag_retrieval_manual(query: str):
    # Use async with for the span, instead of a decorator
    async with langwatch.span(type="rag", name="RAG Document Retrieval") as span:
        # ... your async retrieval logic ...
        await asyncio.sleep(0.05) # Simulate async work
        search_results = [
            {"id": "doc-1", "content": "Content for doc 1."},
            {"id": "doc-2", "content": "Content for doc 2."},
        ]

        # Update the span with input, context, metadata, and output
        span.update(
            input=query,
            contexts=[
                RAGChunk(document_id=doc["id"], content=doc["content"])
                for doc in search_results
            ],
            output=search_results,
            strategy="manual_vector_search"
        )
        return search_results

async def handle_user_query_manual(query: str):
    # Use async with for the trace
    async with langwatch.trace(name="Manual User Query Handling", metadata={"user_id": "manual-user", "query": query}) as trace:
        # Call the manually instrumented RAG function
        retrieved_docs = await rag_retrieval_manual(query)

        # --- Simulate LLM Call Step (manual span) ---
        llm_response = ""
        async with langwatch.span(type="llm", name="Manual LLM Generation") as llm_span:
            llm_input = {"role": "user", "content": f"Context: {retrieved_docs}\nQuery: {query}"}
            llm_metadata = {"model_name": "gpt-5"}

            # ... your async LLM call logic ...
            await asyncio.sleep(0.1)
            llm_response = "This is the manual LLM response."
            llm_output = {"role": "assistant", "content": llm_response}

            # Set input, metadata and output via update
            llm_span.update(
                input=llm_input,
                output=llm_output
                llm_metadata=llm_metadata,
            )

        # Set final trace output via update
        trace.update(output=llm_response)
        return llm_response

# Example execution (in an async context)
async def main():
    result = await handle_user_query_manual("Tell me about manual tracing with context managers.")
    print(result)
asyncio.run(main())
```

Key points for manual instrumentation with context managers:

- Use `with langwatch.trace(...)` or `async with langwatch.trace(...)` to start a trace.
- Use `with langwatch.span(...)` or `async with langwatch.span(...)` inside a trace block to create nested spans.
- The trace or span object is available in the `as trace:` or `as span:` part of the `with` statement.
- Use methods like `span.add_event()`, and primarily `span.update(...)` / `trace.update(...)` to add details. The `update()` method is flexible for adding structured data like `input`, `output`, `metadata`, and `contexts`.
- This approach gives explicit control over the start and end of each instrumented block, as the context manager handles ending the span automatically.

### Direct Span Creation (`span.end()`)

Alternatively, you can manage span and trace lifecycles completely manually. Call `langwatch.span()` or `langwatch.trace()` directly to start them, and then explicitly call the `end()` method on the returned object (`span.end()` or `trace.end()`) when the operation finishes. **This requires careful handling to ensure `end()` is always called, even if errors occur (e.g., using `try...finally`).** Context managers are generally preferred as they handle this automatically.

```python
import langwatch
import time

# Assume langwatch.setup() and a trace context exist

def process_data_manually(data):
    span = langwatch.span(name="Manual Data Processing") # Start the span
    try:
        span.update(input=data)
        # ... synchronous processing logic ...
        time.sleep(0.02)
        result = f"Processed: {data}"
        span.update(output=result)
        return result
    except Exception as e:
        span.record_exception(e) # Record exceptions
        span.set_status("error", description=str(e))
        raise # Re-raise the exception
    finally:
        span.end() # CRITICAL: Ensure the span is ended

# with langwatch.trace(): # Needs to be within a trace
#     processed = process_data_manually("some data")
```

---

# FILE: ./integration/python/tutorials/open-telemetry.mdx

---
title: OpenTelemetry Migration
description: Learn how to integrate the LangWatch Python SDK with your existing OpenTelemetry setup.
keywords: OpenTelemetry, OTel, auto-instrumentation, OpenAI, Celery, HTTP clients, databases, ORMs, LangWatch, Python
---

The LangWatch Python SDK is built entirely on top of the robust [OpenTelemetry (OTel)](https://opentelemetry.io/) standard. This means seamless integration with existing OTel setups and interoperability with the wider OTel ecosystem.

## LangWatch Spans are OpenTelemetry Spans

It's important to understand that LangWatch traces and spans **are** standard OpenTelemetry traces and spans. LangWatch adds specific semantic attributes (like `langwatch.span.type`, `langwatch.inputs`, `langwatch.outputs`, `langwatch.metadata`) to these standard spans to power its observability features.

This foundation provides several benefits:
- **Interoperability:** Traces generated with LangWatch can be sent to any OTel-compatible backend (Jaeger, Tempo, Datadog, etc.) alongside your other application traces.
- **Familiar API:** If you're already familiar with OpenTelemetry concepts and APIs, working with LangWatch's manual instrumentation will feel natural.
- **Leverage Existing Setup:** LangWatch integrates smoothly with your existing OTel `TracerProvider` and instrumentation.

Perhaps the most significant advantage is that **LangWatch seamlessly integrates with the vast ecosystem of standard OpenTelemetry auto-instrumentation libraries.** This means you can easily combine LangWatch's LLM-specific observability with insights from other parts of your application stack. For example, if you use `opentelemetry-instrumentation-celery`, traces initiated by LangWatch for an LLM task can automatically include spans generated within your Celery workers, giving you a complete end-to-end view of the request, including background processing, without any extra configuration.

## Leverage the OpenTelemetry Ecosystem: Auto-Instrumentation

One of the most powerful benefits of LangWatch's OpenTelemetry foundation is its **automatic compatibility with the extensive ecosystem of OpenTelemetry auto-instrumentation libraries.**

When you use standard OTel auto-instrumentation for libraries like web frameworks, databases, or task queues alongside LangWatch, you gain **complete end-to-end visibility** into your LLM application's requests. Because LangWatch and these auto-instrumentors use the same underlying OpenTelemetry tracing system and context propagation mechanisms, spans generated across different parts of your application are automatically linked together into a single, unified trace.

This means you don't need to manually stitch together observability data from your LLM interactions and the surrounding infrastructure. If LangWatch instruments an LLM call, and that call involves fetching data via an instrumented database client or triggering a background task via an instrumented queue, all those operations will appear as connected spans within the same trace view in LangWatch (and any other OTel backend you use).

### Examples of Auto-Instrumentation Integration

Here are common scenarios where combining LangWatch with OTel auto-instrumentation provides significant value:

*   **Web Frameworks (FastAPI, Flask, Django):** Using libraries like `opentelemetry-instrumentation-fastapi`, an incoming HTTP request automatically starts a trace. When your request handler calls a function instrumented with `@langwatch.trace` or `@langwatch.span`, those LangWatch spans become children of the incoming request span. You see the full request lifecycle, from web server entry to LLM processing and response generation.

*   **HTTP Clients (Requests, httpx, aiohttp):** If your LLM application makes outbound API calls (e.g., to fetch external data, call a vector database API, or use a non-instrumented LLM provider via REST) using libraries instrumented by `opentelemetry-instrumentation-requests` or similar, these HTTP request spans will automatically appear within your LangWatch trace, showing the latency and success/failure of these external dependencies.

*   **Task Queues (Celery, RQ):** When a request handled by your web server (and traced by LangWatch) enqueues a background job using `opentelemetry-instrumentation-celery`, the trace context is automatically propagated. The spans generated by the Celery worker processing that job will be linked to the original LangWatch trace, giving you visibility into asynchronous operations triggered by your LLM pipeline.

*   **Databases & ORMs (SQLAlchemy, Psycopg2, Django ORM):** Using libraries like `opentelemetry-instrumentation-sqlalchemy`, any database queries executed during your LLM processing (e.g., for RAG retrieval, user data lookup, logging results) will appear as spans within the relevant LangWatch trace, pinpointing database interaction time and specific queries.

To enable this, simply ensure you have installed and configured the relevant OpenTelemetry auto-instrumentation libraries according to their documentation, typically involving an installation (`pip install opentelemetry-instrumentation-<library>`) and sometimes an initialization step (like `CeleryInstrumentor().instrument()`). As long as they use the same (or the global) `TracerProvider` that LangWatch is configured with, the integration is automatic.

#### Example: Combining LangWatch, RAG, OpenAI, and Celery

Let's illustrate this with a simplified example involving a web request that performs RAG, calls OpenAI, and triggers a background Celery task.

<CodeGroup>

```txt requirements.txt
langwatch
openai
celery
opentelemetry-instrumentation-celery
```

```python example.py
import langwatch
import os
import asyncio
from celery import Celery
from openai import OpenAI
from langwatch.types import RAGChunk

# 1. Configure Celery App
celery_app = Celery('tasks', broker=os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0'))

# 2. Setup LangWatch and OpenTelemetry Instrumentation
from opentelemetry_instrumentation.celery import CeleryInstrumentor
CeleryInstrumentor().instrument()

# Now setup LangWatch (it will likely pick up the global provider configured by Celery)
langwatch.setup(
    # If you have other OTel exporters, configure your TracerProvider manually
    # and pass it via tracer_provider=..., setting ignore_warning=True
    ignore_global_tracer_provider_override_warning=True
)

client = OpenAI()

# 3. Define the Celery Task
@celery_app.task
def process_result_background(result_id: str, llm_output: str):
    # This task execution will be automatically linked to the trace
    # that enqueued it, thanks to CeleryInstrumentor.
    # Spans created here (e.g., database writes) would be part of the same trace.
    print(f"[Celery Worker] Processing result {result_id}...")
    # Simulate work
    import time
    time.sleep(1)
    print(f"[Celery Worker] Finished processing {result_id}")
    return f"Processed: {llm_output[:10]}..."

# 4. Define RAG and Main Processing Logic
@langwatch.span(type="rag")
def retrieve_documents(query: str) -> list:
    # Simulate RAG retrieval
    print(f"Retrieving documents for: {query}")
    chunks = [
        RAGChunk(document_id="doc-abc", content="LangWatch uses OpenTelemetry."),
        RAGChunk(document_id="doc-def", content="Celery integrates with OpenTelemetry."),
    ]
    langwatch.get_current_span().update(contexts=chunks)
    time.sleep(0.1)
    return [c.content for c in chunks]

@langwatch.trace(name="Handle User Query with Celery")
def handle_request(user_query: str):
    # This is the root span for the request
    langwatch.get_current_trace().autotrack_openai_calls(client)
    langwatch.get_current_trace().update(metadata={"user_query": user_query})

    context_docs = retrieve_documents(user_query)

    try:
        completion = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": f"Use this context: {context_docs}"},
                {"role": "user", "content": user_query}
            ],
            temperature=0.5,
        )
        llm_result = completion.choices[0].message.content
    except Exception as e:
        langwatch.get_current_trace().record_exception(e)
        llm_result = "Error calling OpenAI"

    result_id = f"res_{int(time.time())}"
    # The current trace context is automatically propagated
    process_result_background.delay(result_id, llm_result)
    print(f"Enqueued background processing task {result_id}")

    return llm_result

# 5. Simulate Triggering the Request
if __name__ == "__main__":
    print("Simulating web request...")
    final_answer = handle_request("How does LangWatch work with Celery?")
    print(f"\nFinal Answer returned to user: {final_answer}")
    # Allow time for task to be processed if running worker locally
    time.sleep(3) # Add a small delay to see Celery output

    # To run this example:
    # 1. Start a Celery worker: celery -A your_module_name worker --loglevel=info
    # 2. Run this Python script.
    # 3. Observe the logs and the trace in LangWatch/OTel backend.
```

</CodeGroup>

In this example:
- The `handle_request` function is the main trace.
- `retrieve_documents` is a child span created by LangWatch.
- The OpenAI call creates child spans (due to `autotrack_openai_calls`).
- The call to `process_result_background.delay` creates a span indicating the task was enqueued.
- Critically, `CeleryInstrumentor` automatically propagates the trace context, so when the Celery worker picks up the `process_result_background` task, its execution is linked as a child span (or spans, if the task itself creates more) under the original `handle_request` trace.

This gives you a unified view of the entire operation, from the initial request through LLM processing, RAG, and background task execution.

## Integrating with `langwatch.setup()`

When you call `langwatch.setup()`, it intelligently interacts with your existing OpenTelemetry environment:

1.  **Checks for Existing `TracerProvider`:**
    - If you provide a `TracerProvider` instance via the `tracer_provider` argument in `langwatch.setup()`, LangWatch will use that specific provider.
    - If you *don't* provide one, LangWatch checks if a global `TracerProvider` has already been set (e.g., by another library or your own OTel setup code).
    - If neither is found, LangWatch creates a new `TracerProvider`.

2.  **Adding the LangWatch Exporter:**
    - If LangWatch uses an *existing* `TracerProvider` (either provided via the argument or detected globally), it will **add its own OTLP Span Exporter** to that provider's list of Span Processors. It does *not* remove existing processors or exporters.
    - If LangWatch creates a *new* `TracerProvider`, it configures it with the LangWatch OTLP Span Exporter.

## Default Behavior: All Spans Go to LangWatch

A crucial point is that once `langwatch.setup()` runs and attaches its exporter to a `TracerProvider`, **all spans** managed by that provider will be exported to the LangWatch backend by default. This includes:
- Spans created using `@langwatch.trace` and `@langwatch.span`.
- Spans created manually using `langwatch.trace()` or `langwatch.span()` as context managers or via `span.end()`.
- Spans generated by standard OpenTelemetry auto-instrumentation libraries (e.g., `opentelemetry-instrumentation-requests`, `opentelemetry-instrumentation-fastapi`) if they are configured to use the same `TracerProvider`.
- Spans you create directly using the OpenTelemetry API (`tracer.start_as_current_span(...)`).

While seeing all application traces can be useful, you might not want *every single span* sent to LangWatch, especially high-volume or low-value ones (like health checks or database pings).

## Selectively Exporting Spans with `span_exclude_rules`

To control which spans are sent to LangWatch, use the `span_exclude_rules` argument during `langwatch.setup()`. This allows you to define rules to filter spans *before* they are exported to LangWatch, without affecting other exporters attached to the same `TracerProvider`.

Rules are defined using `SpanProcessingExcludeRule` objects.

```python
import langwatch
import os
from langwatch.domain import SpanProcessingExcludeRule
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

# Example: You already have an OTel setup exporting to console
existing_provider = TracerProvider()
existing_provider.add_span_processor(
    SimpleSpanProcessor(ConsoleSpanExporter())
)

# Define rules to prevent specific spans from going to LangWatch
# (They will still go to the Console exporter)
exclude_rules = [
    # Exclude spans exactly named "GET /health_check"
    SpanProcessingExcludeRule(
        field_name="span_name",
        match_value="GET /health_check",
        match_operation="exact_match"
    ),
    # Exclude spans where 'http.method' attribute is 'OPTIONS'
    SpanProcessingExcludeRule(
        field_name="attribute",
        attribute_name="http.method",
        match_value="OPTIONS",
        match_operation="exact_match"
    ),
    # Exclude spans whose names start with "Internal."
    SpanProcessingExcludeRule(
        field_name="span_name",
        match_value="Internal.",
        match_operation="starts_with"
    ),
]

# Setup LangWatch to use the existing provider and apply exclude rules
langwatch.setup(
    api_key=os.getenv("LANGWATCH_API_KEY"),
    tracer_provider=existing_provider, # Use our existing provider
    span_exclude_rules=exclude_rules,
    # Important: Set this if you intend for LangWatch to use the existing provider
    # and want to silence the warning about not overriding it.
    ignore_global_tracer_provider_override_warning=True
)

# Now, create some spans using OTel API directly
tracer = existing_provider.get_tracer("my.app.tracer")

with tracer.start_as_current_span("GET /health_check") as span:
    span.set_attribute("http.method", "GET")
    # This span WILL go to Console Exporter
    # This span WILL NOT go to LangWatch Exporter

with tracer.start_as_current_span("Process User Request") as span:
    span.set_attribute("http.method", "POST")
    span.set_attribute("user.id", "user-123")
    # This span WILL go to Console Exporter
    # This span WILL ALSO go to LangWatch Exporter
```

Refer to the `SpanProcessingExcludeRule` definition for all available fields (`span_name`, `attribute`, `library_name`) and operations (`exact_match`, `contains`, `starts_with`, `ends_with`, `regex`).

## Debugging with Console Exporter

When developing or troubleshooting your OpenTelemetry integration, it's often helpful to see the spans being generated locally without sending them to a backend. The OpenTelemetry SDK provides a `ConsoleSpanExporter` for this purpose.

You can add it to your `TracerProvider` like this:

<CodeGroup>

```python Scenario 1: Managed Provider (Recommended)
import langwatch
import os
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

# Create your own TracerProvider
my_tracer_provider = TracerProvider()

# Add the ConsoleSpanExporter for debugging
my_tracer_provider.add_span_processor(
    SimpleSpanProcessor(ConsoleSpanExporter())
)

# Now, setup LangWatch with your pre-configured provider
langwatch.setup(
    tracer_provider=my_tracer_provider,
    # If you are providing your own tracer_provider that might be global,
    # you might want to set this to True if you see warnings.
    # ignore_global_tracer_provider_override_warning=True
)

# Spans created via LangWatch or directly via OTel API using this provider
# will now also be printed to the console.

# Example of creating a span to test
tracer = my_tracer_provider.get_tracer("my.debug.tracer")
with tracer.start_as_current_span("My Test Span"):
    print("This span should appear in the console.")
```

```python Scenario 2: Global Provider (Illustrative)
# Ensure necessary imports if running this snippet standalone
import os
import langwatch
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider # Needed for isinstance check
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

# In this case, you might try to get the global provider and add the exporter.
# Note: This can be less predictable if other libraries also manipulate the global provider.

langwatch.setup(
    ignore_global_tracer_provider_override_warning=True # If a global provider exists
)

# Try to get the globally configured TracerProvider
global_provider = trace.get_tracer_provider()

# Check if it's an SDK TracerProvider instance that we can add a processor to
if isinstance(global_provider, TracerProvider):
    global_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))

# Example span after attempting to modify global provider
# Note: get_tracer from the global trace module
global_otel_tracer = trace.get_tracer("my.app.tracer.global")

with global_otel_tracer.start_as_current_span("Test Span with Global Provider"):
    print("This span should appear in console if global provider was successfully modified.")
```

</CodeGroup>

This will print all created spans to your console

## Accessing the OpenTelemetry Span API

Since LangWatch spans wrap standard OTel spans, the `LangWatchSpan` object (returned by `langwatch.span()` or accessed via `langwatch.get_current_span()`) directly exposes the standard OpenTelemetry `trace.Span` API methods. This allows you to interact with the span using familiar OTel functions when needed for advanced use cases or compatibility.

You don't need to access a separate underlying object; just call the standard OTel methods directly on the `LangWatchSpan` instance:

```python
import langwatch
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

langwatch.setup() # Assume setup is done

with langwatch.span(name="MyInitialSpanName") as span:

    # Use standard OpenTelemetry Span API methods directly on span:
    span.set_attribute("my.custom.otel.attribute", "value")
    span.add_event("Specific OTel Event", {"detail": "more info"})
    span.set_status(Status(StatusCode.ERROR, description="Something went wrong"))
    span.update_name("MyUpdatedSpanName") # Renaming the span

    print(f"Is Recording? {span.is_recording()}")
    print(f"OTel Span Context: {span.get_span_context()}")

    # You can still use LangWatch-specific methods like update()
    span.update(langwatch_info="extra data")
```

This allows full flexibility, letting you use both LangWatch's structured data methods (`update`, etc.) and the standard OpenTelemetry span manipulation methods on the same object.

## Understanding `ignore_global_tracer_provider_override_warning`

If `langwatch.setup()` detects an existing *global* `TracerProvider` (one set via `opentelemetry.trace.set_tracer_provider()`) and you haven't explicitly passed a `tracer_provider` argument, LangWatch will log a warning by default. The warning states that it found a global provider and will attach its exporter to it rather than replacing it.

This warning exists because replacing a globally configured provider can sometimes break assumptions made by other parts of your application or libraries. However, in many cases, **attaching** the LangWatch exporter to the existing global provider is exactly the desired behavior.

If you are intentionally running LangWatch alongside an existing global OpenTelemetry setup and want LangWatch to simply add its exporter to that setup, you can silence this warning by setting:

```python
langwatch.setup(
    # ... other options
    ignore_global_tracer_provider_override_warning=True
)
```

---

# FILE: ./integration/python/tutorials/tracking-llm-costs.mdx

---
title: Tracking LLM Costs and Tokens
sidebarTitle: Python
description: Troubleshooting & adjusting cost tracking in LangWatch
icon: python
keywords: LangWatch, cost tracking, token counting, debugging, troubleshooting, model costs, metrics, LLM spans
---

By default, LangWatch will automatically capture cost and token data for your LLM calls.

<img
  src="/images/costs/llm-costs-analytics.png"
  alt="LLM costs analytics graph"
/>

If you don't see costs being tracked or you see it being tracked as $0, this guide will help you identify and fix issues when cost and token tracking is not working as expected.

## Understanding Cost and Token Tracking

LangWatch calculates costs and tracks tokens by:

1. **Capturing model names** in LLM spans to match against cost tables
2. **Recording token metrics** (`prompt_tokens`, `completion_tokens`) in span data, or estimating when not available
3. **Mapping models to costs** using the pricing table in Settings > Model Costs

When any of these components are missing, you might see missing or $0 costs and tokens.

## Step 1: Verify LLM Span Data Capture

The most common issue is that your LLM spans aren't capturing the required data: model name, inputs, outputs, and token metrics.

### Check Your Current Spans

First, examine what data is being captured in your LLM spans. In the LangWatch dashboard:

1. Navigate to a trace that should have cost/token data
2. Click on the LLM span to inspect its details
3. Look for these key fields:
   - **Model**: Should show the model identifier (e.g., `openai/gpt-5`)
   - **Input/Output**: Should contain the actual messages sent and received
   - **Metrics**: Should show prompt + completion tokens

<img
  src="/images/costs/llm-span-details.png"
  alt="LLM span showing model, input/output, and token metrics"
/>

## Step 2: Fix Missing Model Information

If your spans don't show model information, the integration framework you're using might not be capturing it automatically.

### Solution A: Use Framework Auto-tracking

LangWatch provides auto-tracking for popular frameworks that automatically captures all the necessary data for cost calculation.

Check the **Integrations** menu in the sidebar to find specific setup instructions for your framework, which will show you how to properly configure automatic model and token tracking.

### Solution B: Manually Set Model Information

If auto-tracking isn't available for your framework, manually update the span with model information:

```python
import langwatch

# Mark the span as an LLM type span
@langwatch.span(type="llm")
def custom_llm_call(prompt: str):
    # Update the current span with model information
    langwatch.get_current_span().update(
        model="openai/gpt-5",  # Use the exact model identifier
        input=prompt,
    )

    # Simulate an LLM response
    response = your_custom_llm_client.generate(prompt)

    # Update with output and metrics
    langwatch.get_current_span().update(
        output=response.text,
        metrics={
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
        }
    )

    return response.text

@langwatch.trace()
def main_handler():
    result = custom_llm_call("Tell me about LangWatch")
    return result
```

### Solution C: Direct OpenTelemetry Integration (without LangWatch SDK)

If you're using a framework with built-in OpenTelemetry integration or community instrumentors, they should be following the [GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/). However, if the integration isn't capturing model information or token counts correctly, you can wrap your LLM calls with a custom span to patch the missing data:

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

def my_llm_call_with_tracking(prompt):
    with tracer.start_as_current_span("llm_call_wrapper") as span:
        # Set the required attributes for cost calculation
        span.set_attribute("gen_ai.operation.name", "chat")
        span.set_attribute("gen_ai.request.model", "gpt-5")

        # Your existing LLM call (may create its own spans)
        response = your_framework_llm_client.generate(prompt)

        # Extract and set token information if available
        if hasattr(response, 'usage'):
            span.set_attribute("gen_ai.usage.input_tokens", response.usage.input_tokens)
            span.set_attribute("gen_ai.usage.completion_tokens", response.usage.completion_tokens)

        return response
```

## Step 3: Configure Model Cost Mapping

If your model information is being captured but costs still show $0, you need to configure the cost mapping.

### Check Existing Model Costs

1. Go to **Settings > Model Costs** in your LangWatch dashboard
2. Look for your model in the list
3. Check if the regex pattern matches your model identifier

<img
  src="/images/costs/model-costs-settings.webp"
  alt="Model Costs settings page showing cost configuration"
/>

### Add Custom Model Costs

If your model isn't in the cost table, add it:

1. Click **"Add New Model"** in Settings > Model Costs
2. Configure the model entry:
   - **Model Name**: Descriptive name (e.g., "gpt-5")
   - **Regex Match Rule**: Pattern to match your model identifier (e.g., `^gpt-5$`)
   - **Input Cost**: Cost per input token (e.g., `0.0000004`)
   - **Output Cost**: Cost per output token (e.g., `0.0000016`)

### Common Model Identifier Patterns

Make sure your regex patterns match how the model names appear in your spans:

| Framework    | Model Identifier Format | Regex Pattern          |
| ------------ | ----------------------- | ---------------------- |
| OpenAI SDK   | `gpt-5`           | `^gpt-5$`        |
| Azure OpenAI | `gpt-5`           | `^gpt-5$`        |
| LangChain    | `openai/gpt-5`    | `^openai/gpt-5$` |
| Custom       | `my-custom-model-v1`    | `^my-custom-model-v1$` |


### Verification Checklist

After running your test, verify in the LangWatch dashboard:

✅ **Trace appears** in the dashboard \
✅ **LLM span shows model name** (e.g., `gpt-5`) \
✅ **Input and output are captured** \
✅ **Token metrics are present** (`prompt_tokens`, `completion_tokens`) \
✅ **Cost is calculated and displayed** (non-zero value)

## Common Issues and Solutions

### Issue: Auto-tracking not working

**Symptoms**: Spans appear but without model/metrics data

**Solutions**:

- Ensure `autotrack_*()` is called on an active trace
- Check that the client instance being tracked is the same one making calls
- Verify the integration is initialized correctly

### Issue: Custom models not calculating costs

**Symptoms**: Model name appears but cost remains $0

**Solutions**:

- Check regex pattern in Model Costs settings
- Ensure the pattern exactly matches your model identifier
- Verify input and output costs are configured correctly

### Issue: Token counts are 0 but model is captured

**Symptoms**: Model name is present but token metrics are missing

**Solutions**:

- Manually set metrics in span updates if not automatically captured
- Check if your LLM provider returns usage information
- Ensure the integration is extracting token counts from responses

### Issue: Framework with OpenTelemetry not capturing model data

**Symptoms**: Using a framework with OpenTelemetry integration that's not capturing model names or token counts

**Solutions**:
- Follow the guidance in [Solution C: Framework with OpenTelemetry Integration](#solution-c-framework-with-opentelemetry-integration) above
- Wrap your LLM calls with custom spans to patch missing data


## Getting Help

If you're still experiencing issues after following this guide:

1. **Check the LangWatch logs** for any error messages
2. **Verify your API key** and endpoint configuration
3. **Share a minimal reproduction** with the specific framework you're using
4. **Contact support** at [support@langwatch.ai](mailto:support@langwatch.ai) with:
   - Your integration method (SDK, OpenTelemetry, etc.)
   - Framework versions
   - Sample span data from the dashboard

Cost and token tracking should work reliably once the model information and metrics are properly captured. Most issues stem from missing model identifiers or incorrect cost table configuration.

---

# FILE: ./integration/typescript/reference.mdx

---
title: TypeScript SDK API Reference
sidebarTitle: Reference
description: LangWatch TypeScript SDK API reference
keywords: langwatch, typescript, sdk, api, reference, observability, tracing, logging, data capture, data collection, data ingestion
icon: terminal
---

## Setup

### `setupObservability()`

Initializes the LangWatch observability system for Node.js environments, enabling data collection and tracing for your LLM application. This is typically the first function you'll call when integrating LangWatch.

<CodeGroup>
```typescript Basic Setup
import { setupObservability } from "langwatch/observability/node";

setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    endpoint: process.env.LANGWATCH_ENDPOINT_URL
  },
  serviceName: "my-service"
});
```

```typescript Advanced Setup
import { setupObservability } from "langwatch/observability/node";
import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";

setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    endpoint: process.env.LANGWATCH_ENDPOINT_URL,
    processorType: "batch" // or "simple"
  },
  serviceName: "my-service",
  attributes: {
    "service.version": "1.0.0",
    "deployment.environment.name": "production"
  },
  dataCapture: "all", // or "input", "output", "none"
  debug: {
    consoleTracing: true,
    logLevel: "debug"
  }
});
```

```typescript Custom Configuration
import { setupObservability } from "langwatch/observability/node";
import { SpanProcessor } from "@opentelemetry/sdk-trace-base";

const customSpanProcessor: SpanProcessor = /* your custom processor */;

setupObservability({
  langwatch: "disabled", // Disable LangWatch integration
  spanProcessors: [customSpanProcessor],
  dataCapture: {
    mode: "all"
  },
  debug: {
    consoleTracing: true,
    logger: customLogger
  }
});
```
</CodeGroup>

<ParamField path="options" type="SetupObservabilityOptions" default="{}">
  Configuration options for the LangWatch observability system.
</ParamField>

**Returns**

<ResponseField name="handle" type="ObservabilityHandle">
  An object containing a `shutdown()` method for graceful cleanup.
</ResponseField>

### `SetupObservabilityOptions`

Configuration options for setting up LangWatch observability.

<ResponseField name="langwatch" type="Optional<LangWatchConfig | 'disabled'>">
  LangWatch configuration. Set to 'disabled' to completely disable LangWatch integration.
</ResponseField>

<ResponseField name="serviceName" type="Optional<string>">
  Name of the service being instrumented.
</ResponseField>

<ResponseField name="attributes" type="Optional<SemConvAttributes>">
  Global attributes added to all telemetry data.
</ResponseField>

<ResponseField name="dataCapture" type="Optional<DataCaptureOptions>">
  Configuration for automatic data capture. Can be "all", "input", "output", "none", or a configuration object.
</ResponseField>

<ResponseField name="spanProcessors" type="Optional<SpanProcessor[]>">
  Custom span processors for advanced trace processing.
</ResponseField>

<ResponseField name="debug" type="Optional<DebugOptions>">
  Debug and development options.
</ResponseField>

<ResponseField name="advanced" type="Optional<AdvancedOptions>">
  Advanced and potentially unsafe configuration options.
</ResponseField>

### `LangWatchConfig`

Configuration for LangWatch integration.

<ResponseField name="apiKey" type="Optional<string>" default="process.env.LANGWATCH_API_KEY">
  LangWatch API key for authentication.
</ResponseField>

<ResponseField name="endpoint" type="Optional<string>" default="https://app.langwatch.ai">
  LangWatch endpoint URL for sending traces and logs.
</ResponseField>

<ResponseField name="processorType" type="Optional<'simple' | 'batch'>" default="'simple'">
  Type of span processor to use for LangWatch exporter.
</ResponseField>

### `DebugOptions`

Debug and development options.

<ResponseField name="consoleTracing" type="Optional<boolean>" default="false">
  Enable console output for traces (debugging).
</ResponseField>

<ResponseField name="consoleLogging" type="Optional<boolean>" default="false">
  Enable console output for logs (debugging).
</ResponseField>

<ResponseField name="logLevel" type="Optional<'debug' | 'info' | 'warn' | 'error'>" default="'warn'">
  Log level for LangWatch SDK internal logging.
</ResponseField>

<ResponseField name="logger" type="Optional<Logger>">
  Custom logger for LangWatch SDK internal logging.
</ResponseField>

### `ObservabilityHandle`

Handle returned from observability setup.

<ResponseField name="shutdown" type="() => Promise<void>">
  Gracefully shuts down the observability system.
</ResponseField>

## Tracing

### `getLangWatchTracer()`

Returns a LangWatch tracer instance that provides enhanced tracing capabilities for LLM applications.

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service", "1.0.0");
```

<ParamField path="name" type="string" required>
  The name of the tracer/service.
</ParamField>

<ParamField path="version" type="Optional<string>" default="undefined">
  The version of the tracer/service.
</ParamField>

**Returns**

<ResponseField name="tracer" type="LangWatchTracer">
  A `LangWatchTracer` instance with enhanced methods for LLM observability.
</ResponseField>

### `getLangWatchTracerFromProvider()`

Get a LangWatch tracer from a specific OpenTelemetry tracer provider.

```typescript
import { getLangWatchTracerFromProvider } from "langwatch/observability";

const tracer = getLangWatchTracerFromProvider(
  customTracerProvider,
  "my-service",
  "1.0.0"
);
```

<ParamField path="tracerProvider" type="TracerProvider" required>
  The OpenTelemetry tracer provider to use.
</ParamField>

<ParamField path="name" type="string" required>
  The name of the tracer/service.
</ParamField>

<ParamField path="version" type="Optional<string>" default="undefined">
  The version of the tracer/service.
</ParamField>

### `LangWatchTracer`

The `LangWatchTracer` extends the standard OpenTelemetry `Tracer` with additional methods for LLM observability.

#### Methods

<ParamField path="startSpan" type="(name: string, options?: SpanOptions, context?: Context) => LangWatchSpan">
  Starts a new `LangWatchSpan` without setting it on context. This method does NOT modify the current Context.
</ParamField>

<ParamField path="startActiveSpan" type="(name: string, fn: (span: LangWatchSpan) => T) => T">
  Starts a new `LangWatchSpan` and calls the given function passing it the created span as first argument. The new span gets set in context and this context is activated for the duration of the function call.
</ParamField>

<ParamField path="withActiveSpan" type="(name: string, fn: (span: LangWatchSpan) => Promise<T> | T) => Promise<T>">
  Starts a new `LangWatchSpan`, runs the provided async function, and automatically handles error recording, status setting, and span ending. This is a safer and more ergonomic alternative to manually using try/catch/finally blocks.
</ParamField>

### `createLangWatchSpan()`

Creates a LangWatchSpan, which adds additional methods to an OpenTelemetry Span. You probably don't need to use this directly, but it's here for completeness.

```typescript
import { createLangWatchSpan } from "langwatch/observability";

const otelSpan = tracer.startSpan('llm-call');
const span = createLangWatchSpan(otelSpan);
span.setType('llm').setInput('Prompt').setOutput('Completion');
```

<ParamField path="span" type="Span" required>
  The OpenTelemetry Span to add LangWatch methods to.
</ParamField>

**Returns**

<ResponseField name="span" type="LangWatchSpan">
  A LangWatchSpan with additional methods for LLM/GenAI observability.
</ResponseField>

### `LangWatchSpan`

The `LangWatchSpan` extends the standard OpenTelemetry `Span` with additional methods for LLM observability.

#### Span Configuration Methods

<ParamField path="setType" type="(type: SpanType) => this">
  Set the type of the span (e.g., 'llm', 'rag', 'tool', etc). This is used for downstream filtering and analytics.
</ParamField>

<ParamField path="setRequestModel" type="(model: string) => this">
  Set the request model name for the span. This is typically the model name sent in the API request (e.g., 'gpt-5', 'claude-3').
</ParamField>

<ParamField path="setResponseModel" type="(model: string) => this">
  Set the response model name for the span. This is the model name returned in the API response, if different from the request.
</ParamField>

<ParamField path="setRAGContexts" type="(ragContexts: LangWatchSpanRAGContext[]) => this">
  Set multiple RAG contexts for the span. Use this to record all retrieved documents/chunks used as context for a generation.
</ParamField>

<ParamField path="setRAGContext" type="(ragContext: LangWatchSpanRAGContext) => this">
  Set a single RAG context for the span. Use this if only one context was retrieved.
</ParamField>

<ParamField path="setMetrics" type="(metrics: LangWatchSpanMetrics) => this">
  Set the metrics for the span.
</ParamField>

<ParamField path="setSelectedPrompt" type="(prompt: Prompt) => this">
  Set the selected prompt for the span. This will attach this prompt to the trace. If this is set on multiple spans, the last one will be used.
</ParamField>

#### Input/Output Methods

<ParamField path="setInput" type="(input: unknown) => this">
  Record the input to the span with automatic type detection.
</ParamField>

<ParamField path="setInput" type="(type: InputOutputType, input: unknown) => this">
  Record the input to the span with explicit type control. Supports "text", "raw", "chat_messages", "list", "json", "guardrail_result", and "evaluation_result" types.
</ParamField>

<ParamField path="setOutput" type="(output: unknown) => this">
  Record the output from the span with automatic type detection.
</ParamField>

<ParamField path="setOutput" type="(type: InputOutputType, output: unknown) => this">
  Record the output from the span with explicit type control. Supports "text", "raw", "chat_messages", "list", "json", "guardrail_result", and "evaluation_result" types.
</ParamField>

## Client SDK

### `LangWatch`

The main LangWatch client class that provides access to LangWatch services.

```typescript
import { LangWatch } from "langwatch";

const langwatch = new LangWatch({
  apiKey: process.env.LANGWATCH_API_KEY,
  endpoint: process.env.LANGWATCH_ENDPOINT_URL
});
```

<ParamField path="options" type="LangWatchConstructorOptions" default="{}">
  Configuration options for the LangWatch client.
</ParamField>

#### Properties

<ResponseField name="prompts" type="PromptsFacade">
  Access to prompt management functionality.
</ResponseField>

<ResponseField name="traces" type="TracesFacade">
  Access to trace management functionality.
</ResponseField>

## Prompt Management

### `langwatch.prompts.get()`

Retrieves a prompt from the LangWatch platform.

```typescript
// Get a prompt without variables
const prompt = await langwatch.prompts.get("prompt-id");
```

<ParamField path="id" type="string" required>
  The ID of the prompt to retrieve.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt">
  The prompt or compiled prompt object.
</ResponseField>

<ResponseField name="error" type="Error">
  Throws an error if the specified prompt version is not found.
</ResponseField>

### `langwatch.prompts.create()`

Creates a new prompt in the LangWatch platform.

```typescript
// Create a basic prompt
const prompt = await langwatch.prompts.create({
  handle: "customer-support-bot",
  name: "Customer Support Bot",
  prompt: "You are a helpful customer support assistant.",
  // ... any other prompt properties
});
```

<ParamField path="options" type="CreatePromptOptions" required>
  Configuration options for creating the prompt.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt">
  The newly created prompt object.
</ResponseField>

### `langwatch.prompts.update()`

Updates an existing prompt, creating a new version automatically.

```typescript
// Update prompt content
const updatedPrompt = await langwatch.prompts.update("customer-support-bot", {
  prompt: "You are a helpful and friendly customer support assistant.",
  // ... any other prompt properties
});
```

<ParamField path="handle" type="string" required>
  The handle (identifier) of the prompt to update.
</ParamField>

<ParamField path="options" type="UpdatePromptOptions" required>
  Configuration options for updating the prompt.
</ParamField>

**Returns**

<ResponseField name="prompt" type="Prompt">
  The updated prompt object (new version).
</ResponseField>

<Warning>
Each update operation creates a new version of the prompt. Previous versions are preserved for version control and rollback purposes.
</Warning>

### `langwatch.prompts.delete()`

Deletes a prompt and all its versions from the LangWatch platform.

```typescript
// Delete a prompt
const result = await langwatch.prompts.delete("customer-support-bot");
```

<ParamField path="handle" type="string" required>
  The handle (identifier) of the prompt to delete.
</ParamField>

**Returns**

<ResponseField name="result" type="DeletePromptResult">
  Confirmation of the deletion operation.
</ResponseField>

<Warning>
This action is irreversible and will permanently remove the prompt and all its versions.
</Warning>

### Prompt Compilation

#### `prompt.compile()`

Compiles a prompt template with provided variables, using lenient compilation that handles missing variables gracefully.

```typescript
// Compile a prompt with variables
const compiledPrompt = prompt.compile({
  name: "Alice",
  topic: "weather"
});

```

<ParamField path="variables" type="Record<string, any>" required>
  Variables to substitute into the prompt template.
</ParamField>

**Returns**

<ResponseField name="compiledPrompt" type="CompiledPrompt">
  The compiled prompt with resolved variables and messages.
</ResponseField>

<Info>
Lenient compilation will not throw errors for missing variables, making it suitable for dynamic content where some variables may be optional.
</Info>

#### `prompt.compileStrict()`

Compiles a prompt template with strict variable validation, throwing an error if any required variables are missing.

```typescript
// Strict compilation with all required variables
const compiledPrompt = prompt.compileStrict({
  name: "Alice",
  topic: "weather"
});
```

<ParamField path="variables" type="Record<string, any>" required>
  Variables to substitute into the prompt template. All template variables must be provided.
</ParamField>

**Returns**

<ResponseField name="compiledPrompt" type="CompiledPrompt">
  The compiled prompt with resolved variables and messages.
</ResponseField>

<ResponseField name="error" type="PromptCompilationError">
  Throws an error if any template variables are missing or invalid.
</ResponseField>

<Warning>
Strict compilation will throw a `PromptCompilationError` if any template variables are missing, ensuring all required data is provided.
</Warning>

## Processors

### `FilterableBatchSpanProcessor`

A span processor that filters spans before processing them.

```typescript
import { LangWatchExporter, FilterableBatchSpanProcessor } from "langwatch";

const processor = new FilterableBatchSpanProcessor(
  new LangWatchExporter(), // Uses environment variables
  [
    {
      fieldName: "span_name",
      matchValue: "health-check",
      matchOperation: "exact_match"
    }
  ]
);
```

<ParamField path="exporter" type="SpanExporter" required>
  The span exporter to use.
</ParamField>

<ParamField path="excludeRules" type="SpanProcessingExcludeRule[]" required>
  Rules to exclude spans from processing.
</ParamField>

## LangChain Integration

### `LangWatchCallbackHandler`

A LangChain callback handler that automatically traces LangChain operations and integrates them with LangWatch.

```typescript
import { LangWatchCallbackHandler } from "langwatch/observability/instrumentation/langchain";
import { ChatOpenAI } from "@langchain/openai";

const handler = new LangWatchCallbackHandler();
const llm = new ChatOpenAI({
  callbacks: [handler]
});

// All operations will now be automatically traced
const response = await llm.invoke("Hello, world!");
```

The `LangWatchCallbackHandler` automatically:
- Creates spans for LLM calls, chains, tools, and retrievers
- Captures input/output data
- Sets appropriate span types and metadata
- Handles errors and status codes
- Integrates with the LangWatch tracing system

### `convertFromLangChainMessages`

Utility function to convert LangChain messages to a format compatible with LangWatch GenAI events.

```typescript
import { convertFromLangChainMessages } from "langwatch/observability/instrumentation/langchain";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";

const messages = [
  new SystemMessage("You are a helpful assistant."),
  new HumanMessage("Hello!"),
];

const convertedMessages = convertFromLangChainMessages(messages);
// Use with span.setInput("chat_messages", convertedMessages)
```

## Exporters

### `LangWatchExporter`

A LangWatch exporter for sending traces to the LangWatch platform. Extends the OpenTelemetry OTLP HTTP trace exporter with proper authentication and metadata headers.

```typescript
import { LangWatchExporter } from "langwatch";

// Using environment variables/fallback configuration
const exporter = new LangWatchExporter();

// Using custom API key and endpoint
const exporter = new LangWatchExporter({
  apiKey: process.env.LANGWATCH_API_KEY,
  endpoint: process.env.LANGWATCH_ENDPOINT_URL
});
```

<ParamField path="apiKey" type="Optional<string>" default="process.env.LANGWATCH_API_KEY">
  Optional API key for LangWatch authentication. If not provided, will use environment variables or fallback configuration.
</ParamField>

<ParamField path="endpointURL" type="Optional<string>" default="process.env.LANGWATCH_ENDPOINT_URL or https://app.langwatch.com">
  Optional custom endpoint URL for LangWatch ingestion. If not provided, will use environment variables or fallback configuration.
</ParamField>

### `LangWatchTraceExporter`

A LangWatch trace exporter with configuration options.

```typescript
import { LangWatchTraceExporter } from "langwatch/observability";

const exporter = new LangWatchTraceExporter({
  apiKey: process.env.LANGWATCH_API_KEY,
  endpoint: process.env.LANGWATCH_ENDPOINT_URL
});
```

### `LangWatchLogsExporter`

A LangWatch logs exporter with configuration options.

```typescript
import { LangWatchLogsExporter } from "langwatch/observability";

const exporter = new LangWatchLogsExporter({
  apiKey: process.env.LANGWATCH_API_KEY,
  endpoint: process.env.LANGWATCH_ENDPOINT_URL
});
```

## Data Capture

### `DataCaptureOptions`

Configuration for automatic data capture.

```typescript
// Simple mode
dataCapture: "all" | "input" | "output" | "none"

// Configuration object
dataCapture: {
  mode: "all" | "input" | "output" | "none"
}
```

### `DataCapturePresets`

Predefined data capture configurations.

```typescript
import { DataCapturePresets } from "langwatch/observability";

// Use predefined configurations
dataCapture: DataCapturePresets.CAPTURE_ALL // Captures everything
dataCapture: DataCapturePresets.CAPTURE_NONE // Captures nothing
dataCapture: DataCapturePresets.INPUT_ONLY // Captures only inputs
dataCapture: DataCapturePresets.OUTPUT_ONLY // Captures only outputs
```

## Logging

### `getLangWatchLogger()`

Returns a LangWatch logger instance for structured logging.

```typescript
import { getLangWatchLogger } from "langwatch";

const logger = getLangWatchLogger("my-service");
```

### `getLangWatchLoggerFromProvider()`

Get a LangWatch logger from a specific logger provider.

```typescript
import { getLangWatchLoggerFromProvider } from "langwatch/observability";

const logger = getLangWatchLoggerFromProvider(
  customLoggerProvider,
  "my-service"
);
```

### `ConsoleLogger`

A console-based logger with configurable log levels and prefixes.

```typescript
import { logger } from "langwatch";

const logger = new logger.ConsoleLogger({
  level: "info",
  prefix: "MyApp"
});

logger.info("Application started");
logger.warn("Deprecated feature used");
logger.error("An error occurred");
```

<ParamField path="options" type="ConsoleLoggerOptions" default="{ level: 'warn' }">
  Logger configuration options.
</ParamField>

### `NoOpLogger`

A no-operation logger that discards all log messages.

```typescript
import { logger } from "langwatch";

const logger = new logger.NoOpLogger();
// All log calls are ignored
```

## CLI

The LangWatch CLI provides command-line tools for managing prompts and interacting with the LangWatch platform.

```bash
# Login to LangWatch
langwatch login

# Initialize a new prompts project
langwatch prompt init

# Create a new prompt
langwatch prompt create my-prompt

# Add a prompt from the registry
langwatch prompt add sentiment-analyzer

# List installed prompts
langwatch prompt list

# Sync prompts with the registry
langwatch prompt sync

# Remove a prompt
langwatch prompt remove my-prompt
```

## Core Data Types

### `SpanType`

Supported types of spans for LangWatch observability:

```typescript
type SpanType =
  | "span"
  | "llm"
  | "chain"
  | "tool"
  | "agent"
  | "guardrail"
  | "evaluation"
  | "rag"
  | "prompt"
  | "workflow"
  | "component"
  | "module"
  | "server"
  | "client"
  | "producer"
  | "consumer"
  | "task"
  | "unknown";
```

### `InputOutputType`

Supported input/output types for span data:

```typescript
type InputOutputType =
  | "text"
  | "raw"
  | "chat_messages"
  | "list"
  | "json"
  | "guardrail_result"
  | "evaluation_result";
```

### `LangWatchSpanRAGContext`

Context for a RAG (Retrieval-Augmented Generation) span.

<ResponseField name="document_id" type="string" required>
  Unique identifier for the source document.
</ResponseField>

<ResponseField name="chunk_id" type="string" required>
  Unique identifier for the chunk within the document.
</ResponseField>

<ResponseField name="content" type="string" required>
  The actual content of the chunk provided to the model.
</ResponseField>

### `LangWatchSpanMetrics`

Metrics for a LangWatch span.

<ResponseField name="promptTokens" type="Optional<number>">
  The number of prompt tokens used.
</ResponseField>

<ResponseField name="completionTokens" type="Optional<number>">
  The number of completion tokens used.
</ResponseField>

<ResponseField name="cost" type="Optional<number>">
  The cost of the span.
</ResponseField>

### `SpanProcessingExcludeRule`

Defines a rule to filter out spans before they are exported to LangWatch.

<ResponseField name="fieldName" type="'span_name'" required>
  The field of the span to match against. Currently, only `"span_name"` is supported.
</ResponseField>

<ResponseField name="matchValue" type="string" required>
  The value to match for the specified `fieldName`.
</ResponseField>

<ResponseField name="matchOperation" type="'includes' | 'exact_match' | 'starts_with' | 'ends_with'" required>
  The operation to use for matching.
</ResponseField>

### `PromptResponse`

The raw prompt response type extracted from the OpenAPI schema.

```typescript
type PromptResponse = NonNullable<
  paths["/api/prompts/{id}"]["get"]["responses"]["200"]["content"]["application/json"]
>;
```

### `Prompt`

A prompt object retrieved from the LangWatch platform with compilation capabilities.

<ResponseField name="id" type="string">
  Unique identifier for the prompt.
</ResponseField>

<ResponseField name="projectId" type="string">
  Project identifier the prompt belongs to.
</ResponseField>

<ResponseField name="organizationId" type="string">
  Organization identifier the prompt belongs to.
</ResponseField>

<ResponseField name="handle" type="string | null">
  Optional handle/slug for the prompt.
</ResponseField>

<ResponseField name="scope" type="'ORGANIZATION' | 'PROJECT'">
  Scope of the prompt - either organization-wide or project-specific.
</ResponseField>

<ResponseField name="name" type="string">
  Name of the prompt.
</ResponseField>

<ResponseField name="updatedAt" type="string">
  Last update timestamp.
</ResponseField>

<ResponseField name="version" type="number">
  Version number.
</ResponseField>

<ResponseField name="versionId" type="string">
  Version identifier.
</ResponseField>

<ResponseField name="model" type="string">
  Model used for the prompt.
</ResponseField>

<ResponseField name="prompt" type="string">
  The prompt template.
</ResponseField>

<ResponseField name="messages" type="Array">
  Array of message objects.
</ResponseField>

<ResponseField name="responseFormat" type="object">
  Response format configuration.
</ResponseField>

<ResponseField name="authorId" type="string | null">
  ID of the prompt author.
</ResponseField>

<ResponseField name="createdAt" type="string">
  Creation timestamp.
</ResponseField>

<ResponseField name="inputs" type="Array">
  Input definitions for the prompt.
</ResponseField>

<ResponseField name="outputs" type="Array">
  Output definitions for the prompt.
</ResponseField>

### `CompiledPrompt`

A compiled prompt that extends Prompt with reference to the original template.

<ResponseField name="original" type="Prompt">
  The original prompt object before compilation.
</ResponseField>

### `TemplateVariables`

Template variables for prompt compilation.

```typescript
type TemplateVariables = Record<string, string | number | boolean | object | null>;
```

### `PromptCompilationError`

Error thrown when prompt compilation fails.

<ResponseField name="template" type="string">
  The template that failed to compile.
</ResponseField>

<ResponseField name="originalError" type="any">
  The original compilation error.
</ResponseField>

### `LangWatchConstructorOptions`

Configuration options for the LangWatch client.

<ResponseField name="apiKey" type="Optional<string>">
  Your LangWatch API key. Defaults to `process.env.LANGWATCH_API_KEY`.
</ResponseField>

<ResponseField name="endpoint" type="Optional<string>">
  The LangWatch endpoint URL. Defaults to `process.env.LANGWATCH_ENDPOINT`.
</ResponseField>

<ResponseField name="options" type="Optional<{ logger?: Logger }>">
  Additional options including custom logger.
</ResponseField>

## Usage Examples

### Basic Tracing

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service"
});

const tracer = getLangWatchTracer("my-service");

await tracer.withActiveSpan("process-request", async (span) => {
  span.setType("llm");
  span.setRequestModel("gpt-5");

  // Your LLM call here
  const response = await openai.chat.completions.create({
    model: "gpt-5",
    messages: [{ role: "user", content: "Hello!" }]
  });

  span.setOutput(response.choices[0].message.content);
  span.setMetrics({
    promptTokens: response.usage?.prompt_tokens,
    completionTokens: response.usage?.completion_tokens
  });
});
```

### RAG Operations

```typescript
await tracer.withActiveSpan("rag-query", async (span) => {
  span.setType("rag");

  // Retrieve documents
  const documents = await vectorStore.similaritySearch("query", 5);

  // Set RAG contexts
  span.setRAGContexts(documents.map(doc => ({
    document_id: doc.metadata.id,
    chunk_id: doc.metadata.chunk_id,
    content: doc.pageContent
  })));

  // Generate response
  const response = await llm.generate([documents, "query"]);
  span.setOutput(response);
});
```

### Using Semantic Conventions

```typescript
import { attributes, VAL_GEN_AI_SYSTEM_OPENAI } from "langwatch/observability";
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

await tracer.withActiveSpan("llm-call", async (span) => {
  // Use semantic convention attributes for consistency
  span.setType("llm");
  span.setAttribute("langwatch.streaming", false);

  // Set input/output with proper typing
  span.setInput("chat_messages", [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Hello!" }
  ]);

  // Set output
  span.setOutput("text", "Hello! How can I help you today?");
});
```

### Prompt Management

```typescript
import { LangWatch } from "langwatch";

const langwatch = new LangWatch({
  apiKey: process.env.LANGWATCH_API_KEY
});

const prompt = await langwatch.prompts.get("customer-support");

const compiledPrompt = prompt.compile({
  name: "John Doe",
  product: "LangWatch"
});

console.log(compiledPrompt.content);
// Output: "Hello John Doe! How can I help you with LangWatch today?"
```

### LangChain Integration

```typescript
import { LangWatchCallbackHandler } from "langwatch/observability/instrumentation/langchain";
import { ChatOpenAI } from "@langchain/openai";
import { setupObservability } from "langwatch/observability/node";

setupObservability();

const handler = new LangWatchCallbackHandler();
const llm = new ChatOpenAI({
  callbacks: [handler],
  model: "gpt-5-mini"
});

// All operations are automatically traced
const response = await llm.invoke("What is the capital of France?");
```

### Custom Span Processing

```typescript
import { LangWatchExporter, FilterableBatchSpanProcessor } from "langwatch";
import { setupObservability } from "langwatch/observability/node";

const processor = new FilterableBatchSpanProcessor(
  new LangWatchExporter(),
  [
    {
      fieldName: "span_name",
      matchValue: "health-check",
      matchOperation: "exact_match"
    }
  ]
);

setupObservability({
  langwatch: 'disabled', // Prevent double exporting to LangWatch
  spanProcessors: [processor]
});
```

### Advanced Setup with Data Capture

```typescript
import { setupObservability } from "langwatch/observability/node";

setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    processorType: "batch"
  },
  serviceName: "my-service",
  attributes: {
    "service.version": "1.0.0",
    "deployment.environment.name": "production"
  },
  dataCapture: (context) => {
    // Don't capture sensitive data in production
    if (context.environment === "production" &&
        context.operationName.includes("password")) {
      return "none";
    }
    return "all";
  },
  debug: {
    consoleTracing: true,
    logLevel: "debug"
  }
});
```

### Graceful Shutdown

```typescript
import { setupObservability } from "langwatch/observability/node";

const { shutdown } = setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  }
});

// Graceful shutdown
process.on('SIGTERM', async () => {
  console.log('Shutting down observability...');
  await shutdown();
  console.log('Observability shutdown complete');
  process.exit(0);
});
```

## Related Documentation

For practical examples and advanced usage patterns, see:

- **[Integration Guide](/integration/typescript/guide)** - Get started with LangWatch TypeScript SDK
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management techniques
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - Standardized attribute naming guidelines
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Troubleshoot integration issues
- **[Framework Integrations](/integration/typescript/integrations)** - Framework-specific setup and examples

<Tip>
Start with the [Integration Guide](/integration/typescript/guide) for a quick setup, then refer to this API reference for detailed configuration options.
</Tip>

---

# FILE: ./integration/typescript/guide.mdx

---
title: TypeScript Integration Guide
sidebarTitle: Guide
description: Get started with LangWatch TypeScript SDK in 5 minutes
keywords: langwatch, typescript, sdk, guide, observability, tracing, logging, data capture, data collection, data ingestion
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/badge/npm-langwatch-007EC6?style=flat&logo=npm" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

Get started with LangWatch TypeScript SDK in under 5 minutes. This guide will walk you through setting up observability for your LLM applications, from basic tracing to advanced features.

## Prerequisites

Before you start, make sure you have:

- **Node.js** 18+ installed
- A **LangWatch account** (sign up at [app.langwatch.ai](https://app.langwatch.ai))
- Your **LangWatch API key** from the dashboard
- An **OpenAI API key** (for the LLM example)

## Quick Start (5 minutes)

### Step 1: Install Dependencies

```bash
npm install langwatch @opentelemetry/sdk-node @opentelemetry/context-async-hooks
npm install @ai-sdk/openai ai
```

<Note>
The `@ai-sdk/openai` and `ai` packages are only required for the example in this guide. You can skip this step if you're only looking to install the LangWatch SDK.
</Note>

### Step 2: Set Up API Keys

1. **LangWatch API Key**:
   - Go to [app.langwatch.ai](https://app.langwatch.ai) and sign up
   - Create a new project
   - Copy your API key from the project settings

2. **OpenAI API Key**:
   - Get your API key from [platform.openai.com](https://platform.openai.com/api-keys)

3. **Set environment variables**:

```bash
export LANGWATCH_API_KEY=your_langwatch_api_key_here
export OPENAI_API_KEY=your_openai_api_key_here
```

### Step 3: Your First LLM Trace

Create a new file `app.ts`:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";

// Setup LangWatch Observability (uses LANGWATCH_API_KEY by default)
await setupObservability({
  serviceName: "my-ai-laundry-startup",
});

// Create a tracer
const tracer = getLangWatchTracer("laundry-chatbot");

// Your first traced LLM interaction
async function askAI(question: string) {
  return await tracer.withActiveSpan("ask-ai", async (span) => {
    // Make the LLM call using Vercel AI SDK
    const response = await generateText({
      model: openai("gpt-5-mini"),
      prompt: question,
      maxTokens: 100,
      // The LangWatch SDK will automatically capture LLM data
      // input, output, metrics, etc.
      experimental_telemetry: { isEnabled: true },
    });

    return response.text;
  });
}

// Test it
const answer = await askAI("What is LangWatch?");
console.log("AI Response:", answer);
console.log("Check your LangWatch dashboard!");
```

### Step 4: Run and See Results

```bash
npx tsx app.ts
```

Now visit your LangWatch dashboard - you should see your first trace! 🎉

<Check>
**What you'll see**: A trace named "greet-user" with input/output data, timing, and status.
</Check>

## What Just Happened?

Let's break down what we just set up:

- **Trace**: The entire `greetUser` function execution
- **Span**: The individual operation within the trace
- **Input/Output**: The data flowing through your function
- **Timing**: How long each operation took
- **Status**: Whether the operation succeeded

## Core Concepts

Think of LangWatch like a **debugger for your LLM applications**:

- **Traces** = Complete user interactions (e.g., "What's the weather?")
- **Spans** = Individual steps within a trace (e.g., "LLM call", "database query")
- **Threads** = Conversations (group related traces together)
- **Users** = Individual users (for analytics)

<Note>
For detailed explanations of all concepts, see our [Concepts Guide](/concepts).
</Note>

<Tip>
For consistent observability across your application, learn about [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) - standardized naming guidelines for attributes and metadata.
</Tip>

## Integrations

LangWatch offers seamless integrations with many popular TypeScript libraries and frameworks. These integrations provide automatic instrumentation, capturing relevant data from your LLM applications with minimal setup.

Below is a list of currently supported integrations. Click on each to learn more about specific setup instructions and available features:

- [Azure AI](/integration/typescript/integrations/azure)
- [Langchain](/integration/typescript/integrations/langchain)
- [Mastra](/integration/typescript/integrations/mastra)
- [OpenAI](/integration/typescript/integrations/open-ai)
- [Vercel AI SDK](/integration/typescript/integrations/vercel-ai-sdk)

<Note>
For detailed integration guides, see our [integration documentation](/integration/typescript/integrations). Each integration includes framework-specific examples and best practices.
</Note>


## Common Development Scenarios

### Scenario 1: LLM Application

```typescript
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";

async function chatWithAI(userMessage: string) {
  return await tracer.withActiveSpan("chat-with-ai", async (span) => {
    // Make the LLM call
    const response = await generateText({
      model: openai("gpt-5-mini"),
      prompt: userMessage,
      experimental_telemetry: { isEnabled: true }, // Auto-captures LLM data
    });

    return response.text;
  });
}
```

### Scenario 2: RAG Application

```typescript
async function answerWithRAG(question: string) {
  return await tracer.withActiveSpan("rag-answer", async (span) => {
    // 1. Retrieve documents
    const docs = await tracer.withActiveSpan("retrieve-docs", async (retrieveSpan) => {
      retrieveSpan.setType("rag");
      const documents = await searchDocuments(question);

      // Record what documents were retrieved
      retrieveSpan.setRAGContexts(
        documents.map(doc => ({
          document_id: doc.id,
          chunk_id: doc.chunkId,
          content: doc.content
        }))
      );

      return documents;
    });

    // 2. Generate answer
    const answer = await generateAnswer(question, docs);

    span.setOutput(answer);

    return answer;
  });
}
```

<Note>
For consistent attribute naming and TypeScript autocomplete support, see our [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) guide. For advanced span management techniques, check out [Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation).
</Note>

### Scenario 3: Conversation Threading

```typescript
async function handleConversation(userId: string, threadId: string, message: string) {
  return await tracer.withActiveSpan("conversation-turn", {
    attributes: {
      "langwatch.user.id": userId,
      "langwatch.thread.id": threadId
    }
  }, async (span) => {
    // Your conversation logic here
    const response = await processMessage(message);

    span.setOutput(response);

    return response;
  });
}
```

## Configuration

### Basic Configuration

```typescript
import { setupObservability } from "langwatch/observability/node";

const handle = await setupObservability({
  // Required: Your service name
  serviceName: "my-ai-service",

  // Optional: Custom API key (defaults to LANGWATCH_API_KEY env var)
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
  },

  // Optional: Global attributes for all traces
  attributes: {
    "service.version": "1.0.0",
    "environment": process.env.NODE_ENV,
  }
});
```

### Environment-Specific Setup

<CodeGroup>
```typescript Development
const handle = await setupObservability({
  serviceName: "my-laundry-startup",
  dataCapture: "all", // Capture everything in dev
  attributes: {
    "deployment.environment.name": process.env.NODE_ENV,
  }
});
```

```typescript Production
const handle = await setupObservability({
  serviceName: "my-laundry-startup",
  dataCapture: "output", // Capture only output data in production
  attributes: {
    "deployment.environment.name": process.env.NODE_ENV,
  }
});
```
</CodeGroup>

### Graceful Shutdown

The `setupObservability` function returns an `ObservabilityHandle` that provides a `shutdown` method for graceful cleanup. This ensures all pending traces are exported before your application terminates.

#### Automatic Shutdown

By default, LangWatch automatically handles shutdown when your application receives a `SIGTERM` signal:

```typescript
// Automatic shutdown is enabled by default
const handle = await setupObservability({
  serviceName: "my-service",
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  }
});

// No manual shutdown needed - handled automatically
```

#### Manual Shutdown

For environments where you can't listen to `SIGTERM` or need custom shutdown logic, you can manually call the shutdown method:

```typescript
const handle = await setupObservability({
  serviceName: "my-service",
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  advanced: {
    disableAutoShutdown: true, // Disable automatic SIGTERM handling
  }
});

// Manual shutdown when your application terminates
process.on('SIGTERM', async () => {
  console.log('Shutting down observability...');
  await handle.shutdown();
  console.log('Observability shutdown complete');
  process.exit(0);
});

// Force shutdown with timeout
process.on('SIGINT', async () => {
  console.log('Force shutdown...');
  await Promise.race([
    handle.shutdown(),
    new Promise(resolve => setTimeout(resolve, 5000))
  ]);
  process.exit(1);
});
```

#### What Happens During Shutdown

The shutdown process ensures data integrity:

1. **Flushes pending traces** to the exporter
2. **Closes the trace exporter** connection
3. **Shuts down the tracer provider**
4. **Cleans up registered instrumentations**

<Tip>
Always call `shutdown()` before your application exits to prevent data loss. The method is safe to call multiple times.
</Tip>

<Warning>
If you don't call `shutdown()`, some traces may be lost when your application terminates abruptly.
</Warning>

## Development Workflow

### Local Development

1. **Set up environment**:
```bash
export LANGWATCH_API_KEY=your_key
export NODE_ENV=development
```

2. **Run your app**:
```bash
npm run dev
```

3. **Check dashboard**: Visit [app.langwatch.ai](https://app.langwatch.ai) to see traces

### Debugging

Enable console logging for local development:

```typescript
const handle = await setupObservability({
  serviceName: "my-service",
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
  },
  debug: {
    consoleTracing: true,
    consoleLogging: true,
    logLevel: 'info' // Lower this to `debug` if you're debugging the LangWatch integration
  },
});
```

## Troubleshooting

### Common Issues

<AccordionGroup>
<Accordion title="No traces appearing in dashboard">
- Check your API key is correct
- Verify network connectivity to app.langwatch.ai
- Ensure `setupObservability` is called before any tracing
- Check browser console for errors
- See [Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript) for detailed solutions
</Accordion>

<Accordion title="High memory usage">
- Use batch processing: `processorType: 'batch'`
- Implement graceful shutdown
- Consider reducing data capture in production
</Accordion>

<Accordion title="Performance impact">
- Tracer overhead is minimal (~1-2ms per span)
- Use module-level tracers (not function-level)
- Consider sampling in high-traffic scenarios
</Accordion>
</AccordionGroup>

### Getting Help

- **Documentation**: [docs.langwatch.ai](https://docs.langwatch.ai)
- **GitHub**: [github.com/langwatch/langwatch](https://github.com/langwatch/langwatch)
- **Discord**: [discord.gg/langwatch](https://discord.gg/langwatch)

## Next Steps

Now that you have basic tracing working, explore:

- **[API Reference](/integration/typescript/reference)** - Complete API documentation for the LangWatch TypeScript SDK
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management and fine-grained control
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - Standardized naming guidelines for attributes and metadata
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug tracing issues and optimize performance
- **[OpenTelemetry Migration](/integration/typescript/tutorials/opentelemetry-migration)** - Migrate your existing OpenTelemetry setup with LangWatch
- **[Framework Integrations](/integration/typescript/integrations)** - Specific guides for OpenAI, LangChain, Azure, and more

<Tip>
Start simple and add complexity gradually. You can always add more detailed tracing later as your application grows!
</Tip>

---

# FILE: ./integration/typescript/integrations/langchain.mdx

---
title: LangChain Instrumentation
sidebarTitle: TypeScript/JS
description: Learn how to instrument Langchain applications with the LangWatch TypeScript SDK.
icon: square-js
keywords: langchain, instrumentation, callback, opentelemetry, langwatch, typescript, tracing, openllmetry
---

Langchain is a powerful framework for building LLM applications. LangWatch integrates with Langchain to provide detailed observability into your chains, agents, LLM calls, and tool usage.

This guide covers how to instrument Langchain with LangWatch using the **LangWatch Langchain Callback Handler** - the most direct and comprehensive method for capturing rich Langchain-specific trace data.

## Using LangWatch's Langchain Callback Handler

This is the preferred and most comprehensive method for instrumenting Langchain with LangWatch. The LangWatch SDK provides a `LangWatchCallbackHandler` that deeply integrates with Langchain's event system.

```typescript
import { setupObservability } from "langwatch/observability/node";
import { LangWatchCallbackHandler } from "langwatch/observability/instrumentation/langchain";
import { getLangWatchTracer } from "langwatch";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

// Initialize LangWatch
setupObservability();

const tracer = getLangWatchTracer("langchain-example");

async function handleMessageWithCallback(userQuestion: string) {
  return await tracer.withActiveSpan("Langchain - QA with Callback", {
    attributes: {
      "langwatch.thread_id": "callback-user",
    },
  }, async (span) => {
    const langWatchCallback = new LangWatchCallbackHandler();

    const model = new ChatOpenAI({
      modelName: "gpt-5",
      temperature: 0.7,
      callbacks: [langWatchCallback],
    });

    const prompt = ChatPromptTemplate.fromMessages([
      ["system", "You are a concise assistant."],
      ["human", "{question}"],
    ]);

    // Modern LCEL (LangChain Expression Language) syntax
    const chain = prompt.pipe(model).pipe(new StringOutputParser());

    const response = await chain.invoke({ question: userQuestion });
    return response;
  });
}

async function mainCallback() {
  if (!process.env.OPENAI_API_KEY) {
    console.log("OPENAI_API_KEY not set. Skipping Langchain callback example.");
    return;
  }

  const response = await handleMessageWithCallback("What is Langchain? Explain briefly.");
  console.log(`AI (Callback): ${response}`);
}

mainCallback().catch(console.error);
```

**How it Works:**
- `setupObservability()`: Initializes LangWatch with default configuration.
- `getLangWatchTracer()`: Creates a tracer instance for your application.
- `tracer.withActiveSpan()`: Creates a parent LangWatch trace with automatic error handling and span management.
- `LangWatchCallbackHandler`: A LangWatch-specific callback handler that captures Langchain events and converts them into detailed LangWatch spans.
- The callback handler is passed to Langchain components via the `callbacks` option.

**Key points:**
- Provides the most detailed Langchain-specific structural information (chains, agents, tools, LLMs as distinct steps).
- Works for all Langchain execution methods (`invoke`, `stream`, `batch`, etc.).
- Automatically handles span lifecycle management with `withActiveSpan()`.

<Note>
### Why Use the LangWatch Langchain Callback Handler?

The **LangWatch Langchain Callback Handler** provides the richest, most Langchain-aware traces directly integrated with LangWatch's tracing context. It's the recommended approach for optimal Langchain-specific observability within LangWatch.
</Note>

## Common Mistakes and Caveats

### 1. Setup and Initialization Issues

<Warning>
**Multiple setup calls**: `setupObservability()` can only be called once per process. Subsequent calls will throw an error.
</Warning>

```typescript
// ❌ Wrong - Multiple setup calls
setupObservability();
setupObservability(); // This will throw an error

// ✅ Correct - Single setup call
setupObservability();
```

### 2. Callback Handler Usage

<Warning>
**Reusing callback handlers**: Each trace should use a fresh `LangWatchCallbackHandler` instance to avoid span conflicts.
</Warning>

```typescript
// ❌ Wrong - Reusing callback handler
const callback = new LangWatchCallbackHandler();

async function processMultipleRequests() {
  // This can cause span conflicts
  const model1 = new ChatOpenAI({ callbacks: [callback] });
  const model2 = new ChatOpenAI({ callbacks: [callback] });
}

// ✅ Correct - Fresh callback handler per trace
async function processMultipleRequests() {
  const callback1 = new LangWatchCallbackHandler();
  const callback2 = new LangWatchCallbackHandler();

  const model1 = new ChatOpenAI({ callbacks: [callback1] });
  const model2 = new ChatOpenAI({ callbacks: [callback2] });
}
```

### 3. Span Management

<Warning>
**Manual span management**: Avoid manually managing spans when using `withActiveSpan()`. The function handles span lifecycle automatically.
</Warning>

```typescript
// ❌ Wrong - Manual span management with withActiveSpan
await tracer.withActiveSpan("my-operation", async (span) => {
  span.setStatus({ code: SpanStatusCode.OK });
  span.end(); // Don't manually end spans in withActiveSpan
});

// ✅ Correct - Let withActiveSpan handle span lifecycle
await tracer.withActiveSpan("my-operation", async (span) => {
  // Your code here - span is automatically ended
});
```

### 4. Environment Configuration

<Warning>
**Missing environment variables**: Ensure all required environment variables are set before running your application.
</Warning>

```typescript
// ❌ Wrong - No environment validation
setupObservability();
const model = new ChatOpenAI(); // May fail if OPENAI_API_KEY not set

// ✅ Correct - Environment validation
if (!process.env.OPENAI_API_KEY) {
  console.error("OPENAI_API_KEY environment variable is required");
  process.exit(1);
}

setupObservability();
const model = new ChatOpenAI();
```

### 5. Error Handling

<Warning>
**Unhandled promise rejections**: Always handle errors in async operations to prevent unhandled promise rejections.
</Warning>

```typescript
// ❌ Wrong - Unhandled promise rejection
mainCallback(); // This can cause unhandled promise rejection

// ✅ Correct - Proper error handling
mainCallback().catch(console.error);
// or
try {
  await mainCallback();
} catch (error) {
  console.error("Error in main callback:", error);
}
```

<Info>
### Best Practices Summary

1. **Call `setupObservability()` only once per process**
2. **Use fresh callback handlers** for each trace to avoid conflicts
3. **Let `withActiveSpan()` handle span lifecycle** - don't manually end spans
4. **Validate environment variables** before starting your application
5. **Handle errors properly** to avoid unhandled promise rejections
</Info>

## Example Project

You can find a complete example project demonstrating LangChain integration with LangWatch [on our GitHub](https://github.com/langwatch/langwatch/tree/main/typescript-sdk/examples/langchain). This example includes:

- **Basic Chatbot**: A simple chatbot that handles conversation flow using LangChain
- **Conversation Management**: User input handling and conversation history management
- **Error Handling**: Comprehensive error handling and exit commands
- **Full LangWatch Integration**: Complete observability and tracing setup

### Key Features

- **Automatic Tracing**: All LangChain operations are automatically traced and sent to LangWatch
- **Conversation Flow**: Demonstrates proper conversation loop management
- **Input/Output Tracking**: Tracks user inputs and AI responses
- **Error Recovery**: Handles errors gracefully with proper cleanup

## Related Documentation

For more advanced LangChain integration patterns and best practices:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management for LangChain operations
- **[Capturing RAG](/integration/typescript/tutorials/capturing-rag)** - RAG-specific patterns with LangChain
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - LangChain-specific attributes and conventions
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug LangChain integration issues

<Tip>
LangChain's automatic instrumentation works well with [Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation) for custom operations and [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent attribute naming.
</Tip>

---

# FILE: ./integration/typescript/integrations/vercel-ai-sdk.mdx

---
title: Vercel AI SDK
description: LangWatch Vercel AI SDK integration guide
sidebarTitle: Vercel AI SDK
keywords: vercel ai sdk, langwatch, tracing, observability, vercel, ai, sdk, langwatch, tracing, observability
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

<LLMsTxtProtip />

<Prerequisites />

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Integration


The Vercel AI SDK supports tracing via Next.js OpenTelemetry integration. By using the `LangWatchExporter`, you can automatically collect those traces to LangWatch.

First, you need to install the necessary dependencies:

```bash
npm install @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```

Then, set up the OpenTelemetry for your application, follow one of the tabs below depending whether you are using AI SDK with Next.js or on Node.js:


### Next.js

You need to enable the `instrumentationHook` in your `next.config.js` file if you haven't already:

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    instrumentationHook: true,
  },
};

module.exports = nextConfig;
```

Next, you need to create a file named `instrumentation.ts` (or `.js`) in the __root directory__ of the project (or inside `src` folder if using one), with `LangWatchExporter` as the traceExporter:

<CodeGroup>
```typescript
import { registerOTel } from '@vercel/otel';
import { LangWatchExporter } from 'langwatch';

export function register() {
  registerOTel({
    serviceName: 'next-app',
    traceExporter: new LangWatchExporter({
      apiKey: process.env.LANGWATCH_API_KEY
    }),
  })
}
```
</CodeGroup>

(Read more about Next.js OpenTelemetry configuration [on the official guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry#manual-opentelemetry-configuration))

Finally, enable `experimental_telemetry` tracking on the AI SDK calls you want to trace:

```typescript
import { attributes } from 'langwatch';

const result = await generateText({
  model: openai('gpt-5'),
  prompt: 'Explain why a chicken would make a terrible astronaut, be creative and humorous about it.',
  experimental_telemetry: {
    isEnabled: true,
    // optional metadata
    metadata: {
      "langwatch.user.id": "myuser-123",
      "langwatch.thread.id": "mythread-123",
    },
  },
});
```

### Node.js

For Node.js, start by following the official OpenTelemetry guide:

- [OpenTelemetry Node.js Getting Started](https://opentelemetry.io/docs/languages/js/getting-started/nodejs/)

Once you have set up OpenTelemetry, you can use `setupObservability` from the LangWatch
SDK to automatically instrument your application and send your traces to LangWatch:

```typescript
import { setupObservability } from 'langwatch/observability/node';

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY, // optional, defaults to LANGWATCH_API_KEY env var
  },
  serviceName: "my-service",
});
```

That's it! Your messages will now be visible on LangWatch:

![Vercel AI SDK](/images/integration/vercel-ai-sdk.png)

## Example Project

You can find a full example project with a more complex pipeline and Vercel AI SDK and LangWatch integration [on our GitHub](https://github.com/langwatch/langwatch/tree/main/typescript-sdk/examples/vercel-ai).

## Related Documentation

For more advanced Vercel AI SDK integration patterns and best practices:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management for Vercel AI SDK calls
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - Vercel AI SDK-specific attributes and conventions
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug Vercel AI SDK integration issues
- **[Capturing Metadata](/integration/typescript/tutorials/capturing-metadata)** - Adding custom metadata to Vercel AI SDK calls

<Tip>
For production Vercel AI SDK applications, combine manual instrumentation with [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent observability and better analytics.
</Tip>

---

# FILE: ./integration/typescript/integrations/azure.mdx

---
title: Azure OpenAI
sidebarTitle: TypeScript/JS
icon: square-js
description: LangWatch Azure OpenAI integration guide
keywords: azure openai, langwatch, typescript, javascript, sdk, instrumentation, opentelemetry
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

<LLMsTxtProtip />

<Prerequisites />

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Integration

Start by setting up observability and initializing the LangWatch tracer:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability first
setupObservability();

const tracer = getLangWatchTracer("my-service");
```

Then to capture your LLM calls, you can use the `withActiveSpan` method to create an LLM span with automatic lifecycle management:

```typescript
import { AzureOpenAI } from "openai";

// Model to be used and messages that will be sent to the LLM
const model = "gpt-5-mini";
const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
];

const openai = new AzureOpenAI({
  apiKey: process.env.AZURE_OPENAI_API_KEY,
  apiVersion: "2024-02-01",
  endpoint: process.env.AZURE_OPENAI_ENDPOINT,
});

// Use withActiveSpan for automatic error handling and span cleanup
const result = await tracer.withActiveSpan("llm-call", async (span) => {
  // Set span type and input
  span.setType("llm");
  span.setInput("chat_messages", messages);
  span.setRequestModel(model);

  // Make the Azure OpenAI call
  const chatCompletion = await openai.chat.completions.create({
    messages: messages,
    model: model,
  });

  // Set output and metrics
  span.setOutput("chat_messages", [chatCompletion.choices[0]!.message]);
  span.setMetrics({
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  });

  return chatCompletion;
});
```

The `withActiveSpan` method automatically:
- Creates the span with the specified name
- Handles errors and sets appropriate span status
- Ends the span when the function completes
- Returns the result of your async function

## Community Auto-Instrumentation

For automatic instrumentation without manual span creation, you can use the [OpenInference instrumentation for OpenAI](https://github.com/Arize-ai/openinference/tree/main/js/packages/openinference-instrumentation-openai), which also works with Azure OpenAI:

<Steps>
<Step title="Install the OpenInference instrumentation">
  ```bash
  npm install @arizeai/openinference-instrumentation-openai
  ```
</Step>

<Step title="Register the instrumentation">
  ```typescript
  import { NodeSDK } from "@opentelemetry/sdk-node";
  import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";
  import { setupObservability } from "langwatch/observability/node";

  // Setup observability with the instrumentation
  setupObservability({
    instrumentations: [new OpenAIInstrumentation()],
  });
  ```
</Step>

<Step title="Use Azure OpenAI normally">
  ```typescript
  import { AzureOpenAI } from "openai";

  const openai = new AzureOpenAI({
    apiKey: process.env.AZURE_OPENAI_API_KEY,
    apiVersion: "2024-02-01",
    endpoint: process.env.AZURE_OPENAI_ENDPOINT,
  });

  // This call will be automatically instrumented
  const completion = await openai.chat.completions.create({
    model: "gpt-5-mini",
    messages: [{ role: "user", content: "Hello!" }],
  });
  ```
</Step>
</Steps>

<Info>
The OpenInference instrumentation automatically captures:
- Input messages and model configuration
- Output responses and token usage
- Error handling and status codes
- Request/response timing
- Azure-specific configuration (endpoint, API version)
</Info>

<Warning>
When using auto-instrumentation, you may need to configure data capture settings to control what information is sent to LangWatch.
</Warning>

<Note>
On short-live environments like Lambdas or Serverless Functions, be sure to call <br /> `await trace.sendSpans();` to wait for all pending requests to be sent before the runtime is destroyed.
</Note>

## Capture a RAG Span

Appart from LLM spans, another very used type of span is the RAG span. This is used to capture the retrieved contexts from a RAG that will be used by the LLM, and enables a whole new set of RAG-based features evaluations for RAG quality on LangWatch.

<TypeScriptRAG />

## Capture an arbritary Span

You can also use generic spans to capture any type of operation, its inputs and outputs, for example for a function call:

<TypeScriptCaptureSpans />

## Capturing Exceptions

To capture also when your code throws an exception, you can simply wrap your code around a try/catch, and update or end the span with the exception:

<TypeScriptExceptions />

## Capturing custom evaluation results

[LangWatch Evaluators](/llm-evaluation/list) can run automatically on your traces, but if you have an in-house custom evaluator, you can also capture the evaluation
results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

<TypeScriptCustomEvaluation />


## Related Documentation

For more advanced Azure AI integration patterns and best practices:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management for Azure AI calls
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - Azure-specific attributes and conventions
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug Azure integration issues
- **[Capturing Metadata](/integration/typescript/tutorials/capturing-metadata)** - Adding custom metadata to Azure AI calls

<Tip>
For production Azure AI applications, combine manual instrumentation with [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent observability and better analytics.
</Tip>


---

# FILE: ./integration/typescript/integrations/langgraph.mdx

---
title: LangGraph Instrumentation
sidebarTitle: TypeScript/JS
description: Learn how to instrument LangGraph applications with the LangWatch TypeScript SDK.
icon: square-js
keywords: langgraph, instrumentation, callback, opentelemetry, langwatch, typescript, tracing, openllmetry, state graph, workflow
---

LangGraph is a powerful framework for building stateful, multi-step AI applications with complex workflows. LangWatch integrates with LangGraph to provide detailed observability into your state graphs, node executions, routing decisions, and workflow patterns.

This guide covers how to instrument LangGraph with LangWatch using the **LangWatch LangChain Callback Handler** - the most direct and comprehensive method for capturing rich LangGraph-specific trace data.

## Using LangWatch's LangChain Callback Handler with LangGraph

LangGraph is built on top of LangChain, so we can use the same `LangWatchCallbackHandler` to instrument LangGraph applications. This provides comprehensive tracing of your state graphs, node executions, and workflow patterns.

```typescript
import { setupObservability } from "langwatch/observability/node";
import { LangWatchCallbackHandler } from "langwatch/observability/instrumentation/langchain";
import { getLangWatchTracer } from "langwatch";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";
import { StateGraph, END, START } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph";
import { z } from "zod";

// Initialize LangWatch
setupObservability();

const tracer = getLangWatchTracer("langgraph-example");

// Define the state schema using Zod
const GraphState = z.object({
  question: z.string(),
  current_step: z.string().default("start"),
  needs_search: z.boolean().default(false),
  search_results: z.string().default(""),
  analysis: z.string().default(""),
  final_answer: z.string().default(""),
  iterations: z.number().default(0),
});

type GraphStateType = z.infer<typeof GraphState>;

async function handleWorkflowWithCallback(userQuestion: string) {
  return await tracer.withActiveSpan("LangGraph - Research Workflow", {
    attributes: {
      "langwatch.thread_id": "langgraph-user",
      "langwatch.tags": ["langgraph", "research-agent", "multi-step"],
    },
  }, async (span) => {
    span.setType("workflow");

    const langWatchCallback = new LangWatchCallbackHandler();

    const chatModel = new ChatOpenAI({
      modelName: "gpt-4o-mini",
      temperature: 0.3,
      callbacks: [langWatchCallback],
    });

    // Node 1: Analyze the question
    const analyzeQuestion = async (state: GraphStateType) => {
      const prompt = `
      Analyze this question and determine if it requires current/recent information that would need web search.

      Question: ${state.question}

      Respond with just "YES" if web search is needed, "NO" if general knowledge is sufficient.
      `;

      const result = await chatModel.invoke([
        new SystemMessage("You are a question analyzer. Respond with only YES or NO."),
        new HumanMessage(prompt),
      ]);

      const needsSearch = (result.content as string).toUpperCase().includes("YES");

      return {
        current_step: "question_analyzed",
        needs_search: needsSearch,
      };
    };

    // Mock search tool for demo purposes
    const performWebSearch = async (query: string): Promise<string> => {
      // Simulate search delay
      await new Promise((resolve) => setTimeout(resolve, 1000));
      return `Mock search results for "${query}":
- Recent developments and current information
- Latest news and analysis from reliable sources
- Expert opinions and academic research
- Current market trends and data points`;
    };

    // Node 2: Perform web search
    const performSearch = async (state: GraphStateType) => {
      const searchResults = await performWebSearch(state.question);

      return {
        current_step: "search_completed",
        search_results: searchResults,
      };
    };

    // Node 3: Analyze information
    const analyzeInformation = async (state: GraphStateType) => {
      const context = state.search_results
        ? `Search Results:\n${state.search_results}\n\n`
        : "Using general knowledge (no search performed).\n\n";

      const prompt = `
      ${context}Question: ${state.question}

      Provide a thorough analysis of this question, considering multiple perspectives and available information.
      `;

      const result = await chatModel.invoke([
        new SystemMessage("You are an expert analyst. Provide comprehensive analysis."),
        new HumanMessage(prompt),
      ]);

      return {
        current_step: "analysis_completed",
        analysis: result.content as string,
      };
    };

    // Node 4: Generate final answer
    const generateAnswer = async (state: GraphStateType) => {
      const prompt = `
      Question: ${state.question}
      Analysis: ${state.analysis}
      ${state.search_results ? `Search Results: ${state.search_results}` : ""}

      Based on the analysis and available information, provide a comprehensive, well-structured answer.
      `;

      const result = await chatModel.invoke([
        new SystemMessage("You are a helpful assistant. Provide clear, comprehensive answers."),
        new HumanMessage(prompt),
      ]);

      return {
        current_step: "answer_generated",
        final_answer: result.content as string,
      };
    };

    // Router function to determine the next step
    const router = (state: GraphStateType): string => {
      switch (state.current_step) {
        case "question_analyzed":
          return state.needs_search ? "search" : "analyze";
        case "search_completed":
          return "analyze";
        case "analysis_completed":
          return "generate_answer";
        case "answer_generated":
          return END;
        default:
          return "analyze_question";
      }
    };

    // Build the StateGraph
    const workflow = new StateGraph(GraphState)
      .addNode("analyze_question", analyzeQuestion)
      .addNode("search", performSearch)
      .addNode("analyze", analyzeInformation)
      .addNode("generate_answer", generateAnswer)
      .addEdge(START, "analyze_question")
      .addConditionalEdges("analyze_question", router, {
        search: "search",
        analyze: "analyze",
      })
      .addConditionalEdges("search", router, {
        analyze: "analyze",
      })
      .addConditionalEdges("analyze", router, {
        generate_answer: "generate_answer",
      })
      .addConditionalEdges("generate_answer", router, {
        [END]: END,
      });

    // Compile the graph with memory and callbacks
    const memory = new MemorySaver();
    const app = workflow
      .compile({ checkpointer: memory })
      .withConfig({ callbacks: [langWatchCallback] });

    // Create initial state
    const initialState: GraphStateType = {
      question: userQuestion,
      current_step: "start",
      needs_search: false,
      search_results: "",
      analysis: "",
      final_answer: "",
      iterations: 0,
    };

    // Execute the workflow
    const config = {
      configurable: { thread_id: "langgraph-user" },
    };

    let finalState: GraphStateType = initialState;

    // Stream through each node execution
    for await (const step of await app.stream(initialState, config)) {
      const nodeNames = Object.keys(step);

      // Update final state with all node outputs
      for (const nodeName of nodeNames) {
        const nodeOutput = (step as any)[nodeName];
        if (nodeOutput && typeof nodeOutput === "object") {
          finalState = { ...finalState, ...nodeOutput };
        }
      }
    }

    return finalState.final_answer;
  });
}

async function mainCallback() {
  if (!process.env.OPENAI_API_KEY) {
    console.log("OPENAI_API_KEY not set. Skipping LangGraph callback example.");
    return;
  }

  const response = await handleWorkflowWithCallback("What is LangGraph? Explain briefly.");
  console.log(`AI (LangGraph): ${response}`);
}

mainCallback().catch(console.error);
```

**How it Works:**
- `setupObservability()`: Initializes LangWatch with default configuration.
- `getLangWatchTracer()`: Creates a tracer instance for your application.
- `tracer.withActiveSpan()`: Creates a parent LangWatch trace that encompasses the entire workflow.
- `LangWatchCallbackHandler`: Captures LangGraph node executions and LangChain events, converting them into detailed LangWatch spans.
- `StateGraph`: Defines the workflow structure with nodes and conditional edges.
- `app.stream()`: Executes the workflow with streaming support for real-time tracing.
- The callback handler is passed to both individual LangChain components and the compiled graph.

**Key points:**
- Provides detailed tracing of each node execution within the state graph.
- Captures routing decisions and workflow patterns.
- Works with all LangGraph execution methods (`invoke`, `stream`, `batch`).
- Automatically handles span lifecycle management with `withActiveSpan()`.

<Note>
### Why Use the LangWatch LangChain Callback Handler with LangGraph?

The **LangWatch LangChain Callback Handler** provides comprehensive tracing for LangGraph applications by capturing both the LangGraph workflow structure and the underlying LangChain operations. This gives you complete visibility into your multi-step AI workflows.
</Note>

## Advanced Patterns

### Node-Level Tracing

You can add custom tracing to individual nodes for more detailed observability:

```typescript
const analyzeQuestion = async (state: GraphStateType) => {
  return await tracer.withActiveSpan("Analyze Question Node", {
    attributes: {
      "node.type": "analyzer",
      "node.input.question": state.question,
    },
  }, async (span) => {
    const result = await chatModel.invoke([
      new SystemMessage("You are a question analyzer."),
      new HumanMessage(`Analyze: ${state.question}`),
    ]);

    const needsSearch = (result.content as string).toUpperCase().includes("YES");

    span.setAttributes({
      "node.output.needs_search": needsSearch,
    });

    return {
      current_step: "question_analyzed",
      needs_search: needsSearch,
    };
  });
};
```

## Common Mistakes and Caveats

### 1. Setup and Initialization Issues

<Warning>
**Multiple setup calls**: `setupObservability()` can only be called once per process. Subsequent calls will throw an error.
</Warning>

```typescript
// ❌ Wrong - Multiple setup calls
setupObservability();
setupObservability(); // This will throw an error

// ✅ Correct - Single setup call
setupObservability();
```

### 2. Callback Handler Usage

<Warning>
**Reusing callback handlers**: Each workflow execution should use a fresh `LangWatchCallbackHandler` instance to avoid span conflicts.
</Warning>

```typescript
// ❌ Wrong - Reusing callback handler across workflows
const callback = new LangWatchCallbackHandler();

async function processMultipleWorkflows() {
  // This can cause span conflicts
  const app1 = workflow1.compile().withConfig({ callbacks: [callback] });
  const app2 = workflow2.compile().withConfig({ callbacks: [callback] });
}

// ✅ Correct - Fresh callback handler per workflow execution
async function processMultipleWorkflows() {
  const callback1 = new LangWatchCallbackHandler();
  const callback2 = new LangWatchCallbackHandler();

  const app1 = workflow1.compile().withConfig({ callbacks: [callback1] });
  const app2 = workflow2.compile().withConfig({ callbacks: [callback2] });
}
```

### 3. State Management

<Warning>
**State mutation**: Avoid directly mutating state objects in LangGraph nodes. Always return new state objects.
</Warning>

```typescript
// ❌ Wrong - Mutating state directly
const analyzeQuestion = async (state: GraphStateType) => {
  state.current_step = "question_analyzed"; // Don't mutate
  state.needs_search = true; // Don't mutate
  return state;
};

// ✅ Correct - Return new state object
const analyzeQuestion = async (state: GraphStateType) => {
  return {
    ...state,
    current_step: "question_analyzed",
    needs_search: true,
  };
};
```

### 4. Error Handling in Workflows

<Warning>
**Unhandled errors in nodes**: Always handle errors in individual nodes to prevent workflow crashes.
</Warning>

```typescript
// ❌ Wrong - No error handling in nodes
const analyzeQuestion = async (state: GraphStateType) => {
  const result = await chatModel.invoke([...]); // May throw
  return { current_step: "question_analyzed" };
};

// ✅ Correct - Proper error handling
const analyzeQuestion = async (state: GraphStateType) => {
  try {
    const result = await chatModel.invoke([...]);
    return { current_step: "question_analyzed" };
  } catch (error) {
    console.error("Error in analyzeQuestion:", error);
    return {
      current_step: "error",
      error: error.message
    };
  }
};
```

### 5. Environment Configuration

<Warning>
**Missing environment variables**: Ensure all required environment variables are set before running your LangGraph application.
</Warning>

```typescript
// ❌ Wrong - No environment validation
setupObservability();
const chatModel = new ChatOpenAI(); // May fail if OPENAI_API_KEY not set

// ✅ Correct - Environment validation
if (!process.env.OPENAI_API_KEY) {
  console.error("OPENAI_API_KEY environment variable is required");
  process.exit(1);
}

setupObservability();
const chatModel = new ChatOpenAI();
```

<Info>
### Best Practices Summary

1. **Call `setupObservability()` only once per process**
2. **Use fresh callback handlers** for each workflow execution to avoid conflicts
3. **Avoid state mutation** - always return new state objects from nodes
4. **Handle errors properly** in individual nodes and workflows
5. **Validate environment variables** before starting your application
</Info>

## Example Project

You can find a complete example project demonstrating LangGraph integration with LangWatch [on our GitHub](https://github.com/langwatch/langwatch/tree/main/typescript-sdk/examples/langgraph). This example includes:

- **Basic Chatbot**: A simple chatbot that handles conversation flow using LangGraph
- **State Management**: Proper state graph management and workflow patterns
- **Conversation Management**: User input handling and conversation history management
- **Error Handling**: Comprehensive error handling and exit commands
- **Full LangWatch Integration**: Complete observability and tracing setup

### Key Features

- **Automatic Tracing**: All LangGraph node executions and workflow patterns are automatically traced
- **State Graph Visualization**: Demonstrates proper state graph construction and execution
- **Workflow Patterns**: Shows how to build complex multi-step AI workflows
- **Node-Level Observability**: Detailed tracing of individual node executions
- **Error Recovery**: Handles errors gracefully with proper cleanup

## Related Documentation

For more advanced LangGraph integration patterns and best practices:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management for LangGraph workflows
- **[Capturing RAG](/integration/typescript/tutorials/capturing-rag)** - RAG patterns within LangGraph workflows
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - LangGraph-specific attributes and conventions
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug LangGraph integration issues

<Tip>
LangGraph's workflow tracing works well with [Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation) for custom nodes and [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent attribute naming across your workflow.
</Tip>

---

# FILE: ./integration/typescript/integrations/mastra.mdx

---
title: Mastra
description: Learn how to integrate Mastra, a TypeScript agent framework, with LangWatch for observability and tracing.
sidebarTitle: Mastra
keywords: mastra, langwatch, tracing, observability, typescript, agent framework, ai agents
---

# Observability for Mastra With LangWatch

This guide shows you how to integrate **Mastra** with **LangWatch** for observability and tracing. By following these steps, you'll be able to monitor and debug your Mastra agents in the LangWatch dashboard.

## Integration

<Steps>
<Step title="Create a Mastra project">
Create a Mastra project using the Mastra CLI:

```bash
npx create-mastra
```

Move into the project directory:

```bash
cd your-mastra-project
```

For more information, view Mastra installation instructions [here](https://mastra.ai/docs/getting-started/installation)
</Step>

<Step title="Set up LangWatch project">
Create a project in [LangWatch](https://app.langwatch.ai) and get your API keys from the project settings page.
</Step>

<Step title="Add environment variables">
Create or update your `.env` file with the following variables:

```bash
# Your LLM API key
OPENAI_API_KEY=your-api-key

# LangWatch credentials
LANGWATCH_API_KEY=sk-...
```
</Step>

<Step title="Install required packages">
Add the necessary packages to your project:

```bash
npm install langwatch @opentelemetry/context-async-hooks @opentelemetry/sdk-node
```
</Step>

<Step title="Set up LangWatch observability">
Set up LangWatch observability in your main application file using `setupObservability`:

```typescript
import { setupObservability } from "langwatch/observability/node";

setupObservability();
```
</Step>

<Step title="Configure your Mastra instance">
Configure your Mastra instance with telemetry enabled:

```typescript
import { Mastra } from '@mastra/core/mastra';
import { PinoLogger } from '@mastra/loggers';
import { LibSQLStore } from '@mastra/libsql';
import { weatherAgent } from './agents/weather-agent.js';

export const mastra = new Mastra({
  agents: { weatherAgent },
  storage: new LibSQLStore({
    url: ":memory:", // or "file:./mastra.db" for persistence
  }),
  logger: new PinoLogger({
    name: 'Mastra',
    level: 'info',
  }),
  telemetry: {
    enabled: true,
  },
});
```
</Step>

<Step title="Add tracing to your agent calls">
Use the LangWatch tracer to add detailed tracing to your agent interactions:

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("mastra-weather-agent-example");

// In your agent interaction code
await tracer.withActiveSpan("agent-interaction", {
  attributes: {
    "langwatch.thread_id": threadId,
    "langwatch.tags": ["mastra.sdk.example"],
  },
}, async (span) => {
  // Set input for tracing
  span.setInput("chat_messages", conversationHistory);

  const agent = mastra.getAgent("weatherAgent");
  const response = await agent.generate(conversationHistory, {
    // Optionally provide a tracer to have more control over tracing
    telemetry: { isEnabled: true, tracer: tracer },
  });

  // Set output for tracing
  span.setOutput("chat_messages", [{ role: "assistant", content: response.text }]);
});
```
</Step>

<Step title="Run your Mastra application">
Start your Mastra development server:

```bash
npm run dev
```

Or run your application:

```bash
npm start
```

<Check>
Visit your [LangWatch dashboard](https://app.langwatch.ai) to explore detailed insights into your agent interactions. Monitor and analyze every aspect of your AI conversations, from prompt engineering to response quality, helping you optimize your AI applications.
</Check>
</Step>
</Steps>

## Example Project

You can find a complete example project demonstrating Mastra integration with LangWatch [on our GitHub](https://github.com/langwatch/langwatch/tree/main/typescript-sdk/examples/mastra). This example includes:

- **Weather Agent**: An AI agent that fetches weather data and suggests activities
- **Weather Tool**: A tool that fetches real-time weather data from Open-Meteo API
- **CLI Chatbox Interface**: Interactive command-line interface for chatting with the weather agent
- **Workflow Example**: Demonstrates Mastra workflows for programmatic weather data fetching
- **Full LangWatch Integration**: Complete observability and tracing setup

### Key Features

- **Automatic Tracing**: All agent interactions are automatically traced and sent to LangWatch
- **Custom Spans**: Create custom spans for detailed monitoring of specific operations
- **Input/Output Tracking**: Track conversation history and agent responses
- **Thread Management**: Organize conversations by thread ID for better analysis
- **Tagging**: Add custom tags to categorize and filter your traces
- **Tool Integration**: Demonstrates how to trace custom tools and their usage
- **Workflow Patterns**: Shows how to build and trace complex agent workflows

## Related Documentation

For more advanced Mastra integration patterns and best practices:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management for Mastra operations
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - Mastra-specific attributes and conventions
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug Mastra integration issues
- **[Capturing Metadata](/integration/typescript/tutorials/capturing-metadata)** - Adding custom metadata to Mastra calls

<Tip>
For production Mastra applications, combine manual instrumentation with [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent observability and better analytics.
</Tip>

---

# FILE: ./integration/typescript/integrations/open-ai.mdx

---
title: OpenAI
sidebarTitle: TypeScript/JS
description: LangWatch OpenAI TypeScript integration guide
icon: square-js
keywords: openai, langwatch, typescript, javascript, sdk, instrumentation, opentelemetry
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

<LLMsTxtProtip />

<Prerequisites />

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Integration

Start by setting up observability and initializing the LangWatch tracer:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability first
setupObservability();

const tracer = getLangWatchTracer("my-service");
```

Then to capture your LLM calls, you can use the `withActiveSpan` method to create an LLM span with automatic lifecycle management:

```typescript
import { OpenAI } from "openai";

// Model to be used and messages that will be sent to the LLM
const model = "gpt-5-mini";
const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
];

const openai = new OpenAI();

// Use withActiveSpan for automatic error handling and span cleanup
const result = await tracer.withActiveSpan("llm-call", async (span) => {
  // Set span type and input
  span.setType("llm");
  span.setInput("chat_messages", messages);
  span.setRequestModel(model);

  // Make the OpenAI call
  const chatCompletion = await openai.chat.completions.create({
    messages: messages,
    model: model,
  });

  // Set output and metrics
  span.setOutput("chat_messages", [chatCompletion.choices[0]!.message]);
  span.setMetrics({
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  });

  return chatCompletion;
});
```

The `withActiveSpan` method automatically:
- Creates the span with the specified name
- Handles errors and sets appropriate span status
- Ends the span when the function completes
- Returns the result of your async function

## Community Auto-Instrumentation

For automatic instrumentation without manual span creation, you can use the [OpenInference instrumentation for OpenAI](https://github.com/Arize-ai/openinference/tree/main/js/packages/openinference-instrumentation-openai):

<Steps>
<Step title="Install the OpenInference instrumentation">
  ```bash
  npm install @arizeai/openinference-instrumentation-openai
  ```
</Step>

<Step title="Register the instrumentation">
  ```typescript
  import { NodeSDK } from "@opentelemetry/sdk-node";
  import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";
  import { setupObservability } from "langwatch/observability/node";

  // Setup observability with the instrumentation
  setupObservability({
    instrumentations: [new OpenAIInstrumentation()],
  });
  ```
</Step>

<Step title="Use OpenAI normally">
  ```typescript
  import { OpenAI } from "openai";

  const openai = new OpenAI();

  // This call will be automatically instrumented
  const completion = await openai.chat.completions.create({
    model: "gpt-5-mini",
    messages: [{ role: "user", content: "Hello!" }],
  });
  ```
</Step>
</Steps>

<Info>
The OpenInference instrumentation automatically captures:
- Input messages and model configuration
- Output responses and token usage
- Error handling and status codes
- Request/response timing
</Info>

<Warning>
When using auto-instrumentation, you may need to configure data capture settings to control what information is sent to LangWatch.
</Warning>

<Note>
On short-live environments like Lambdas or Serverless Functions, be sure to call <br /> `await trace.sendSpans();` to wait for all pending requests to be sent before the runtime is destroyed.
</Note>

## Capture a RAG Span

Appart from LLM spans, another very used type of span is the RAG span. This is used to capture the retrieved contexts from a RAG that will be used by the LLM, and enables a whole new set of RAG-based features evaluations for RAG quality on LangWatch.

<TypeScriptRAG />

## Capture an arbritary Span

You can also use generic spans to capture any type of operation, its inputs and outputs, for example for a function call:

<TypeScriptCaptureSpans />

## Capturing Exceptions

To capture also when your code throws an exception, you can simply wrap your code around a try/catch, and update or end the span with the exception:

<TypeScriptExceptions />

## Capturing custom evaluation results

[LangWatch Evaluators](/llm-evaluation/list) can run automatically on your traces, but if you have an in-house custom evaluator, you can also capture the evaluation
results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

<TypeScriptCustomEvaluation />


## Related Documentation

For more advanced OpenAI integration patterns and best practices:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management for OpenAI calls
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - OpenAI-specific attributes and conventions
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug OpenAI integration issues
- **[Capturing Metadata](/integration/typescript/tutorials/capturing-metadata)** - Adding custom metadata to OpenAI calls

<Tip>
For production OpenAI applications, combine manual instrumentation with [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent observability and better analytics.
</Tip>


---

# FILE: ./integration/typescript/tutorials/capturing-metadata.mdx

---
title: Capturing Metadata and Attributes
sidebarTitle: TypeScript/JS
description: Learn how to enrich your traces and spans with custom metadata and attributes using the LangWatch TypeScript SDK.
icon: square-js
keywords: langwatch, typescript, javascript, metadata, attributes, tracing, spans, traces
---

Metadata and attributes are key-value pairs that allow you to add custom contextual information to your traces and spans. This enrichment is invaluable for debugging, analysis, filtering, and gaining deeper insights into your LLM application's behavior.

In the TypeScript SDK, all metadata is captured through **span attributes**. You can set attributes on any span to provide context for that operation or the entire trace.

This tutorial will guide you through capturing metadata using span attributes with the TypeScript SDK.

<Note>
  For a comprehensive reference of all available attributes and semantic conventions, see the [Semantic Conventions guide](/integration/typescript/tutorials/semantic-conventions).
</Note>

## Understanding Span Attributes

Span attributes provide contextual information for any span in your trace. They can be used to capture:

*   **Trace-level context**: Information that applies to the entire trace (set on the root span)
*   **Span-specific details**: Information relevant to a particular operation or step
*   **Business logic metadata**: Custom flags, parameters, or results specific to your application

### Common Use Cases

*   **User and session information**: `langwatch.user.id`, `langwatch.thread.id`
*   **Application context**: `app.version`, `environment`, `region`
*   **LLM operation details**: `gen_ai.request.model`, `gen_ai.request.temperature`, `gen_ai.response.prompt_tokens`
*   **Tool and API calls**: `tool.name`, `api.endpoint`, `response.status`
*   **RAG operations**: `retrieved.document.ids`, `chunk.count`
*   **Custom business logic**: `customer.tier`, `feature.flags`, `processing.stage`

## Setting Attributes

You can set attributes on any span using the `setAttributes()` method. This method accepts an object with key-value pairs.

### Basic Attribute Setting

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("metadata-example");

async function handleUserQuery(query: string): Promise<string> {
  return await tracer.withActiveSpan("UserQueryHandler", async (span) => {
    // Set attributes on the root span for trace-level context
    span.setAttributes({
      "langwatch.user.id": "user_123",
      "langwatch.thread.id": "session_abc",
      "app.version": "1.0.0"
    });

    // Your application logic here
    const processedQuery = `Query processed: ${query}`;

    // Add more attributes based on processing
    span.setAttributes({
      "query.language": "en",
      "processing.completed": true
    });

    return processedQuery;
  });
}

await handleUserQuery("Hello, LangWatch!");
```

### Setting Attributes on Child Spans

You can set attributes on any span in your trace hierarchy:

```typescript
async function processWithChildSpans(): Promise<void> {
  return await tracer.withActiveSpan("ParentOperation", async (parentSpan) => {
    // Set attributes on the parent span
    parentSpan.setAttributes({
      "operation.type": "batch_processing",
      "batch.size": 100
    });

    await tracer.withActiveSpan("ChildOperation", async (childSpan) => {
      // Set attributes on the child span
      childSpan.setAttributes({
        "child.operation": "data_validation",
        "validation.rules": 5
      });

      // ... logic for child operation ...

      // Add more attributes based on results
      childSpan.setAttributes({
        "validation.passed": true,
        "items.processed": 95
      });
    });
  });
}
```

## Dynamic Attribute Updates

You can update attributes at any point during span execution:

```typescript
async function dynamicAttributes(customerId: string, requestDetails: any): Promise<string> {
  return await tracer.withActiveSpan("CustomerRequestFlow", async (span) => {
    // Set initial attributes
    span.setAttributes({
      "langwatch.customer.id": customerId,
      "request.type": requestDetails.type
    });

    // Simulate processing
    console.log(`Processing request for customer ${customerId}`);

    // Update attributes based on conditions
    const isPriorityCustomer = customerId.startsWith("vip_");
    span.setAttributes({
      "customer.priority": isPriorityCustomer
    });

    // ... further processing ...

    if (requestDetails.type === "complaint") {
      span.setAttributes({
        "escalation.needed": true
      });
    }

    return "Request processed successfully";
  });
}
```

## Using Semantic Conventions

LangWatch supports OpenTelemetry semantic conventions for consistent attribute naming. You can import semantic convention attributes for type-safe attribute setting:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";
import { attributes } from "langwatch";
import * as semconv from "@opentelemetry/semantic-conventions";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("semantic-conventions-example");

async function exampleWithSemanticConventions(): Promise<void> {
  return await tracer.withActiveSpan("SemanticConventionsExample", async (span) => {
    // Use semantic convention attributes for consistency
    span.setAttributes({
      [attributes.ATTR_LANGWATCH_USER_ID]: "user-123",
      [attributes.ATTR_LANGWATCH_THREAD_ID]: "thread-456",
      [attributes.ATTR_LANGWATCH_SPAN_TYPE]: "llm",
      [semconv.ATTR_GEN_AI_REQUEST_MODEL]: "gpt-5",
      [semconv.ATTR_GEN_AI_REQUEST_TEMPERATURE]: 0.7
    });
  });
}
```

<Note>
  For a complete list of available semantic conventions and attributes, see the [Semantic Conventions guide](/integration/typescript/tutorials/semantic-conventions).
</Note>

## Advanced Attribute Patterns

### Conditional Attributes

Set attributes based on your application logic:

```typescript
async function conditionalAttributes(userId: string, isPremium: boolean): Promise<void> {
  return await tracer.withActiveSpan("ConditionalAttributes", async (span) => {
    // Always set these attributes
    span.setAttributes({
      "langwatch.user.id": userId,
      "user.type": isPremium ? "premium" : "standard"
    });

    // Conditionally set additional attributes
    if (isPremium) {
      span.setAttributes({
        "premium.features": ["priority_support", "advanced_analytics"],
        "billing.tier": "premium"
      });
    }

    // Set attributes based on processing results
    const processingTime = Date.now();
    if (processingTime > 5000) {
      span.setAttributes({
        "performance.slow": true,
        "processing.time.ms": processingTime
      });
    }
  });
}
```

### Structured Data Attributes

For complex data, you can serialize objects as JSON strings:

```typescript
async function structuredAttributes(): Promise<void> {
  return await tracer.withActiveSpan("StructuredData", async (span) => {
    const userPreferences = {
      language: "en",
      theme: "dark",
      notifications: ["email", "push"]
    };

    const systemInfo = {
      version: "1.2.3",
      environment: "production",
      region: "us-east-1"
    };

    span.setAttributes({
      "user.preferences": JSON.stringify(userPreferences),
      "system.info": JSON.stringify(systemInfo),
      "feature.flags": JSON.stringify({
        "new_ui": true,
        "beta_features": false
      })
    });
  });
}
```

### LLM-Specific Attributes

For LLM operations, you can use GenAI semantic convention attributes:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";
import * as semconv from "@opentelemetry/semantic-conventions";

async function llmAttributes(): Promise<void> {
  return await tracer.withActiveSpan("LLMOperation", async (span) => {
    span.setType("llm");
    span.setRequestModel("gpt-5");

    span.setAttributes({
      [semconv.ATTR_GEN_AI_REQUEST_MODEL]: "gpt-5",
      [semconv.ATTR_GEN_AI_REQUEST_TEMPERATURE]: 0.7,
      [semconv.ATTR_GEN_AI_REQUEST_MAX_TOKENS]: 1000,
      [semconv.ATTR_GEN_AI_REQUEST_TOP_P]: 1.0,
      [semconv.ATTR_GEN_AI_REQUEST_FREQUENCY_PENALTY]: 0.0,
      [semconv.ATTR_GEN_AI_REQUEST_PRESENCE_PENALTY]: 0.0,
      [semconv.ATTR_GEN_AI_REQUEST_STREAMING]: false
    });

    // Simulate LLM call
    const response = "Generated response";

    span.setAttributes({
      [semconv.ATTR_GEN_AI_RESPONSE_MODEL]: "gpt-5",
      [semconv.ATTR_GEN_AI_RESPONSE_PROMPT_TOKENS]: 150,
      [semconv.ATTR_GEN_AI_RESPONSE_COMPLETION_TOKENS]: 75,
      [semconv.ATTR_GEN_AI_RESPONSE_TOTAL_TOKENS]: 225,
      [semconv.ATTR_GEN_AI_RESPONSE_USAGE_TOTAL_TOKENS]: 225
    });
  });
}
```

## Best Practices

### Attribute Naming

Follow consistent naming conventions for your attributes:

```typescript
// ✅ Good: Use consistent naming patterns
span.setAttributes({
  "langwatch.user.id": "user-123",
  "request.method": "POST",
  "response.status": 200,
  "processing.time.ms": 1500
});

// ❌ Avoid: Inconsistent naming
span.setAttributes({
  "user_id": "user-123", // Inconsistent with langwatch.user.id
  "method": "POST", // Too generic
  "status": 200, // Too generic
  "time": 1500 // Missing units
});
```

### Sensitive Data

Never include sensitive information in attributes:

```typescript
// ✅ Good: Safe attributes
span.setAttributes({
  "langwatch.user.id": "user-123",
  "request.type": "authentication",
  "auth.method": "oauth"
});

// ❌ Avoid: Sensitive data in attributes
span.setAttributes({
  "api_key": "sk-...", // Never include API keys
  "password": "secret123", // Never include passwords
  "credit_card": "1234-5678-9012-3456", // Never include PII
  "session_token": "eyJ..." // Never include tokens
});
```

### Performance Considerations

Limit the number and size of attributes for performance:

```typescript
// ✅ Good: Essential attributes only
span.setAttributes({
  "langwatch.user.id": "user-123",
  [semconv.ATTR_GEN_AI_REQUEST_MODEL]: "gpt-5",
  [semconv.ATTR_GEN_AI_REQUEST_TEMPERATURE]: 0.7,
  [semconv.ATTR_GEN_AI_RESPONSE_TOTAL_TOKENS]: 150
});

// ❌ Avoid: Too many attributes
span.setAttributes({
  // ... 50+ attributes that aren't essential
});
```

### When to Set Attributes

*   **At span creation**: Set attributes that are known from the start
*   **During processing**: Update attributes as you learn more about the operation
*   **At completion**: Add final results, metrics, or status information

## Viewing in LangWatch

All captured span attributes will be visible in the LangWatch UI:
- **Root span attributes** are typically displayed in the trace details view, providing an overview of the entire operation
- **Child span attributes** are shown when you inspect individual spans within a trace

This rich contextual data allows you to:
- **Filter and search** for traces and spans based on specific attribute values
- **Analyze performance** by correlating metrics with different attributes (e.g., comparing latencies for different `langwatch.user.id`s or `gen_ai.request.model`s)
- **Debug issues** by quickly understanding the context and parameters of a failed or slow operation

## Conclusion

Effectively using span attributes is crucial for maximizing the observability of your LLM applications. By enriching your traces with relevant contextual information, you empower yourself to better understand, debug, and optimize your systems with LangWatch.

Remember to instrument your code thoughtfully, adding data that provides meaningful insights without being overly verbose. Use semantic conventions for consistency and leverage TypeScript's autocomplete support for better developer experience.

---

# FILE: ./integration/typescript/tutorials/capturing-rag.mdx

---
title: Capturing RAG
sidebarTitle: TypeScript/JS
description: Learn how to capture Retrieval Augmented Generation (RAG) data with LangWatch.
icon: square-js
keywords: RAG, Retrieval Augmented Generation, LangChain, LangWatch, LangChain RAG, RAG Span, RAG Chunk, RAG Tool
---

Retrieval Augmented Generation (RAG) is a common pattern in LLM applications where you first retrieve relevant context from a knowledge base and then use that context to generate a response. LangWatch provides specific ways to capture RAG data, enabling better observability and evaluation of your RAG pipelines.

By capturing the `contexts` (retrieved documents) used by the LLM, you unlock several benefits in LangWatch:
- Specialized RAG evaluators (e.g., Faithfulness, Context Relevancy).
- Analytics on document usage (e.g., which documents are retrieved most often, which ones lead to better responses).
- Deeper insights into the retrieval step of your pipeline.

There are two main ways to capture RAG spans: manually creating a RAG span or using framework-specific integrations like the one for LangChain.

## Manual RAG Span Creation

You can manually create a RAG span by using `tracer.withActiveSpan()` with `type: "rag"`. Inside this span, you should perform the retrieval and then update the span with the retrieved contexts.

The `contexts` should be a list of `LangWatchSpanRAGContext` objects. The `LangWatchSpanRAGContext` object allows you to provide more metadata about each retrieved chunk, such as `document_id`, `chunk_id`, and `content`.

Here's an example:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";
import type { LangWatchSpanRAGContext } from "langwatch/observability";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("rag-example");

async function generateAnswerFromContext(contexts: string[], userQuery: string): Promise<string> {
  return await tracer.withActiveSpan("GenerateAnswerFromContext", async (span) => {
    span.setType("llm");
    span.setRequestModel("gpt-5-mini");

    // Simulate LLM call using the contexts
    await new Promise(resolve => setTimeout(resolve, 500));
    const response = `Based on the context, the answer to '${userQuery}' is...`;

    // You can update the LLM span with model details, token counts, etc.
    span.setInput("text", `Contexts: ${contexts.join(", ")}\nQuery: ${userQuery}`);
    span.setOutput("text", response);

    return response;
  });
}

async function performRAG(userQuery: string): Promise<string> {
  return await tracer.withActiveSpan("My Custom RAG Process", async (span) => {
    span.setType("rag");

    // 1. Retrieve contexts
    // Simulate retrieval from a vector store or other source
    await new Promise(resolve => setTimeout(resolve, 300));
    const retrievedDocs = [
      "LangWatch helps monitor LLM applications.",
      "RAG combines retrieval with generation for better answers.",
      "TypeScript is a popular language for AI development."
    ];

    // Update the current RAG span with the retrieved contexts
    // You can pass a list of strings directly
    const ragContexts: LangWatchSpanRAGContext[] = retrievedDocs.map((content, index) => ({
      document_id: `doc${index + 1}`,
      chunk_id: `chunk${index + 1}`,
      content
    }));

    span.setRAGContexts(ragContexts);

    // Alternatively, for simpler context information:
    // span.setRAGContexts(retrievedDocs.map(content => ({
    //   document_id: "unknown",
    //   chunk_id: "unknown",
    //   content
    // })));

    // 2. Generate answer using the contexts
    const finalAnswer = await generateAnswerFromContext(contexts: retrievedDocs, userQuery: userQuery);

    // The RAG span automatically captures its input (userQuery) and output (finalAnswer)
    // if dataCapture is not set to "none".
    return finalAnswer;
  });
}

async function handleUserQuestion(question: string): Promise<string> {
  return await tracer.withActiveSpan("User Question Handler", async (span) => {
    span.setInput("text", question);
    span.setAttributes({ "user_id": "example_user_123" });

    const answer = await performRAG(userQuery: question);

    span.setOutput("text", answer);
    return answer;
  });
}

// Example usage
async function main() {
  const userQuestion = "What is LangWatch used for?";
  const response = await handleUserQuestion(userQuestion);
  console.log(`Question: ${userQuestion}`);
  console.log(`Answer: ${response}`);
}

main().catch(console.error);
```

In this example:
1.  `performRAG` uses `tracer.withActiveSpan()` with `type: "rag"`.
2.  Inside `performRAG`, we simulate a retrieval step.
3.  `span.setRAGContexts(ragContexts)` is called to explicitly log the retrieved documents.
4.  The generation step (`generateAnswerFromContext`) is called, which itself can be another span (e.g., an LLM span).

## Advanced RAG Patterns

### Multiple Retrieval Sources

You can capture RAG contexts from multiple sources in a single span:

```typescript
async function multiSourceRAG(query: string): Promise<string> {
  return await tracer.withActiveSpan("Multi-Source RAG", async (span) => {
    span.setType("rag");

    // Simulate retrieval from multiple sources
    const vectorStoreContexts: LangWatchSpanRAGContext[] = [
      {
        document_id: "vector_doc_1",
        chunk_id: "vector_chunk_1",
        content: "Information from vector store"
      }
    ];

    const databaseContexts: LangWatchSpanRAGContext[] = [
      {
        document_id: "db_doc_1",
        chunk_id: "db_chunk_1",
        content: "Information from database"
      }
    ];

    const apiContexts: LangWatchSpanRAGContext[] = [
      {
        document_id: "api_doc_1",
        chunk_id: "api_chunk_1",
        content: "Information from API"
      }
    ];

    // Combine all contexts
    const allContexts = [
      ...vectorStoreContexts,
      ...databaseContexts,
      ...apiContexts
    ];

    span.setRAGContexts(allContexts);

    // Generate response using all contexts
    const response = `Based on ${allContexts.length} sources: ${query}`;
    return response;
  });
}
```

### RAG with Metadata

You can include additional metadata in your RAG contexts:

```typescript
async function ragWithMetadata(query: string): Promise<string> {
  return await tracer.withActiveSpan("RAG with Metadata", async (span) => {
    span.setType("rag");

    const contexts: LangWatchSpanRAGContext[] = [
      {
        document_id: "doc_123",
        chunk_id: "chunk_456",
        content: "Relevant content here"
      }
    ];

    // Add additional metadata to the span
    span.setAttributes({
      "rag.source": "vector_store",
      "rag.retrieval_method": "semantic_search",
      "rag.top_k": 5,
      "rag.threshold": 0.7
    });

    span.setRAGContexts(contexts);

    const response = `Based on the retrieved context: ${query}`;
    return response;
  });
}
```

## Error Handling

When working with RAG operations, it's important to handle errors gracefully and capture error information in your spans:

```typescript
async function robustRAGRetrieval(query: string): Promise<LangWatchSpanRAGContext[]> {
  return await tracer.withActiveSpan("Robust RAG Retrieval", async (span) => {
    span.setType("rag");
    span.setInput("text", query);

    try {
      // Simulate retrieval that might fail
      const retrievedContexts: LangWatchSpanRAGContext[] = [
        {
          document_id: "doc_123",
          chunk_id: "chunk_456",
          content: "Relevant information from document 123"
        }
      ];

      span.setRAGContexts(retrievedContexts);
      span.setOutput("json", { status: "success", count: retrievedContexts.length });

      return retrievedContexts;
    } catch (error) {
      // Capture error information in the span
      span.setOutput("json", {
        status: "error",
        error_message: error instanceof Error ? error.message : String(error),
        error_type: error instanceof Error ? error.constructor.name : typeof error
      });

      // Re-throw the error (withActiveSpan will automatically mark the span as ERROR)
      throw error;
    }
  });
}
```

## Best Practices

1. **Use Descriptive Span Names**: Name your RAG spans clearly to identify the retrieval method or source.
2. **Include Metadata**: Add relevant attributes like retrieval method, thresholds, or source information.
3. **Handle Errors Gracefully**: Wrap RAG operations in try-catch blocks and capture error information.
4. **Optimize Context Size**: Be mindful of the size of context content to avoid performance issues.
5. **Use Consistent Document IDs**: Use consistent naming conventions for document and chunk IDs.
6. **Control Data Capture**: Use data capture configuration to manage what gets captured in sensitive operations.

By effectively capturing RAG spans, you gain much richer data in LangWatch, enabling more powerful analysis and evaluation of your RAG systems. Refer to the SDK examples for more detailed implementations.

## Related Documentation

For more advanced RAG patterns and framework-specific implementations:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Advanced span management for RAG pipelines
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - RAG-specific attributes and naming conventions
- **[LangChain Integration](/integration/typescript/integrations/langchain)** - Automatic RAG instrumentation with LangChain
- **[Capturing Metadata](/integration/typescript/tutorials/capturing-metadata)** - Adding custom metadata to RAG contexts

<Tip>
For production RAG applications, combine manual RAG spans with [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent observability and better analytics.
</Tip>

---

# FILE: ./integration/typescript/tutorials/semantic-conventions.mdx

---
title: "Semantic Conventions"
sidebarTitle: "Semantic Conventions"
description: "Learn about OpenTelemetry semantic conventions and LangWatch's custom attributes for consistent observability"
keywords: langwatch, typescript, sdk, guide, observability, attributes, semantic conventions, opentelemetry, standards, naming
---

# Semantic Conventions

This guide covers OpenTelemetry semantic conventions and how LangWatch implements them, along with our custom attributes for LLM-specific observability.

<CardGroup cols={2}>
<Card title="OpenTelemetry Standards" icon="standards" href="#opentelemetry-semantic-conventions">
  Understand the OpenTelemetry semantic conventions that LangWatch follows for consistent observability.
</Card>

<Card title="LangWatch Attributes" icon="attributes" href="#langwatch-custom-attributes">
  Explore LangWatch's custom attributes designed specifically for LLM applications and AI observability.
</Card>
</CardGroup>

## What Are Semantic Conventions?

Semantic conventions are standardized naming and structure guidelines for observability data. They ensure consistency across different systems and make it easier to analyze and correlate data from various sources.

<Info>
OpenTelemetry semantic conventions provide a standardized way to name attributes, events, and other observability data, making it easier to build tools and dashboards that work across different applications and services. For practical examples of these conventions in action, see [Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation).
</Info>

### Benefits of Semantic Conventions

- **Consistency**: Standardized naming across all your services
- **Interoperability**: Works with any OpenTelemetry-compatible tool
- **Analytics**: Easier to build dashboards and alerts
- **Debugging**: Familiar patterns make troubleshooting faster
- **Team Collaboration**: Shared understanding of observability data

## OpenTelemetry Semantic Conventions

LangWatch fully implements OpenTelemetry semantic conventions, ensuring your traces are compatible with any OpenTelemetry-compatible observability platform.

### Core Semantic Conventions

The OpenTelemetry specification defines conventions for common observability scenarios. LangWatch supports all OpenTelemetry semantic conventions while also providing its own custom attributes for LLM-specific observability.

```typescript
import * as semconv from "@opentelemetry/semantic-conventions";
// Or for bleeding edge attributes, you can import from the `incubating` module
import * as semconv from "@opentelemetry/semantic-conventions/incubating";

// Resource attributes (service information)
const resourceAttributes = {
  [semconv.ATTR_SERVICE_NAME]: "my-ai-service",
  [semconv.ATTR_SERVICE_VERSION]: "1.0.0",
  [semconv.ATTR_DEPLOYMENT_ENVIRONMENT_NAME]: "production",
  [semconv.ATTR_HOST_NAME]: "server-01",
  [semconv.ATTR_PROCESS_PID]: process.pid,
};
```

### Span Types and Attributes

OpenTelemetry defines standard span types and their associated attributes. LangWatch extends these with custom span types for LLM operations:

```typescript
// HTTP client span (OpenTelemetry standard)
span.setAttributes({
  "http.method": "GET",
  "http.url": "https://api.example.com/data",
  "http.status_code": 200,
  "http.request.header.user_agent": "MyApp/1.0",
});

// Database span (OpenTelemetry standard)
span.setAttributes({
  "db.system": "mysql",
  "db.name": "production_db",
  "db.operation": "INSERT",
  "db.statement": "INSERT INTO users (name, email) VALUES (?, ?)",
});

// LLM span (LangWatch custom)
span.setType("llm");
span.setAttributes({
  "langwatch.user.id": "user-123",
  "langwatch.thread.id": "thread-456",
  "langwatch.streaming": false,
});
```

## TypeScript Autocomplete Support

All attribute setting methods in LangWatch provide full TypeScript autocomplete support,
you don't need to import anything, just use the attribute names directly and autocomplete
will appear in your editor.

### Autocomplete in Span Methods

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

await tracer.withActiveSpan("llm-operation", async (span) => {
  // TypeScript autocomplete works for all LangWatch attributes
  span.setAttributes({
    // Autocomplete shows all available attributes
    "code.function": "getLangWatchTracer",
    "langwatch.span.type": "llm",
    "langwatch.user.id": "user-123",
    "langwatch.thread.id": "thread-456",
    "langwatch.streaming": false,
    // ... more attributes with autocomplete
  });
});
```

### Autocomplete in Configuration

```typescript
import { setupObservability } from "langwatch/observability/node";
import { attributes } from "langwatch";

const handle = await setupObservability({
  serviceName: "my-service",
  attributes: {
    // Autocomplete shows all available LangWatch attributes
    "langwatch.sdk.version": "1.0.0",
    "langwatch.sdk.name": "langwatch-typescript",
    "langwatch.sdk.language": "typescript",
  }
});
```

## LangWatch Attributes Reference

LangWatch provides a comprehensive set of custom attributes for LLM-specific observability. All attributes are available with TypeScript autocomplete support.

### Core LangWatch Attributes

| Attribute | Type | Description | Example |
|-----------|------|-------------|---------|
| `langwatch.span.type` | string | Type of span being traced | `"llm"`, `"rag"`, `"prompt"` |
| `langwatch.user.id` | string | User identifier | `"user-123"` |
| `langwatch.thread.id` | string | Conversation thread identifier | `"thread-456"` |
| `langwatch.customer.id` | string | Customer identifier | `"customer-789"` |
| `langwatch.streaming` | boolean | Whether the operation involves streaming | `true`, `false` |
| `langwatch.input` | string/object | Input data for the span | `"Hello, how are you?"` |
| `langwatch.output` | string/object | Output data from the span | `"I'm doing well, thank you!"` |
| `langwatch.contexts` | array | RAG contexts for retrieval-augmented generation | Array of document contexts |
| `langwatch.tags` | array | Tags for categorizing spans | `["chat", "greeting"]` |
| `langwatch.params` | object | Parameter data for operations | `{ temperature: 0.7 }` |
| `langwatch.metrics` | object | Custom metrics data | `{ response_time: 1250 }` |
| `langwatch.timestamps` | object | Timing information for events | `{ start: 1234567890 }` |
| `langwatch.evaluation.custom` | object | Custom evaluation data | `{ score: 0.95 }` |

### SDK Information Attributes

| Attribute | Type | Description | Example |
|-----------|------|-------------|---------|
| `langwatch.sdk.name` | string | LangWatch SDK implementation name | `"langwatch-typescript"` |
| `langwatch.sdk.version` | string | Version of the LangWatch SDK | `"1.0.0"` |
| `langwatch.sdk.language` | string | Programming language of the SDK | `"typescript"` |

### Prompt Management Attributes

| Attribute | Type | Description | Example |
|-----------|------|-------------|---------|
| `langwatch.prompt.id` | string | Unique prompt identifier | `"prompt-123"` |
| `langwatch.prompt.handle` | string | Human-readable prompt handle | `"customer-support-greeting"` |
| `langwatch.prompt.version.id` | string | Prompt version identifier | `"version-456"` |
| `langwatch.prompt.version.number` | number | Prompt version number | `2` |
| `langwatch.prompt.selected.id` | string | Selected prompt from a set | `"selected-prompt-789"` |
| `langwatch.prompt.variables` | object | Variables used in prompt templates | `{ customer_name: "John" }` |

### LangChain Integration Attributes

| Attribute | Type | Description | Example |
|-----------|------|-------------|---------|
| `langwatch.langchain.run.id` | string | LangChain run identifier | `"run-123"` |
| `langwatch.langchain.run.type` | string | Type of LangChain run | `"chain"`, `"tool"` |
| `langwatch.langchain.run.parent.id` | string | Parent run identifier | `"parent-run-456"` |
| `langwatch.langchain.event_name` | string | LangChain event type | `"chain_start"` |
| `langwatch.langchain.run.metadata` | object | Run metadata | `{ model: "gpt-5-mini" }` |
| `langwatch.langchain.run.extra_params` | object | Additional run parameters | `{ max_tokens: 1000 }` |
| `langwatch.langchain.run.tags` | array | Run-specific tags | `["production", "chain"]` |
| `langwatch.langchain.tags` | array | LangChain operation tags | `["langchain", "llm"]` |

## Best Practices

### Attribute Naming

Follow these conventions for consistent observability:

```typescript
// ✅ Good: Use LangWatch semantic convention attributes
span.setAttributes({
  "langwatch.span.type": "llm",
  "langwatch.user.id": "user-123",
  "langwatch.thread.id": "thread-456",
});

// ❌ Avoid: Custom attribute names without conventions
span.setAttributes({
  "span_type": "llm", // Use correct values or attributes.ATTR_LANGWATCH_SPAN_TYPE instead
  "user": "user-123", // Use correct values or attributes.ATTR_LANGWATCH_USER_ID instead
});
```

### Attribute Values

Use appropriate data types and formats:

```typescript
// ✅ Good: Proper data types
span.setAttributes({
  "langwatch.streaming": false, // boolean
  "langwatch.user.id": "user-123", // string
  "langwatch.prompt.version.number": 2, // number
  "langwatch.tags": ["chat", "greeting"], // array
});

// ❌ Avoid: Inconsistent data types
span.setAttributes({
  "langwatch.streaming": "false", // string instead of boolean
  "langwatch.prompt.version.number": "2", // string instead of number
});
```

### Sensitive Data

Never include sensitive information in attributes:

```typescript
// ✅ Good: Safe attributes
span.setAttributes({
  "langwatch.user.id": "user-123",
  "langwatch.span.type": "llm",
  "langwatch.sdk.version": "1.0.0",
});

// ❌ Avoid: Sensitive data in attributes
span.setAttributes({
  [attributes.ATTR_LANGWATCH_USER_ID]: "user-123",
  "api_key": "sk-...", // Never include API keys
  "password": "secret123", // Never include passwords
  "credit_card": "1234-5678-9012-3456", // Never include PII
});
```

### Performance Considerations

Limit the number and size of attributes for performance:

| ✅ Good | ❌ Avoid | Reason |
|---------|----------|---------|
| 4-8 attributes per span | 50+ attributes | Too many impacts performance |
| Short string values | Large text content | Use `span.setInput()` for large content |
| Structured data | Nested objects | Keep attributes simple |
| Essential metadata | Redundant information | Only include what's needed |

## Summary

Semantic conventions provide a standardized approach to observability data that:

- **Ensures consistency** across your entire application
- **Enables interoperability** with OpenTelemetry-compatible tools
- **Improves debugging** with familiar patterns
- **Supports team collaboration** with shared understanding

LangWatch implements both OpenTelemetry semantic conventions and custom LLM-specific attributes, all with full TypeScript autocomplete support to help you use the right attributes consistently.

<Check>
**Key takeaways**:
- Use semantic convention attributes for consistency
- Import `attributes` from LangWatch for autocomplete
- Follow OpenTelemetry standards for interoperability
- Leverage LangWatch's LLM-specific attributes for AI observability
</Check>

## Related Documentation

For practical examples and advanced usage patterns:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Practical examples of semantic conventions in action
- **[API Reference](/integration/typescript/reference)** - Complete API documentation with attribute details
- **[Framework Integrations](/integration/typescript/integrations)** - Framework-specific semantic conventions
- **[Capturing RAG](/integration/typescript/tutorials/capturing-rag)** - RAG-specific attributes and conventions

<Tip>
Use semantic conventions consistently across your application for better analytics, debugging, and team collaboration. Start with the [Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation) tutorial to see these conventions in practice.
</Tip>

---

# FILE: ./integration/typescript/tutorials/opentelemetry-migration.mdx

---
title: OpenTelemetry Migration
description: "Migrate from OpenTelemetry to LangWatch while preserving all your custom configurations"
---

# OpenTelemetry Migration

This guide covers migrating from existing OpenTelemetry setups to LangWatch while maintaining all your custom configurations, instrumentations, and advanced features.

<CardGroup cols={2}>
<Card title="Configuration Migration" icon="migration" href="#complete-nodesdk-configuration">
  Preserve all your OpenTelemetry NodeSDK configuration options and custom settings.
</Card>

<Card title="Migration Checklist" icon="checklist" href="#migration-checklist">
  Step-by-step process to safely migrate your observability setup.
</Card>
</CardGroup>

## Overview

The LangWatch observability SDK is built on OpenTelemetry and passes through all NodeSDK configuration options, making it easy to migrate from existing OpenTelemetry setups while maintaining all your custom configuration.

<Info>
LangWatch supports all OpenTelemetry NodeSDK configuration options, so you can migrate without losing any functionality or custom settings.
</Info>

<Note>
For consistent attribute naming and semantic conventions, see our [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) guide which covers both OpenTelemetry standards and LangWatch's custom attributes.
</Note>

## Complete NodeSDK Configuration

LangWatch supports all OpenTelemetry NodeSDK configuration options:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { TraceIdRatioBasedSampler } from "@opentelemetry/sdk-trace-base";
import { HttpInstrumentation } from "@opentelemetry/instrumentation-http";
import { W3CTraceContextPropagator } from "@opentelemetry/core";
import { envDetector, processDetector, hostDetector } from "@opentelemetry/resources";

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    processorType: 'batch'
  },
  serviceName: "my-service",

  // All NodeSDK options are supported
  autoDetectResources: true,
  contextManager: undefined, // Use default
  textMapPropagator: new W3CTraceContextPropagator(),
  resourceDetectors: [envDetector, processDetector, hostDetector],

  // Sampling strategy
  sampler: new TraceIdRatioBasedSampler(0.1), // Sample 10% of traces

  // Span limits
  spanLimits: {
    attributeCountLimit: 128,
    eventCountLimit: 128,
    linkCountLimit: 128
  },

  // Auto-instrumentations
  instrumentations: [
    new HttpInstrumentation(),
    // Add other instrumentations as needed
  ],

  // Advanced options
  advanced: {
    throwOnSetupError: false, // Don't throw on setup errors
    skipOpenTelemetrySetup: false, // Handle setup yourself
    UNSAFE_forceOpenTelemetryReinitialization: false // Force reinit (dangerous)
  }
});
```

## Migration Example: From NodeSDK to LangWatch

<Steps>
<Step title="Before: Direct NodeSDK Usage">
  ```typescript
  import { NodeSDK } from "@opentelemetry/sdk-node";
  import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";
  import { JaegerExporter } from "@opentelemetry/exporter-jaeger";

  const sdk = new NodeSDK({
    serviceName: "my-service",
    spanProcessors: [
      new BatchSpanProcessor(new JaegerExporter())
    ],
    instrumentations: [new HttpInstrumentation()],
    sampler: new TraceIdRatioBasedSampler(0.1),
    spanLimits: { attributeCountLimit: 128 }
  });

  sdk.start();
  ```
</Step>

<Step title="After: Using LangWatch with Same Configuration">
  ```typescript
  import { setupObservability } from "langwatch/observability/node";
  import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";
  import { JaegerExporter } from "@opentelemetry/exporter-jaeger";

  const handle = await setupObservability({
    langwatch: {
      apiKey: process.env.LANGWATCH_API_KEY
    },
    serviceName: "my-service",
    spanProcessors: [
      new BatchSpanProcessor(new JaegerExporter())
    ],
    instrumentations: [new HttpInstrumentation()],
    sampler: new TraceIdRatioBasedSampler(0.1),
    spanLimits: { attributeCountLimit: 128 }
  });

  // Graceful shutdown
  process.on('SIGTERM', async () => {
    await handle.shutdown();
    process.exit(0);
  });
  ```
</Step>
</Steps>

## Advanced Sampling Strategies

Implement sophisticated sampling strategies for different use cases:

```typescript
import { TraceIdRatioBasedSampler, ParentBasedSampler } from "@opentelemetry/sdk-trace-base";

// Sample based on trace ID ratio
const ratioSampler = new TraceIdRatioBasedSampler(0.1); // 10% sampling

// Parent-based sampling (respect parent span sampling decision)
const parentBasedSampler = new ParentBasedSampler({
  root: ratioSampler,
  remoteParentSampled: new AlwaysOnSampler(),
  remoteParentNotSampled: new AlwaysOffSampler(),
  localParentSampled: new AlwaysOnSampler(),
  localParentNotSampled: new AlwaysOffSampler(),
});

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service",
  sampler: parentBasedSampler
});
```

## Custom Resource Detection

Configure custom resource detection for better service identification:

```typescript
import { Resource } from "@opentelemetry/resources";
import { SemanticResourceAttributes } from "@opentelemetry/semantic-conventions";

const customResource = new Resource({
  [SemanticResourceAttributes.SERVICE_NAME]: "my-service",
  [SemanticResourceAttributes.SERVICE_VERSION]: "1.0.0",
  [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV,
  "custom.team": "ai-platform",
  "custom.datacenter": "us-west-2"
});

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service",
  resource: customResource
});
```

<Tip>
For consistent attribute naming and TypeScript autocomplete support, consider using LangWatch's semantic conventions. See our [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) guide for details.
</Tip>

## Custom Instrumentations

Add custom instrumentations for specific libraries or frameworks:

```typescript
import { HttpInstrumentation } from "@opentelemetry/instrumentation-http";
import { ExpressInstrumentation } from "@opentelemetry/instrumentation-express";
import { MongoDBInstrumentation } from "@opentelemetry/instrumentation-mongodb";

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service",
  instrumentations: [
    new HttpInstrumentation({
      ignoreIncomingPaths: ['/health', '/metrics'],
      ignoreOutgoingUrls: ['https://external-service.com/health']
    }),
    new ExpressInstrumentation(),
    new MongoDBInstrumentation()
  ]
});
```

## Context Propagation Configuration

Configure custom context propagation for distributed tracing:

```typescript
import { W3CTraceContextPropagator, W3CBaggagePropagator } from "@opentelemetry/core";
import { CompositePropagator } from "@opentelemetry/core";

const compositePropagator = new CompositePropagator({
  propagators: [
    new W3CTraceContextPropagator(),
    new W3CBaggagePropagator()
  ]
});

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service",
  textMapPropagator: compositePropagator
});
```

## Environment-Specific Configuration

Create different configurations for different environments:

```typescript
const getObservabilityConfig = (environment: string) => {
  const baseConfig = {
    serviceName: "my-service",
    langwatch: {
      apiKey: process.env.LANGWATCH_API_KEY
    }
  };

  switch (environment) {
    case 'development':
      return {
        ...baseConfig,
        langwatch: {
          ...baseConfig.langwatch,
          processorType: 'simple'
        },
        debug: {
          consoleTracing: true,
          logLevel: 'debug'
        }
      };

    case 'staging':
      return {
        ...baseConfig,
        langwatch: {
          ...baseConfig.langwatch,
          processorType: 'batch'
        },
        sampler: new TraceIdRatioBasedSampler(0.5), // 50% sampling
        debug: {
          consoleTracing: false,
          logLevel: 'info'
        }
      };

    case 'production':
      return {
        ...baseConfig,
        langwatch: {
          ...baseConfig.langwatch,
          processorType: 'batch'
        },
        sampler: new TraceIdRatioBasedSampler(0.1), // 10% sampling
        debug: {
          consoleTracing: false,
          logLevel: 'warn'
        }
      };

    default:
      return baseConfig;
  }
};

const handle = await setupObservability(
  getObservabilityConfig(process.env.NODE_ENV)
);
```

## Performance Tuning

Optimize performance for high-volume applications:

```typescript
const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    processorType: 'batch'
  },
  serviceName: "my-service",

  // Performance tuning
  spanLimits: {
    attributeCountLimit: 64, // Reduce attribute count
    eventCountLimit: 32,     // Reduce event count
    linkCountLimit: 32       // Reduce link count
  },

  // Sampling for high volume
  sampler: new TraceIdRatioBasedSampler(0.05), // 5% sampling

  // Batch processing configuration
  spanProcessors: [
    new BatchSpanProcessor(new LangWatchExporter({
      apiKey: process.env.LANGWATCH_API_KEY
    }), {
      maxQueueSize: 4096,
      maxExportBatchSize: 1024,
      scheduledDelayMillis: 1000,
      exportTimeoutMillis: 30000
    })
  ]
});
```

## Migration Checklist

<Steps>
<Step title="Inventory Current Setup">
  Document all current instrumentations, exporters, and configurations in your OpenTelemetry setup.
</Step>

<Step title="Test in Development">
  Start with development environment migration to validate the configuration.
</Step>

<Step title="Verify Data Flow">
  Ensure traces are appearing in LangWatch dashboard with correct attributes and structure.
</Step>

<Step title="Performance Testing">
  Monitor application performance impact and adjust sampling/processing settings as needed.
</Step>

<Step title="Gradual Rollout">
  Migrate environments one at a time, starting with staging before production.
</Step>

<Step title="Fallback Plan">
  Keep existing OpenTelemetry setup as backup during transition period.
</Step>

<Step title="Documentation">
  Update team documentation and runbooks with new observability configuration.
</Step>
</Steps>

## Troubleshooting Migration Issues

### Common Migration Problems

<AccordionGroup>
<Accordion title="Duplicate Spans">
  **Problem**: Spans appearing twice in your traces.

  **Solution**: Ensure only one observability setup is running. Check for multiple `setupObservability` calls or conflicting OpenTelemetry initializations.
</Accordion>

<Accordion title="Missing Traces">
  **Problem**: No traces appearing in LangWatch dashboard.

  **Solution**: Verify API key configuration, check network connectivity to LangWatch endpoints, and ensure spans are being created and ended properly.
</Accordion>

<Accordion title="Performance Degradation">
  **Problem**: Application performance impacted after migration.

  **Solution**: Adjust sampling rates, optimize batch processing settings, and monitor memory usage of span processors.
</Accordion>

<Accordion title="Context Loss">
  **Problem**: Span context not propagating across async boundaries.

  **Solution**: Verify context propagation configuration and ensure proper async context management in your code.
</Accordion>

<Accordion title="Instrumentation Conflicts">
  **Problem**: Conflicting instrumentations causing errors or unexpected behavior.

  **Solution**: Review instrumentation configuration, check for duplicate instrumentations, and verify compatibility between different instrumentations.
</Accordion>
</AccordionGroup>

### Debugging Migration

Enable detailed logging during migration to identify issues:

```typescript
// Enable detailed logging during migration
const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service",
  debug: {
    consoleTracing: true,
    consoleLogging: true,
    logLevel: 'debug'
  },
  advanced: {
    throwOnSetupError: true
  }
});
```

## Migration Benefits

<CardGroup cols={2}>
<Card title="Zero Configuration Loss" icon="preserve">
  All your existing OpenTelemetry configurations, instrumentations, and custom settings are preserved.
</Card>

<Card title="Enhanced Features" icon="features">
  Gain access to LangWatch's specialized LLM observability features while keeping your existing setup.
</Card>

<Card title="Gradual Migration" icon="gradual">
  Migrate at your own pace with the ability to run both systems in parallel during transition.
</Card>

<Card title="Production Ready" icon="production">
  LangWatch is built on OpenTelemetry standards, ensuring production-grade reliability and performance.
</Card>
</CardGroup>

<Info>
The migration process is designed to be non-disruptive. You can run your existing OpenTelemetry setup alongside LangWatch during the transition period to ensure everything works correctly.
</Info>

---

# FILE: ./integration/typescript/tutorials/manual-instrumentation.mdx

---
title: "Manual Instrumentation"
sidebarTitle: "Manual Control"
description: "Learn advanced manual span management techniques for fine-grained observability control"
---

# Manual Instrumentation

This guide covers advanced manual span management techniques for TypeScript/JavaScript applications when you need fine-grained control over observability beyond the automatic `withActiveSpan` method.

<CardGroup cols={2}>
<Card title="withActiveSpan Method" icon="auto" href="#withactivespan-method">
  The recommended approach for most use cases with automatic context management and error handling.
</Card>

<Card title="Manual Span Control" icon="settings" href="#basic-manual-span-management">
  Complete manual control over span lifecycle, attributes, and context propagation.
</Card>
</CardGroup>

## withActiveSpan Method

The `withActiveSpan` method is the recommended approach for most manual instrumentation needs. It automatically handles context propagation, error handling, and span cleanup, making it both safer and easier to use than manual span management. For consistent attribute naming, combine this with [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions).

### Basic Usage

```typescript
import { getLangWatchTracer, SpanStatusCode } from "langwatch";

const tracer = getLangWatchTracer("my-service");

// Simple usage with automatic cleanup
await tracer.withActiveSpan("my-operation", async (span) => {
  span.setType("llm");
  span.setInput("Hello, world!");

  // Your business logic here
  const result = await processRequest("Hello, world!");

  span.setOutput(result);
  span.setStatus({ code: SpanStatusCode.OK });

  return result;
});
```

### Error Handling

`withActiveSpan` automatically handles errors and ensures proper span cleanup:

```typescript
await tracer.withActiveSpan("risky-operation", async (span) => {
  span.setType("external_api");
  span.setInput({ userId: "123", action: "update_profile" });

  try {
    // This might throw an error
    const result = await externalApiCall();
    span.setOutput(result);
    span.setStatus({ code: SpanStatusCode.OK });
    return result;
  } catch (error) {
    // Error is automatically recorded and span status is set to ERROR
    span.setStatus({
      code: SpanStatusCode.ERROR,
      message: error.message
    });
    span.recordException(error);
    throw error; // Re-throw to maintain error flow
  }
  // Span is automatically ended in finally block
});
```

### Context Propagation

`withActiveSpan` automatically propagates span context to child operations:

```typescript
async function processUserRequest(userId: string) {
  return await tracer.withActiveSpan("process-user-request", async (span) => {
    span.setType("user_operation");
    span.setInput({ userId });

    // Child operations automatically inherit the span context
    const userData = await fetchUserData(userId);
    const userProfile = await updateUserProfile(userId);

    const result = { userData, userProfile };
    span.setOutput(result);
    span.setStatus({ code: SpanStatusCode.OK });

    return result;
  });
}

// Child operations automatically create child spans
async function fetchUserData(userId: string) {
  return await tracer.withActiveSpan("fetch-user-data", async (span) => {
    span.setType("database_query");
    // This span is automatically a child of the parent span
    // ... database logic ...
  });
}
```

### Custom Attributes and Events

Add rich metadata to your spans:

```typescript
await tracer.withActiveSpan("custom-operation", async (span) => {
  // Set span type
  span.setType("llm");

  // Add custom attributes for filtering and analysis
  span.setAttributes({
    "custom.business_unit": "marketing",
    "custom.campaign_id": "summer-2024",
    "custom.user_tier": "premium",
    "custom.operation_type": "batch_processing",
    "llm.model": "gpt-5-mini",
    "llm.temperature": 0.7
  });

  // Add events to track important milestones
  span.addEvent("processing_started", {
    timestamp: Date.now(),
    batch_size: 1000
  });

  // Your business logic
  const result = await processBatch();

  span.addEvent("processing_completed", {
    timestamp: Date.now(),
    processed_count: result.length
  });

  span.setOutput(result);
  span.setStatus({ code: SpanStatusCode.OK });

  return result;
});
```

<Tip>
For consistent attribute naming and TypeScript autocomplete support, use semantic conventions. See our [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) guide for best practices.
</Tip>

### Conditional Span Creation

Create spans conditionally based on your application logic:

```typescript
async function conditionalOperation(shouldTrace: boolean, data: any) {
  if (shouldTrace) {
    return await tracer.withActiveSpan("conditional-operation", async (span) => {
      span.setType("conditional");
      span.setInput(data);

      const result = await processData(data);

      span.setOutput(result);
      span.setStatus({ code: SpanStatusCode.OK });

      return result;
    });
  } else {
    // No tracing overhead when not needed
    return await processData(data);
  }
}
```

## Basic Manual Span Management

When you need fine-grained control over spans beyond what `withActiveSpan` provides, you can manually manage span lifecycle, attributes, and context propagation.

### Using startActiveSpan

`startActiveSpan` provides automatic context management but requires manual error handling:

```typescript
// Using startActiveSpan (automatic context management)
tracer.startActiveSpan("my-operation", (span) => {
  try {
    span.setType("llm");
    span.setInput("Hello, world!");
    // ... your business logic ...
    span.setOutput("Hello! How can I help you?");
    span.setStatus({ code: SpanStatusCode.OK });
  } catch (error) {
    span.setStatus({
      code: SpanStatusCode.ERROR,
      message: error.message
    });
    span.recordException(error);
    throw error;
  } finally {
    span.end();
  }
});
```

### Using startSpan (Complete Manual Control)

`startSpan` gives you complete control but requires manual context management:

```typescript
// Using startSpan (complete manual control)
const span = tracer.startSpan("my-operation");
try {
  span.setType("llm");
  span.setInput("Hello, world!");
  // ... your business logic ...
  span.setOutput("Hello! How can I help you?");
  span.setStatus({ code: SpanStatusCode.OK });
} catch (error) {
  span.setStatus({
    code: SpanStatusCode.ERROR,
    message: error.message
  });
  span.recordException(error);
  throw error;
} finally {
  span.end();
}
```

## Span Context Propagation

Manually propagate span context across async boundaries and service boundaries when `withActiveSpan` isn't sufficient:

```typescript
import { context, trace } from "@opentelemetry/api";

async function processWithContext(userId: string) {
  const span = tracer.startSpan("process-user");
  const ctx = trace.setSpan(context.active(), span);

  try {
    // Propagate context to async operations
    await context.with(ctx, async () => {
      await processUserData(userId);
      await updateUserProfile(userId);
    });

    span.setStatus({ code: SpanStatusCode.OK });
  } catch (error) {
    span.setStatus({
      code: SpanStatusCode.ERROR,
      message: error.message
    });
    span.recordException(error);
    throw error;
  } finally {
    span.end();
  }
}
```

## Error Handling Patterns

Implement robust error handling for manual span management:

```typescript
class SpanManager {
  private tracer = getLangWatchTracer("my-service");

  async executeWithSpan<T>(
    operationName: string,
    operation: (span: Span) => Promise<T>
  ): Promise<T> {
    const span = this.tracer.startSpan(operationName);

    try {
      const result = await operation(span);
      span.setStatus({ code: SpanStatusCode.OK });
      return result;
    } catch (error) {
      span.setStatus({
        code: SpanStatusCode.ERROR,
        message: error.message
      });
      span.recordException(error);
      throw error;
    } finally {
      span.end();
    }
  }
}

// Usage example
const spanManager = new SpanManager();
const result = await spanManager.executeWithSpan("my-operation", async (span) => {
  span.setType("llm");
  span.setInput("Hello");
  // ... your business logic ...
  return "World";
});
```

## Custom Span Processors

Create custom span processors for specialized processing needs, filtering, and multiple export destinations.

### Custom Exporters

Configure custom exporters alongside LangWatch:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-http";

const handle = await setupObservability({
  // Use custom span processors
  spanProcessors: [
    new BatchSpanProcessor(new OTLPTraceExporter({
      url: 'https://custom-collector.com/v1/traces'
    }))
  ],

  // Or use a single trace exporter
  traceExporter: new OTLPTraceExporter({
    url: 'https://custom-collector.com/v1/traces'
  })
});
```

### Span Filtering

Implement span filtering to control which spans are processed:

```typescript
import { FilterableBatchSpanProcessor, LangWatchExporter } from "langwatch";

const processor = new FilterableBatchSpanProcessor(
  new LangWatchExporter({
    apiKey: "your-api-key",
  }),
  [
    { attribute: "http.url", value: "/health" },
    { attribute: "span.type", value: "health" },
    { attribute: "custom.ignore", value: "true" }
  ]
);

const handle = await setupObservability({
  langwatch: 'disabled',
  spanProcessors: [processor]
});
```

### Multiple Exporters

Configure multiple exporters for different destinations:

```typescript
import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";
import { JaegerExporter } from "@opentelemetry/exporter-jaeger";
import { LangWatchExporter } from "langwatch";

const handle = await setupObservability({
  serviceName: "my-service",
  spanProcessors: [
    // Send to Jaeger for debugging
    new BatchSpanProcessor(new JaegerExporter({
      endpoint: "http://localhost:14268/api/traces"
    })),
    // Send to LangWatch for production monitoring
    new BatchSpanProcessor(new LangWatchExporter({
      apiKey: process.env.LANGWATCH_API_KEY
    }))
  ]
});
```

### Batch Processing Configuration

Optimize batch processing for high-volume applications:

```typescript
import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";
import { LangWatchExporter } from "langwatch";

const batchProcessor = new BatchSpanProcessor(
  new LangWatchExporter({
    apiKey: process.env.LANGWATCH_API_KEY
  }),
  {
    maxQueueSize: 2048, // Maximum number of spans in queue
    maxExportBatchSize: 512, // Maximum spans per batch
    scheduledDelayMillis: 5000, // Export interval
    exportTimeoutMillis: 30000, // Export timeout
  }
);

const handle = await setupObservability({
  langwatch: 'disabled', // Disabled we report to LangWatch via the `batchProcessor`
  spanProcessors: [batchProcessor]
});
```

## Performance Considerations

When using manual span management, consider these performance implications:

<Warning>
Manual span management requires careful attention to memory usage and proper cleanup to avoid memory leaks.
</Warning>

1. **Memory Usage**: Manually created spans consume memory until explicitly ended
2. **Context Propagation**: Manual context management can be error-prone and impact performance
3. **Error Handling**: Ensure spans are always ended, even when exceptions occur
4. **Batch Processing**: Use batch processors for high-volume applications to reduce overhead
5. **Sampling**: Implement sampling to reduce overhead in production environments

## Best Practices

<CardGroup cols={2}>
<Card title="Use withActiveSpan" icon="auto">
  - Prefer `withActiveSpan` for most use cases
  - Automatic context propagation and error handling
  - Guaranteed span cleanup
</Card>

<Card title="Manual Control" icon="settings">
  - Use manual span management only when needed
  - Always end spans in finally blocks
  - Use try-catch-finally patterns consistently
</Card>

<Card title="Context Management" icon="context">
  - Propagate span context across async boundaries
  - Use context.with() for async operations
  - Maintain span hierarchy properly
</Card>

<Card title="Attributes and Events" icon="attributes">
  - Add meaningful custom attributes for filtering
  - Use consistent attribute naming conventions
  - Include relevant business context
</Card>

<Card title="Performance" icon="performance">
  - Implement appropriate sampling strategies
  - Use batch processors for high volume
  - Monitor observability overhead
</Card>

<Card title="Error Handling" icon="error">
  - Set appropriate status codes and error messages
  - Record exceptions with context
  - Maintain error flow in your application
</Card>
</CardGroup>

## When to Use Each Approach


### withActiveSpan (Recommended)

Use `withActiveSpan` for:
- Most application logic
- Operations that need automatic context propagation
- When you want automatic error handling and cleanup
- Simple to moderate complexity operations

```typescript
await tracer.withActiveSpan("my-operation", async (span) => {
  // Automatic context propagation, error handling, and cleanup
  return await processData();
});
```


### startActiveSpan

Use `startActiveSpan` for:
- When you need manual error handling logic
- Operations with complex conditional logic
- When you need to control exactly when the span ends

```typescript
tracer.startActiveSpan("my-operation", (span) => {
  try {
    // Manual error handling
    return processData();
  } catch (error) {
    // Custom error handling logic
    handleError(error);
    throw error;
  } finally {
    span.end();
  }
});
```


### startSpan (Manual)

Use `startSpan` for:
- Maximum control over span lifecycle
- Complex context propagation scenarios
- When you need to manage multiple spans simultaneously
- Advanced use cases requiring manual context management

```typescript
const span = tracer.startSpan("my-operation");
try {
  // Complete manual control
  const ctx = trace.setSpan(context.active(), span);
  await context.with(ctx, async () => {
    // Manual context propagation
  });
} finally {
  span.end();
}
```

<Info>
For most use cases, the `withActiveSpan` method provides the best balance of ease of use, safety, and functionality. Only use manual span management when you need specific control over span lifecycle or context propagation that `withActiveSpan` cannot provide.
</Info>

## Related Documentation

For more advanced observability patterns and best practices:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and core concepts
- **[API Reference](/integration/typescript/reference)** - Complete API documentation
- **[Semantic Conventions](/integration/typescript/tutorials/semantic-conventions)** - Standardized attribute naming guidelines
- **[Debugging and Troubleshooting](/integration/typescript/tutorials/debugging-typescript)** - Debug manual instrumentation issues
- **[Framework Integrations](/integration/typescript/integrations)** - Framework-specific instrumentation approaches

<Tip>
Combine manual instrumentation with [Semantic Conventions](/integration/typescript/tutorials/semantic-conventions) for consistent, maintainable observability across your application.
</Tip>

---

# FILE: ./integration/typescript/tutorials/debugging-typescript.mdx

---
title: Debugging and Troubleshooting
description: Debug LangWatch TypeScript SDK integration issues
sidebarTitle: Debugging
---

# Debugging and Troubleshooting

This guide covers debugging techniques and troubleshooting common issues when integrating LangWatch with TypeScript applications.

## Console Tracing and Logging

Enable console output and detailed logging for development and troubleshooting.

```typescript
import { setupObservability } from "langwatch/observability/node";

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    processorType: 'simple' // Use 'simple' for immediate export during debugging
  },
  serviceName: "my-service",

  // Debug options for development
  debug: {
    consoleTracing: true, // Log spans to console
    consoleLogging: true, // Log records to console
    logLevel: 'debug'     // SDK internal logging
  }
});
```

## Custom Logger

Create a custom logger for better integration with your existing logging system:

```typescript
import { setupObservability } from "langwatch/observability/node";

// Create a custom logger
const customLogger = {
  debug: (message: string) => console.log(`[DEBUG] ${message}`),
  info: (message: string) => console.log(`[INFO] ${message}`),
  warn: (message: string) => console.warn(`[WARN] ${message}`),
  error: (message: string) => console.error(`[ERROR] ${message}`),
};

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service",

  debug: {
    logger: customLogger,
    logLevel: 'debug'
  }
});
```

## Error Handling

Configure error handling behavior for different environments:

```typescript
import { setupObservability } from "langwatch/observability/node";

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY
  },
  serviceName: "my-service",

  // Advanced options for error handling
  advanced: {
    throwOnSetupError: true, // Throw errors instead of returning no-op handles
  }
});
```

## Common Issues

### Spans Not Appearing in Dashboard

1. **Check API Key**: Ensure your `LANGWATCH_API_KEY` is correctly set
2. **Verify Endpoint**: Confirm the `LANGWATCH_ENDPOINT` is accessible
3. **Check Network**: Ensure your application can reach the LangWatch API
4. **Processor Type**: Use `'simple'` processor for immediate export during debugging

### Performance Issues

1. **Batch Processing**: Use `'batch'` processor for production to reduce API calls
2. **Sampling**: Implement sampling for high-volume applications
3. **Data Capture**: Limit data capture to essential information

### Integration Issues

1. **Framework Compatibility**: Ensure you're using the correct integration for your framework
2. **Version Compatibility**: Check that your LangWatch SDK version is compatible with your framework
3. **Configuration**: Verify that all required configuration options are set

## Environment-Specific Debugging

### Development Environment

```typescript
const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    processorType: 'simple' // Immediate export for debugging
  },
  serviceName: "my-service",
  debug: {
    consoleTracing: true,
    consoleLogging: true,
    logLevel: 'info' // Raise this to `debug` if you're debugging the LangWatch integration
  }
});
```

### Production Environment

```typescript
const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY,
    processorType: 'batch' // Efficient batching for production
  },
  serviceName: "my-service",
  debug: {
    consoleTracing: false, // Disable console output in production
    logLevel: 'warn' // Only log warnings and errors
  }
});
```

## Getting Help

If you're still experiencing issues:

1. **Check Logs**: Review console output and application logs
2. **Verify Configuration**: Double-check all configuration options
3. **Test Connectivity**: Ensure your application can reach LangWatch services
4. **Community Support**: Visit our [Discord community](https://discord.gg/langwatch) for help
5. **GitHub Issues**: Report bugs and feature requests on [GitHub](https://github.com/langwatch/langwatch/issues)

## Related Documentation

For more debugging techniques and advanced troubleshooting:

- **[Integration Guide](/integration/typescript/guide)** - Basic setup and common issues
- **[API Reference](/integration/typescript/reference)** - Configuration options and error handling
- **[Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation)** - Debugging manual span management
- **[Framework Integrations](/integration/typescript/integrations)** - Framework-specific debugging guides
- **[OpenTelemetry Migration](/integration/typescript/tutorials/opentelemetry-migration)** - Troubleshooting migration issues

<Tip>
For complex debugging scenarios, combine console tracing with [Manual Instrumentation](/integration/typescript/tutorials/manual-instrumentation) techniques for detailed span analysis.
</Tip>

---

# FILE: ./integration/typescript/tutorials/capturing-input-output.mdx

---
title: Capturing and Mapping Inputs & Outputs
sidebarTitle: TypeScript/JS
icon: square-js
description: Learn how to control the capture and structure of input and output data for traces and spans with the LangWatch TypeScript SDK.
keywords: langwatch, typescript, javascript, input, output, capture, mapping, data, tracing, spans, observability
---

Effectively capturing the inputs and outputs of your LLM application's operations is crucial for observability. LangWatch provides flexible ways to manage this data, whether you prefer automatic capture or explicit control to map complex objects, format data, or redact sensitive information.

This tutorial covers how to:
*   Understand automatic input/output capture.
*   Explicitly set inputs and outputs for traces and spans.
*   Dynamically update this data on active traces/spans.
*   Handle different data formats, especially for chat messages.

## Automatic Input and Output Capture

By default, when you use `tracer.withActiveSpan()` or `tracer.startActiveSpan()`, the SDK attempts to automatically capture:

*   **Inputs**: The arguments passed to the function within the span context.
*   **Outputs**: The value returned by the function within the span context.

This behavior can be controlled using the data capture configuration in your observability setup.

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability with data capture configuration
setupObservability({
  dataCapture: "all", // Capture both input and output (default)
});

const tracer = getLangWatchTracer("input-output-example");

// Automatic capture example
await tracer.withActiveSpan("GreetUser", async (span) => {
  // Function arguments and return value will be automatically captured
  const name = "Alice";
  const greeting = "Hello";

  span.setAttributes({ operation: "greeting" });
  return `${greeting}, ${name}!`;
});

// Disable automatic capture for sensitive operations
await tracer.withActiveSpan("SensitiveOperation", async (span) => {
  // Inputs and outputs for this span will not be automatically captured
  // You might explicitly set a sanitized version if needed
  console.log("Processing sensitive data...");
  return { status: "processed" };
}, { dataCapture: "none" });
```

<Note>
  Refer to the API reference for [`getLangWatchTracer()`](/integration/typescript/reference#getlangwatchtracer) and [`LangWatchTracer`](/integration/typescript/reference#langwatchtracer) for more details on data capture configuration.
</Note>

## Explicitly Setting Inputs and Outputs

You often need more control over what data is recorded. You can explicitly set inputs and outputs using the `setInput()` and `setOutput()` methods on span objects.

This is useful for:
*   Capturing only specific parts of complex objects.
*   Formatting data in a more readable or structured way (e.g., as a list of `ChatMessage` objects).
*   Redacting sensitive information before it's sent to LangWatch.
*   Providing inputs/outputs when automatic capture is disabled.

### At Span Creation

When using `tracer.withActiveSpan()` or `tracer.startActiveSpan()`, you can set inputs and outputs directly on the span object.

<CodeGroup>
```typescript Trace with explicit input/output
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("input-output-example");

await tracer.withActiveSpan("UserIntentProcessing", async (span) => {
  // Set explicit input for the span
  span.setInput("json", {
    user_query: "Book a flight to London"
  });

  // raw_query_data might be large or contain sensitive info
  // The setInput() call above provides a clean version
  const rawQueryData = { query: "Book a flight to London", user_id: "123" };

  const intent = "book_flight";
  const entities = { destination: "London" };

  // Explicitly set the output for the span
  span.setOutput("json", {
    intent,
    entities
  });

  return { status: "success", intent }; // Actual function return
});
```

```typescript Span with explicit input/output
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("chatbot-example");

await tracer.withActiveSpan("ChatbotInteraction", async (span) => {
  const userMessage = { role: "user", content: "What is LangWatch?" };

  // Create a child span for LLM call
  await tracer.withActiveSpan("LLMCall", async (llmSpan) => {
    llmSpan.setType("llm");
    llmSpan.setRequestModel("gpt-5-mini");

    // Set input as chat messages
    llmSpan.setInput("chat_messages", [userMessage]);

    // Simulate LLM call
    const assistantResponseContent = "LangWatch helps you monitor your LLM applications.";
    const assistantMessage = { role: "assistant", content: assistantResponseContent };

    // Set output on the span object
    llmSpan.setOutput("chat_messages", [assistantMessage]);
  });

  console.log("Chat finished.");
});
```
</CodeGroup>

### Dynamically Updating Inputs and Outputs

You can modify the input or output of an active span using its `setInput()` and `setOutput()` methods. This is particularly useful when the input/output data is determined or refined during the operation.

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("pipeline-example");

await tracer.withActiveSpan("DataTransformationPipeline", async (span) => {
  // Initial input is automatically captured if dataCapture is enabled

  await tracer.withActiveSpan("Step1_CleanData", async (step1Span) => {
    // Suppose initial_data is complex, we want to record a summary as input
    const initialData = { a: 1, b: null, c: 3 };
    step1Span.setInput("json", { data_keys: Object.keys(initialData) });

    const cleanedData = Object.fromEntries(
      Object.entries(initialData).filter(([_, v]) => v !== null)
    );

    step1Span.setOutput("json", { cleaned_item_count: Object.keys(cleanedData).length });
  });

  // ... further steps ...

  // Update the root span's output for the entire trace
  const finalResult = { status: "completed", items_processed: 2 };
  span.setOutput("json", finalResult);

  return finalResult;
});
```

<Note>
  The `setInput()` and `setOutput()` methods on `LangWatchSpan` objects are versatile and support multiple data types. See the reference for [`LangWatchSpan` methods](/integration/typescript/reference#langwatchspan).
</Note>

## Handling Different Data Formats

LangWatch can store various types of input and output data:

*   **Strings**: Simple text using `"text"` type.
*   **Objects**: Automatically serialized as JSON using `"json"` type. This is useful for structured data.
*   **Chat Messages**: Arrays of chat message objects using `"chat_messages"` type. This ensures proper display and analysis in the LangWatch UI.
*   **Raw Data**: Any data type using `"raw"` type.
*   **Lists**: Arrays of structured data using `"list"` type.

### Capturing Chat Messages

For LLM interactions, structure your inputs and outputs as chat messages using the `"chat_messages"` type.

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("advanced-chat-example");

await tracer.withActiveSpan("AdvancedChat", async (span) => {
  const messages = [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "What is the weather in London?" }
  ];

  let assistantResponseWithTool: any;

  await tracer.withActiveSpan("GetWeatherToolCall", async (llmSpan) => {
    llmSpan.setType("llm");
    llmSpan.setRequestModel("gpt-5-mini");
    llmSpan.setInput("chat_messages", messages);

    // Simulate model deciding to call a tool
    const toolCallId = "call_abc123";
    assistantResponseWithTool = {
      role: "assistant",
      tool_calls: [
        {
          id: toolCallId,
          type: "function",
          function: {
            name: "get_weather",
            arguments: JSON.stringify({ location: "London" })
          }
        }
      ]
    };

    llmSpan.setOutput("chat_messages", [assistantResponseWithTool]);
  });

  // Simulate tool execution
  await tracer.withActiveSpan("RunGetWeatherTool", async (toolSpan) => {
    toolSpan.setType("tool");

    const toolInput = {
      tool_name: "get_weather",
      arguments: { location: "London" }
    };
    toolSpan.setInput("json", toolInput);

    const toolResultContent = JSON.stringify({
      temperature: "15C",
      condition: "Cloudy"
    });
    toolSpan.setOutput("text", toolResultContent);

    // Prepare message for next LLM call
    const toolResponseMessage = {
      role: "tool",
      tool_call_id: "call_abc123",
      name: "get_weather",
      content: toolResultContent
    };

    messages.push(assistantResponseWithTool); // Assistant's decision to call tool
    messages.push(toolResponseMessage);       // Tool's response
  });

  await tracer.withActiveSpan("FinalLLMResponse", async (finalLlmSpan) => {
    finalLlmSpan.setType("llm");
    finalLlmSpan.setRequestModel("gpt-5-mini");
    finalLlmSpan.setInput("chat_messages", messages);

    const finalAssistantContent = "The weather in London is 15°C and cloudy.";
    const finalAssistantMessage = {
      role: "assistant",
      content: finalAssistantContent
    };

    finalLlmSpan.setOutput("chat_messages", [finalAssistantMessage]);
  });
});
```

<Note>
  For the detailed structure of chat messages and other related types, please refer to the [Core Data Types section in the API Reference](/integration/typescript/reference#core-data-types).
</Note>

## Data Capture Configuration

You can control automatic data capture at different levels:

### Global Configuration

Set the default data capture behavior for your entire application:

```typescript
import { setupObservability } from "langwatch/observability/node";

// Setup with different capture modes
setupObservability({
  dataCapture: "all", // Capture both input and output (default)
  // dataCapture: "none", // Capture nothing
  // dataCapture: "input", // Capture only inputs
  // dataCapture: "output", // Capture only outputs
});
```

## Use Cases and Best Practices

*   **Redacting Sensitive Information**: If your function arguments or return values contain sensitive data (PII, API keys), disable automatic capture and explicitly set sanitized versions using `setInput()` and `setOutput()`.
*   **Mapping Complex Objects**: If your inputs/outputs are complex JavaScript objects, map them to a simplified object or string representation for clearer display in LangWatch.
*   **Improving Readability**: For long text inputs/outputs (e.g., full documents), consider capturing a summary or metadata instead of the entire content to reduce noise, unless the full content is essential for debugging or evaluating.
*   **Error Handling**: Use try-catch blocks within spans to capture error information and set appropriate outputs.
*   **Clearing Captured Data**: You can set `input` or `output` to `null` or an empty object via the `setInput()` or `setOutput()` methods to remove previously captured data if it's no longer relevant.

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("redaction-example");

await tracer.withActiveSpan("DataRedactionExample", async (span) => {
  // user_profile might contain PII
  const userProfile = {
    id: "user_xyz",
    email: "test@example.com",
    name: "Sensitive Name"
  };

  // Update the input to a redacted version
  const redactedInput = {
    user_id: userProfile.id,
    has_email: "email" in userProfile
  };
  span.setInput("json", redactedInput);

  // Process data...
  const result = {
    status: "processed",
    user_id: userProfile.id
  };
  span.setOutput("json", result);

  return result; // Actual function return can still be the full data
});
```

### Error Handling Example

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("error-handling-example");

await tracer.withActiveSpan("RiskyOperation", async (span) => {
  try {
    span.setInput("json", { operation: "data_processing" });

    // Simulate a risky operation that might fail
    const result = await processData();

    span.setOutput("json", { status: "success", result });
    return result;
  } catch (error) {
    // Capture error information in the span
    span.setOutput("json", {
      status: "error",
      error_message: error instanceof Error ? error.message : String(error),
      error_type: error instanceof Error ? error.constructor.name : typeof error
    });

    // Re-throw the error (withActiveSpan will automatically mark the span as ERROR)
    throw error;
  }
});
```

## Conclusion

Controlling how inputs and outputs are captured in LangWatch allows you to tailor the observability data to your specific needs. By using data capture configuration, explicit `setInput()` and `setOutput()` methods, and appropriate data formatting (especially `"chat_messages"` for conversations), you can ensure that your traces provide clear, relevant, and secure insights into your LLM application's behavior.

---

# FILE: ./integration/typescript/tutorials/tracking-llm-costs.mdx

---
title: Tracking LLM Costs and Tokens
sidebarTitle: TypeScript/JS
description: Troubleshooting & adjusting cost tracking in LangWatch
icon: square-js
keywords: LangWatch, cost tracking, token counting, debugging, troubleshooting, model costs, metrics, LLM spans
---

By default, LangWatch will automatically capture cost and token data for your LLM calls.

<img
  src="/images/costs/llm-costs-analytics.png"
  alt="LLM costs analytics graph"
/>

If you don't see costs being tracked or you see it being tracked as $0, this guide will help you identify and fix issues when cost and token tracking is not working as expected.

## Understanding Cost and Token Tracking

LangWatch calculates costs and tracks tokens by:

1. **Capturing model names** in LLM spans to match against cost tables
2. **Recording token metrics** (`prompt_tokens`, `completion_tokens`) in span data, or estimating when not available
3. **Mapping models to costs** using the pricing table in Settings > Model Costs

When any of these components are missing, you might see missing or $0 costs and tokens.

## Step 1: Verify LLM Span Data Capture

The most common issue is that your LLM spans aren't capturing the required data: model name, inputs, outputs, and token metrics.

### Check Your Current Spans

First, examine what data is being captured in your LLM spans. In the LangWatch dashboard:

1. Navigate to a trace that should have cost/token data
2. Click on the LLM span to inspect its details
3. Look for these key fields:
   - **Model**: Should show the model identifier (e.g., `openai/gpt-5`)
   - **Input/Output**: Should contain the actual messages sent and received
   - **Metrics**: Should show prompt + completion tokens

<img
  src="/images/costs/llm-span-details.png"
  alt="LLM span showing model, input/output, and token metrics"
/>

## Step 2: Fix Missing Model Information

If your spans don't show model information, the integration framework you're using might not be capturing it automatically.

### Solution A: Use Framework Auto-tracking

LangWatch provides auto-tracking for popular frameworks that automatically captures all the necessary data for cost calculation.

Check the **Integrations** menu in the sidebar to find specific setup instructions for your framework, which will show you how to properly configure automatic model and token tracking.

### Solution B: Manually Set Model Information

If auto-tracking isn't available for your framework, manually update the span with model information:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

// Setup observability
setupObservability();

const tracer = getLangWatchTracer("cost-tracking-example");

async function customLLMCall(prompt: string): Promise<string> {
  return await tracer.withActiveSpan("CustomLLMCall", async (span) => {
    // Mark the span as an LLM type span
    span.setType("llm");
    span.setRequestModel("gpt-5-mini"); // Use the exact model identifier
    span.setInput("text", prompt);

    // Simulate an LLM response
    const response = await yourCustomLLMClient.generate(prompt);

    // Set output and token metrics
    span.setOutput("text", response.text);
    span.setMetrics({
      promptTokens: response.usage.prompt_tokens,
      completionTokens: response.usage.completion_tokens,
    });

    return response.text;
  });
}
```

## Step 3: Configure Model Cost Mapping

If your model information is being captured but costs still show $0, you need to configure the cost mapping.

### Check Existing Model Costs

1. Go to **Settings > Model Costs** in your LangWatch dashboard
2. Look for your model in the list
3. Check if the regex pattern matches your model identifier

<img
  src="/images/costs/model-costs-settings.webp"
  alt="Model Costs settings page showing cost configuration"
/>

### Add Custom Model Costs

If your model isn't in the cost table, add it:

1. Click **"Add New Model"** in Settings > Model Costs
2. Configure the model entry:
   - **Model Name**: Descriptive name (e.g., "gpt-5-mini")
   - **Regex Match Rule**: Pattern to match your model identifier (e.g., `^gpt-5-mini$`)
   - **Input Cost**: Cost per input token (e.g., `0.0000004`)
   - **Output Cost**: Cost per output token (e.g., `0.0000016`)

### Common Model Identifier Patterns

Make sure your regex patterns match how the model names appear in your spans:

| Framework    | Model Identifier Format | Regex Pattern          |
| ------------ | ----------------------- | ---------------------- |
| OpenAI SDK   | `gpt-5-mini`           | `^gpt-5-mini$`        |
| Azure OpenAI | `gpt-5-mini`           | `^gpt-5-mini$`        |
| LangChain    | `openai/gpt-5-mini`    | `^openai/gpt-5-mini$` |
| Custom       | `my-custom-model-v1`    | `^my-custom-model-v1$` |

### Verification Checklist

After running your test, verify in the LangWatch dashboard:

✅ **Trace appears** in the dashboard \
✅ **LLM span shows model name** (e.g., `gpt-5-mini`) \
✅ **Input and output are captured** \
✅ **Token metrics are present** (`prompt_tokens`, `completion_tokens`) \
✅ **Cost is calculated and displayed** (non-zero value)

## Common Issues and Solutions

### Issue: Auto-tracking not working

**Symptoms**: Spans appear but without model/metrics data

**Solutions**:

- Ensure `setupObservability()` is called before any LLM operations
- Check that the client instance being tracked is the same one making calls
- Verify the integration is initialized correctly

### Issue: Custom models not calculating costs

**Symptoms**: Model name appears but cost remains $0

**Solutions**:

- Check regex pattern in Model Costs settings
- Ensure the pattern exactly matches your model identifier
- Verify input and output costs are configured correctly

### Issue: Token counts are 0 but model is captured

**Symptoms**: Model name is present but token metrics are missing

**Solutions**:

- Manually set token metrics using `span.setMetrics()` if not automatically captured
- Check if your LLM provider returns usage information
- Ensure the integration is extracting token counts from responses

### Issue: Framework with OpenTelemetry not capturing model data

**Symptoms**: Using a framework with OpenTelemetry integration that's not capturing model names or token counts

**Solutions**:
- Follow the guidance in [Solution C: Framework with OpenTelemetry Integration](#solution-c-framework-with-opentelemetry-integration) above
- Wrap your LLM calls with custom spans to patch missing data

## Advanced Examples

### LangChain Integration

The `LangWatchCallbackHandler` automatically captures model information and token metrics:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { LangWatchCallbackHandler } from "langwatch/instrumentation/langchain";
import { ChatOpenAI } from "@langchain/openai";

setupObservability();

const llm = new ChatOpenAI({
  modelName: "gpt-5-mini",
  temperature: 0.7,
  callbacks: [new LangWatchCallbackHandler()],
});
```

### Manual Token Counting

If your LLM provider doesn't return token counts:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("manual-token-counting");

async function llmWithManualTokenCounting(prompt: string): Promise<string> {
  return await tracer.withActiveSpan("LLMWithManualCounting", async (span) => {
    span.setType("llm");
    span.setRequestModel("custom-model-v1");
    span.setInput("text", prompt);

    const response = await yourCustomLLMClient.generate(prompt);

    // Manual token counting (simplified example)
    const estimatedPromptTokens = Math.ceil(prompt.length / 4);
    const estimatedCompletionTokens = Math.ceil(response.text.length / 4);

    span.setOutput("text", response.text);
    span.setMetrics({
      promptTokens: estimatedPromptTokens,
      completionTokens: estimatedCompletionTokens,
    });

    return response.text;
  });
}
```

## Getting Help

If you're still experiencing issues after following this guide:

1. **Check the LangWatch logs** for any error messages
2. **Verify your API key** and endpoint configuration
3. **Share a minimal reproduction** with the specific framework you're using

Cost and token tracking should work reliably once the model information and metrics are properly captured. Most issues stem from missing model identifiers or incorrect cost table configuration.

---

# FILE: ./integration/rest-api.mdx

---
title: REST API
sidebarTitle: HTTP API
icon: globe
description: Integrate LangWatch with any language by using the REST API
keywords: LangWatch, REST API, HTTP API, curl, integration, observability, evaluation, prompts, datasets, workflows, automation
---

If your preferred programming language or platform is not directly supported by the existing LangWatch libraries, you can use the REST API with `curl` to send trace data. This guide will walk you through how to integrate LangWatch with any system that allows HTTP requests.

**Prerequisites:**

- Ensure you have `curl` installed on your system.

**Configuration:**

Set the `LANGWATCH_API_KEY` environment variable in your environment:

```bash
export LANGWATCH_API_KEY='your_api_key_here'
```

**Usage:**

You will need to prepare your span data in accordance with the Span type definitions provided by LangWatch. Below is an example of how to send span data using curl:

    1. Prepare your JSON data. Make sure it's properly formatted as expected by LangWatch.
    2. Use the curl command to send your trace data. Here is a basic template:

```bash
# Set your API key and endpoint URL
LANGWATCH_API_KEY="your_langwatch_api_key"
LANGWATCH_ENDPOINT="https://app.langwatch.ai"

# Use curl to send the POST request, e.g.:
curl -X POST "$LANGWATCH_ENDPOINT/api/collector" \
     -H "X-Auth-Token: $LANGWATCH_API_KEY" \
     -H "Content-Type: application/json" \
     -d @- <<EOF
{
  "trace_id": "trace-123",
  "spans": [
    {
      "type": "llm",
      "span_id": "span-456",
      "vendor": "openai",
      "model": "gpt-5",
      "input": {
        "type": "chat_messages",
        "value": [
          {
            "role": "user",
            "content": "Input to the LLM"
          }
        ]
      },
      "output": {
        "type": "chat_messages",
        "value": [
            {
                "role": "assistant",
                "content": "Output from the LLM",
                "function_call": null,
                "tool_calls": []
            }
        ]
      },
      "params": {
        "temperature": 0.7,
        "stream": false
      },
      "metrics": {
        "prompt_tokens": 100,
        "completion_tokens": 150
      },
      "timestamps": {
        "started_at": $(($(date +%s) * 1000)),
        "finished_at": $((($(date +%s) + 1) * 1000))
      }
    }
  ],
  "metadata": {
    "user_id": "optional_end_user_identifier",
    "thread_id": "optional_thread_identifier",
    "customer_id": "optional_platform_customer_identifier",
    "labels": ["optional_label_1", "optional_label_2"]
  }
}
EOF
```

Replace the placeholders with your actual data. The `@-` tells `curl` to read the JSON data from the standard input, which we provide via the `EOF`-delimited here-document.

For the type reference of how a `span` should look like, check out our [types definitions](https://github.com/langwatch/langwatch/blob/main/python-sdk/src/langwatch/types.py).

It's optional but highly recommended to pass the `user_id` on the metadata if you want to leverage user-specific analytics and the `thread_id` to group related traces together. To connect it to an event later on. Read more about those and other concepts [here](../concepts).

3.  Execute the `curl` command. If successful, LangWatch will process your trace data.

This method of integration offers a flexible approach for sending traces from any system capable of making HTTP requests. Whether you're using a less common programming language or a custom-built platform, this RESTful approach ensures you can benefit from LangWatch's capabilities.

Remember to handle errors and retries as needed. You might need to script additional logic around the `curl` command to handle these cases.

After following the above guide, your interactions with LLMs should now be captured by LangWatch. Once integrated, you can visit your LangWatch dashboard to view and analyze the traces collected from your applications.

---

# FILE: ./integration/opentelemetry/guide.mdx

---
title: OpenTelemetry Integration Guide
sidebarTitle: OpenTelemetry
description: Use OpenTelemetry to capture LLM traces and send them to LangWatch from any programming language
icon: telescope
keywords: langwatch, opentelemetry, integration, guide, java, c#, .net, python, typescript, javascript, go, sdk, open telemetry, open telemetry integration, open telemetry guide, open telemetry integration guide, open telemetry integration guide java, open telemetry integration guide c#, open telemetry integration guide .net, open telemetry integration guide python, open telemetry integration guide typescript, open telemetry integration guide javascript, open telemetry integration guide go
---

OpenTelemetry is a vendor-neutral standard for observability that provides a unified way to capture traces, metrics, and logs. LangWatch is fully compatible with OpenTelemetry, allowing you to use any OpenTelemetry-compatible library in any programming language to capture your LLM traces and send them to LangWatch.

This guide shows you how to set up OpenTelemetry instrumentation in any language and configure it to export traces to LangWatch's OTEL API endpoint.

## Prerequisites

- Obtain your `LANGWATCH_API_KEY` from the [LangWatch dashboard](https://app.langwatch.ai/)
- Install the OpenTelemetry SDK for your programming language

## LangWatch OTEL API Endpoint

LangWatch provides a standard OpenTelemetry Protocol (OTLP) endpoint for receiving traces:

```
https://app.langwatch.ai/api/otel/v1/traces
```

This endpoint accepts OTLP over HTTP and gRPC protocols, making it compatible with all OpenTelemetry SDKs.

## General Setup Pattern

The setup follows this general pattern across all languages:

1. **Install OpenTelemetry SDK** for your language
2. **Configure the OTLP exporter** to point to LangWatch's endpoint
3. **Set up authentication** using your API key
4. **Initialize the trace provider** with the exporter
5. **Instrument your LLM calls** using available instrumentation libraries

## Language-Specific Examples


  ### Java

    <Steps>
      <Step title="Install OpenTelemetry">
        Add to your `pom.xml`:
        ```xml
        <dependency>
            <groupId>io.opentelemetry</groupId>
            <artifactId>opentelemetry-sdk</artifactId>
            <version>1.32.0</version>
        </dependency>
        <dependency>
            <groupId>io.opentelemetry</groupId>
            <artifactId>opentelemetry-exporter-otlp</artifactId>
            <version>1.32.0</version>
        </dependency>
        ```
      </Step>

      <Step title="Configure the exporter">
        ```java
        import io.opentelemetry.api.OpenTelemetry;
        import io.opentelemetry.api.trace.propagation.W3CTraceContextPropagator;
        import io.opentelemetry.context.propagation.ContextPropagators;
        import io.opentelemetry.sdk.OpenTelemetrySdk;
        import io.opentelemetry.sdk.trace.SdkTracerProvider;
        import io.opentelemetry.sdk.trace.export.BatchSpanProcessor;
        import io.opentelemetry.sdk.trace.export.OtlpHttpSpanExporter;
        import io.opentelemetry.semconv.resource.attributes.ResourceAttributes;

        public class OpenTelemetryConfig {
            public static OpenTelemetry initOpenTelemetry() {
                OtlpHttpSpanExporter spanExporter = OtlpHttpSpanExporter.builder()
                    .setEndpoint("https://app.langwatch.ai/api/otel/v1/traces")
                    .addHeader("Authorization", "Bearer " + System.getenv("LANGWATCH_API_KEY"))
                    .build();

                SdkTracerProvider sdkTracerProvider = SdkTracerProvider.builder()
                    .addSpanProcessor(BatchSpanProcessor.builder(spanExporter).build())
                    .setResource(Resource.getDefault().toBuilder()
                        .put(ResourceAttributes.SERVICE_NAME, "my-service")
                        .build())
                    .build();

                return OpenTelemetrySdk.builder()
                    .setTracerProvider(sdkTracerProvider)
                    .setPropagators(ContextPropagators.create(W3CTraceContextPropagator.getInstance()))
                    .buildAndRegisterGlobal();
            }
        }
        ```
      </Step>

      <Step title="Instrument your LLM calls">
        ```java
        import io.opentelemetry.api.trace.Tracer;

        public class LLMService {
            private final Tracer tracer = OpenTelemetry.getGlobalTracer("my-service");

            public void callLLM() {
                var span = tracer.spanBuilder("llm-call").startSpan();
                try (var scope = span.makeCurrent()) {
                    // Your LLM call here
                } finally {
                    span.end();
                }
            }
        }
        ```
      </Step>
    </Steps>


  ### C#/.NET

    <Steps>
      <Step title="Install OpenTelemetry">
        ```bash
        dotnet add package OpenTelemetry
        dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol
        ```
      </Step>

      <Step title="Configure the exporter">
        ```csharp
        using OpenTelemetry;
        using OpenTelemetry.Resources;
        using OpenTelemetry.Trace;

        public class Program
        {
            public static void Main(string[] args)
            {
                var builder = Sdk.CreateTracerProviderBuilder()
                    .SetResourceBuilder(ResourceBuilder.CreateDefault()
                        .AddService(serviceName: "my-service"))
                    .AddOtlpExporter(opts => opts
                        .Endpoint = new Uri("https://app.langwatch.ai/api/otel/v1/traces")
                        .Headers = "Authorization=Bearer " + Environment.GetEnvironmentVariable("LANGWATCH_API_KEY"))
                    .Build();
            }
        }
        ```
      </Step>

      <Step title="Instrument your LLM calls">
        ```csharp
        using OpenTelemetry.Trace;

        public class LLMService
        {
            private readonly Tracer _tracer = TracerProvider.Default.GetTracer("my-service");

            public async Task<string> CallLLMAsync()
            {
                using var span = _tracer.StartActiveSpan("llm-call");
                // Your LLM call here
                return "response";
            }
        }
        ```
      </Step>
    </Steps>



## Available Instrumentation Libraries

LangWatch works with any OpenTelemetry-compatible instrumentation library. Here are some popular options:

### Java Libraries
- **[Spring AI](https://docs.spring.io/spring-ai/reference/index.html)**: Spring AI provides built-in observability support for AI applications, including OpenTelemetry integration for tracing LLM calls and AI operations
- **OpenTelemetry Java SDK**: Use OpenTelemetry Java SDK with custom spans

### .NET Libraries
- **[Azure Monitor OpenTelemetry](https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-enable?tabs=aspnetcore)**: Azure Monitor OpenTelemetry provides comprehensive OpenTelemetry support for .NET applications, including automatic instrumentation and Azure-specific features
- **OpenTelemetry .NET SDK**: Use OpenTelemetry .NET SDK with custom instrumentation

## Manual Instrumentation

If no automatic instrumentation is available for your LLM provider, you can manually create spans:

```java
import io.opentelemetry.api.trace.Tracer;
import io.opentelemetry.api.trace.Span;
import io.opentelemetry.context.Context;

public class LLMService {
    private final Tracer tracer = OpenTelemetry.getGlobalTracer("my-service");

    public String callLLM(String prompt) {
        Span span = tracer.spanBuilder("llm-call").startSpan();

        try (var scope = span.makeCurrent()) {
            // Add relevant attributes
            span.setAttribute("llm.provider", "custom-provider");
            span.setAttribute("llm.model", "gpt-5-mini");
            span.setAttribute("llm.prompt", prompt);

            // Your LLM call here
            String response = yourLLMClient.generate(prompt);

            return response;
        } finally {
            span.end();
        }
    }
}
```

## Environment Variables

Set these environment variables for authentication:

```bash
export LANGWATCH_API_KEY="your-api-key-here"
```

## Verification

After setting up your instrumentation, you can verify that traces are being sent to LangWatch by:

1. Making a few LLM calls in your application
2. Checking the [LangWatch dashboard](https://app.langwatch.ai/) for incoming traces
3. Looking for spans with your service name and LLM call details

## Troubleshooting

<AccordionGroup>
  <Accordion title="Traces not appearing in LangWatch">
    - Verify your API key is correct and has proper permissions
    - Check that the endpoint URL is correct: `https://app.langwatch.ai/api/otel/v1/traces`
    - Ensure your application is making LLM calls after instrumentation is set up
    - Check network connectivity to the LangWatch endpoint
  </Accordion>

  <Accordion title="Authentication errors">
    - Verify the Authorization header format: `Bearer YOUR_API_KEY`
    - Ensure the API key is valid and not expired
    - Check that the API key has the necessary permissions for trace ingestion
  </Accordion>

  <Accordion title="Performance issues">
    - Consider using batch span processors for high-volume applications
    - Implement sampling to reduce the number of traces sent
    - Use async span processors to avoid blocking your application
  </Accordion>
</AccordionGroup>

## Next Steps

- Explore the [LangWatch dashboard](https://app.langwatch.ai/) to view your traces
- Set up [custom evaluations](/evaluations) for your LLM calls

---

# FILE: ./integration/rags-context-tracking.mdx

---
title: "RAG Context Tracking"
description: Capture the RAG documents used in your LLM pipelines
---

Retrieval Augmented Generation (RAGs) is a common way to augment the generation of your LLM by retrieving a set of documents based on the user query and giving it to the LLM to use as context for answering, either by using a vector database, getting responses from an API, or integrated agent files and memory.

It can be challenging, however, to build a good quality RAG pipeline, making sure the right data was retrieved, preventing the LLM from hallucinating, monitor which documents are the most used and keep iterating to improve it, this is where integrating with LangWatch can help, by integrating your RAG you unlock a series of Guardrails, Measurements and Analytics for RAGs LangWatch.

### Python (General)

To capture a RAG span, you can use the `@langwatch.span(type="rag")` decorator, along with a call to `.update()` to add the `contexts` to the span:

```python
@langwatch.span(type="rag")
def rag_retrieval():
    # the documents you retrieved from your vector database
    search_results = ["France is a country in Europe.", "Paris is the capital of France."]

    # capture them on the span contexts before returning
    langwatch.get_current_span().update(contexts=search_results)

    return search_results
```

If you have document or chunk ids from the results, we recommend you can to capture them along with the id using `RAGChunk`, as this allows them to be grouped together and generate documents analytics on LangWatch dashboard:

```python
from langwatch.types import RAGChunk

@langwatch.span(type="rag")
def rag_retrieval():
    # the documents you retrieved from your vector database
    search_results = [
        {
            "id": "doc-1",
            "content": "France is a country in Europe.",
        },
        {
            "id": "doc-2",
            "content": "Paris is the capital of France.",
        },
    ]

    # capture then on the span contexts with RAGChunk before returning
    langwatch.get_current_span().update(
        contexts=[
            RAGChunk(
                document_id=document["id"],
                content=document["content"],
            )
            for document in search_results
        ]
    )

    return search_results
```

Then you'll be able to see the captured contexts that will also be used later on for evaluatios on LangWatch dashboard:

![RAG Spans](/images/integration/rag.png)

### Python (LangChain)

When using LangChain, generally your RAG happens by calling a [`Retriever`](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/).

We provide a utility `langwatch.langchain.capture_rag_from_retriever` to capture the documents found by the retriever and convert it into a LangWatch compatible format for tracking. For that you need to pass the retriever as first argument, and then a function to map each document to a `RAGChunk`, like in the example below:

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.trace()
def main():
    retriever = ...
    retriever_tool = create_retriever_tool(
        langwatch.langchain.capture_rag_from_retriever(
            retriever,
            lambda document: RAGChunk(
                document_id=document.metadata["source"],
                content=document.page_content
            ),
        ),
        "langwatch_search",
        "Search for information about LangWatch. For any questions about LangWatch, use this tool if you didn't already",
    )

    tools = [retriever_tool]
    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful assistant that only reply in short tweet-like responses, using lots of emojis and use tools only once.\n\n{agent_scratchpad}",
            ),
            ("human", "{question}"),
        ]
    )
    agent = create_tool_calling_agent(model, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return executor.invoke(user_input, config=RunnableConfig(
        callbacks=[langwatch.get_current_trace().get_langchain_callback()]
    ))
```

Alternatively, if you don't use retrievers, but still want to capture the context for example from a tool call that you do, we also provide a utility `langwatch.langchain.capture_rag_from_tool` to capture RAG contexts around a tool. For that you need to pass the tool as first argument, and then a function to map the tool's output to `RAGChunk`s, like in the example below:

```python
import langwatch
from langwatch.types import RAGChunk

@langwatch.trace()
def main():
    my_custom_tool = ...
    wrapped_tool = langwatch.langchain.capture_rag_from_tool(
        my_custom_tool, lambda response: [
          RAGChunk(
            document_id=response["id"], # optional
            chunk_id=response["chunk_id"], # optional
            content=response["content"]
          )
        ]
    )

    tools = [wrapped_tool] # use the new wrapped tool in your agent instead of the original one
    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful assistant that only reply in short tweet-like responses, using lots of emojis and use tools only once.\n\n{agent_scratchpad}",
            ),
            ("human", "{question}"),
        ]
    )
    agent = create_tool_calling_agent(model, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return executor.invoke(user_input, config=RunnableConfig(
        callbacks=[langWatchCallback]
    ))
```

Then you'll be able to see the captured contexts that will also be used later on for evaluatios on LangWatch dashboard:

![RAG Spans](/images/integration/langchain-rag.png)

### TypeScript

To capture a RAG, you can simply start a RAG span inside the trace, giving it the input query being used:

```typescript
const ragSpan = trace.startRAGSpan({
  name: "my-vectordb-retrieval", // optional
  input: { type: "text", value: "search query" },
});

// proceed to do the retrieval normally
```

Then, after doing the retrieval, you can end the RAG span with the contexts that were retrieved and will be used by the LLM:

```typescript
ragSpan.end({
  contexts: [
    {
      documentId: "doc1",
      content: "document chunk 1",
    },
    {
      documentId: "doc2",
      content: "document chunk 2",
    },
  ],
});
```

<Note>
On LangChain.js, RAG spans are captured automatically by the LangWatch callback when using LangChain Retrievers, with `source` as the documentId.
</Note>

### REST API

To track the RAG context when using the REST API, add a new span of type `rag`, you may also refer the LLM generation as the child of it:

```bash
curl -X POST "https://app.langwatch.ai/api/collector" \\
     -H "X-Auth-Token: $API_KEY" \\
     -H "Content-Type: application/json" \\
     -d @- <<EOF
{
  "trace_id": "trace-123",
  "spans": [
    {
      "type": "rag",
      "name": null,
      "span_id": "span-123",
      "input": {
          "type": "text",
          "value": "What is the capital of France?"
      },
      "timestamps": {
          "started_at": $(($(date +%s) * 1000)),
          "finished_at": $((($(date +%s) + 1) * 1000))
      },
      "contexts": [
        {
            "document_id": "doc-1",
            "chunk_id": "0",
            "content": "France is a country in Europe.",
        },
        {
            "document_id": "doc-2",
            "chunk_id": "0",
            "content": "Paris is the capital of France.",
        },
      ]
    },
    {
      "type": "llm",
      "span_id": "span-456",
      "parent_id": "span-123",
      "vendor": "openai",
      "model": "gpt-5",
      "input": {
        "type": "chat_messages",
        "value": [
          {
            "role": "user",
            "content": "Input to the LLM"
          }
        ]
      },
      "outputs": [
        {
          "type": "chat_messages",
          "value": [
              {
                  "role": "assistant",
                  "content": "Output from the LLM",
                  "function_call": null,
                  "tool_calls": []
              }
          ]
        }
      ],
      "params": {
        "temperature": 0.7,
        "stream": false
      },
      "metrics": {
        "prompt_tokens": 100,
        "completion_tokens": 150
      },
      "timestamps": {
        "started_at": $(($(date +%s) * 1000)),
        "finished_at": $((($(date +%s) + 1) * 1000))
      }
    }
  ],
}
EOF
```


---

# FILE: ./prompt-management/cli.mdx

---
title: "Prompts CLI"
description: "Manage AI prompts as code with version control and dependency management"
---

The `langwatch prompt` command provides dependency management for AI prompts as plain YAML files, enabling you to version prompts locally with Git while synchronizing with the LangWatch platform for testing, evaluation, and team collaboration.

## Installation

Install the CLI globally:

```bash
npm install -g langwatch
```

Authenticate:

```bash
langwatch login
```

## Quick Start

### 1. Initialize Your Project

Create a new prompts project:

```bash
langwatch prompt init
```

This will create the following structure to manage your prompts:

```bash
├── prompts # directory to create your prompts
│   └── .materialized # where remote fetched prompts will be stored
├── prompts.json # prompt dependencies
└── prompts-lock.json # lock file
```

### 2. Add Your First Prompt

Create a local prompt:

```bash
langwatch prompt create my-summarizer
```

Or add an existing remote prompt dependency:

```bash
langwatch prompt add agent/customer-service
```

### 3. Synchronize

Sync all prompts (fetch remote, push local changes):

```bash
langwatch prompt sync
```

Go to [app.langwatch.ai](https://app.langwatch.ai) to see your new synced prompts.

## Core Concepts

### Dependency Management

The CLI uses two configuration files:

**`prompts.json`** - Declares your prompt dependencies:

```json
{
  "prompts": {
    "agent/customer-service": "latest",
    "shared/guidelines": "5",
    "my-local-prompt": "file:./prompts/my-local-prompt.prompt.yaml"
  }
}
```

**`prompts-lock.json`** - Tracks resolved versions and materialized file paths:

```json
{
  "lockfileVersion": 1,
  "prompts": {
    "agent/customer-service": {
      "version": 12,
      "versionId": "prompt_version_scRQwSRMIyJvoSxTqP2nR",
      "materialized": "prompts/.materialized/agent/customer-service.prompt.yaml"
    }
  }
}
```

### Local vs Remote Prompts

**Remote Prompts** (`agent/customer-service@latest`)

- Pulled from LangWatch platform
- Fetched and materialized locally in `./prompts/.materialized/`
- Read-only locally

**Local Prompts** (`file:./prompts/my-prompt.prompt.yaml`)

- Stored as local YAML files
- Version controlled with Git
- Pushed to platform during sync for sharing and evaluation

### YAML Format

Prompts files end with `.prompt.yaml` extension and follow this format:

```yaml
model: openai/gpt-5
modelParameters:
  temperature: 0.7
  max_tokens: 1000
messages:
  - role: system
    content: You are a helpful assistant specializing in customer service.
  - role: user
    content: |
      Please help the customer with their inquiry:

      {{customer_message}}
```

This is the same structure as [GitHub Prompts](https://docs.github.com/en/github-models/use-github-models/storing-prompts-in-github-repositories).

## Commands Reference

### `langwatch prompt init`

Initialize a new prompts project in the current directory.

```bash
langwatch prompt init
```

### `langwatch prompt add <spec> [localFile]`

Add a new prompt dependency and immediately fetch/materialize it.

```bash
# Add remote prompt
langwatch prompt add shared/guidelines@latest

# Add specific version
langwatch prompt add agent/support@5

# Add local file as dependency
langwatch prompt add my-prompt ./prompts/my-prompt.prompt.yaml
```

**Arguments:**

- `<spec>` - Prompt specification (name@version or name for latest)
- `[localFile]` - Optional path to local YAML file to add

**Behavior:**

- Updates `prompts.json` with new dependency
- Fetches prompt from server and materializes locally
- Updates `prompts-lock.json` with resolved version

### `langwatch prompt remove <name>`

Remove a prompt dependency and clean up associated files.

```bash
langwatch prompt remove agent/support
```

**Behavior:**

- Removes entry from `prompts.json`
- Removes entry from `prompts-lock.json`
- Deletes materialized file
- For local prompts: deletes source file and warns about server state

### `langwatch prompt create <name>`

Create a new local prompt file with default content.

```bash
langwatch prompt create my-new-prompt
```

**Behavior:**

- Creates `./prompts/<name>.prompt.yaml` with template content
- Automatically adds to `prompts.json` as `file:` dependency
- Updates `prompts-lock.json`

### `langwatch prompt sync`

Synchronize all prompts between local files and the server.

```bash
langwatch prompt sync
```

**Behavior:**

- Fetches remote prompts if new versions available
- Pushes local prompt changes to server
- Handles conflict resolution interactively
- Cleans up orphaned materialized files
- Reports what was synced

**Conflict Resolution:**
When local and remote versions have both changed:

```
Conflict detected for my-prompt:
Local changes: Updated system message
Remote changes: Added temperature parameter

Choose resolution:
  l) Use local version (will create new version on server)
  r) Use remote version (will overwrite local file)
  a) Abort sync for this prompt
```

### `langwatch prompt list`

Display current prompt dependencies and their status.

```bash
langwatch prompt list
```

## CI/CD Integration

Integrate prompt materialization into your deployment pipeline:

```yaml .github/workflows/deploy.yml
name: Deploy with Prompts

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"

      - name: Install LangWatch CLI
        run: npm install -g langwatch

      - name: Materialize prompts
        env:
          LANGWATCH_API_KEY: ${{ secrets.LANGWATCH_API_KEY }}
        run: langwatch prompt sync

      - name: Build application
        run: npm run build

      - name: Deploy with materialized prompts
        run: |
          # Deploy application including prompts/.materialized/
          # Your deployment commands here
```

## Workflows

### Team Collaboration

**Setup:**

1. One team member initializes project with `langwatch prompt init`
2. Commit `prompts.json` and `prompts-lock.json` to Git
3. Add `prompts/.materialized` to `.gitignore`
4. Team members run `langwatch prompt sync` after pulling

**Adding Shared Prompts:**

```bash
# Add organization prompts
langwatch prompt add company/brand-guidelines@latest
langwatch prompt add legal/privacy-policy@^2

# Sync to materialize
langwatch prompt sync
```

**Creating Local Prompts:**

```bash
# Create and develop locally
langwatch prompt create feature/user-onboarding

# Edit the file
vim prompts/feature/user-onboarding.prompt.yaml

# Push to platform for evaluation
langwatch prompt sync
```

### Version Management

**Pinning Versions:**

```json
{
  "prompts": {
    "critical/system-prompt": "5", // Exact version
    "experimental/new-feature": "latest" // Auto-update
  }
}
```

**Upgrading Dependencies:**

```bash
# Edit prompts.json to change versions
# Then sync to fetch new versions
langwatch prompt sync
```

**Rolling Back:**

```bash
# Restore from Git
git checkout prompts.json prompts-lock.json

# Re-sync to match committed state
langwatch prompt sync
```

**Development Workflow:**

```bash
# Create new feature prompt
langwatch prompt create features/new-capability

# Edit and test locally
# When ready, sync to platform
langwatch prompt sync

# Platform team can now evaluate and provide feedback
```

## Coding Assistant Integration

Since prompts are just YAML files, you refer to them directly from other tools or coding assistants.

### Cursor Integration

Reference prompts in a `.cursor/rules/*.mdc` file:

```yaml
---
description:
globs:
alwaysApply: true
---

@/prompts/.materialized/company/java-code-guidelines.prompt.yaml
```

### Cloud Code Integration

Include prompt content in cloud development environments by referencing the YAML files in the `prompts/.materialized` directory.

---

# FILE: ./prompt-management/scope.mdx

---
title: "Scope"
description: "Understand how prompt scope affects access, sharing, and collaboration across projects and organizations"
---

Scope determines where your prompts are accessible and how they can be shared across your LangWatch projects and organization. Understanding scope is crucial for effective prompt management and team collaboration.

## Overview

Every prompt in LangWatch has a **scope** that defines its visibility and accessibility:

- **PROJECT scope** - Prompts accessible only within a single project
- **ORGANIZATION scope** - Prompts shared across all projects in your organization

<Frame>
  <img
    className="block"
    src="/images/prompts/view-set-prompt-scope.png"
    alt="Prompt scope selector in LangWatch UI"
  />
</Frame>

## Project Scope (Default)

**Project scope** is the default setting for all new prompts. Prompts with project scope **are**:

- **Isolated** to a single project
- **Private** to project members only
- **Independent** from other projects
- **Fully controlled** by the project team

### Example: Project-Scoped Prompt

```json
{
  "id": "prompt_abc123",
  "handle": "internal-chatbot",
  "scope": "PROJECT",
  "projectId": "proj_456",
  "organizationId": null
}
```

This prompt is only accessible within `proj_456` and cannot be seen or used by other projects.

## Organization Scope

**Organization scope** makes prompts available across all projects in your organization. Organization-scoped prompts are:

- **Shared** across all organization projects
- **Collaborative** for cross-project teams

### When to Use Organization Scope

- **Brand guidelines** and company voice
- **Legal compliance** prompts (privacy policies, terms of service)
- **Common workflows** used across multiple projects
- **Shared knowledge** and best practices
- **Standard operating procedures**

### Example: Organization-Scoped Prompt

```json
{
  "id": "prompt_def789",
  "handle": "company-brand-voice",
  "scope": "ORGANIZATION",
  "projectId": "proj_456",
  "organizationId": "org_123"
}
```

This prompt is accessible to all projects within `org_123` but can only be modified by the original project (`proj_456`).

## Scope Management

### Creating Scoped Prompts


### UI

1. Navigate to **Prompt Management**
2. Click **"Create New Prompt"**
3. Fill in prompt details
4. Select **Scope** from the dropdown:
   - **Project** - Only visible in current project
   - **Organization** - Visible across all organization projects
5. Save your prompt


### TypeScript SDK

```typescript
import { LangWatch } from "langwatch";

const langwatch = new LangWatch({
apiKey: process.env.LANGWATCH_API_KEY
});

// Create project-scoped prompt (default)
const projectPrompt = await langwatch.prompts.create({
handle: "project-specific-bot",
scope: "PROJECT", // Default, can be omitted
prompt: "You are a project-specific assistant...",
// ... other fields
});

// Create organization-scoped prompt
const orgPrompt = await langwatch.prompts.create({
handle: "company-guidelines",
scope: "ORGANIZATION",
prompt: "You represent our company brand...",
// ... other fields
});

````


### Python SDK

```python
import langwatch

# Create project-scoped prompt (default)
project_prompt = langwatch.prompts.create(
    handle="project-specific-bot",
    scope="PROJECT",  # Default, can be omitted
    prompt="You are a project-specific assistant...",
    # ... other fields
)

# Create organization-scoped prompt
org_prompt = langwatch.prompts.create(
    handle="company-guidelines",
    scope="ORGANIZATION",
    prompt="You represent our company brand...",
    # ... other fields
)
````

### REST API

```bash
# Create project-scoped prompt
curl -X POST "https://app.langwatch.ai/api/prompts" \
  -H "Content-Type: application/json" \
  -H "X-Auth-Token: your-api-key" \
  -d '{
    "handle": "project-specific-bot",
    "scope": "PROJECT",
    "prompt": "You are a project-specific assistant...",
    "messages": [...],
    "inputs": [...],
    "outputs": [...]
  }'

# Create organization-scoped prompt

curl -X POST "https://app.langwatch.ai/api/prompts" \
 -H "Content-Type: application/json" \
 -H "X-Auth-Token: your-api-key" \
 -d '{
"handle": "company-guidelines",
"scope": "ORGANIZATION",
"prompt": "You represent our company brand...",
"messages": [...],
"inputs": [...],
"outputs": [...]
}'

```

### Changing Prompt Scope

<Warning>
A prompt handle that is unique within a project might not be unique across projects in an organization.
When you change the scope of a prompt, you might need to change the handle to avoid conflicts.
</Warning>

To change prompt scope:

1. **Navigate** to the prompt in Prompt Management
2. **Click** the scope dropdown
3. **Select** the new scope
4. **Confirm** the change
5. **Save** the prompt

<Note>
Only prompt owners can change scope.
</Note>
```

---

# FILE: ./prompt-management/overview.mdx

---
title: "Overview"
description: "Organize, version, and optimize your AI prompts with LangWatch's comprehensive prompt management system"
---

LangWatch's prompt management system helps you organize, version, and optimize your AI prompts across your entire application. Whether you're building a simple chatbot or a complex AI workflow, our tools help you maintain consistency, track changes, and collaborate effectively with your team.

<Frame>
  <img
    className="block"
    src="/images/prompts/view-prompt-management-dashboard.png"
    alt="LangWatch Prompt Management Dashboard"
  />
</Frame>

## What You Can Do

- **Organize prompts** in a centralized library accessible to your entire team
- **Track changes** with full version history and easy rollback capabilities
- **Test and iterate** using our Optimization Studio for prompt refinement
- **Collaborate effectively** with team members on prompt development
- **Integrate seamlessly** with your existing workflows via API
- **Monitor performance** by linking prompts to execution traces

## Getting Started

Ready to take control of your prompts? Start with our [quick start guide](/prompt-management/getting-started) to create your first managed prompt.

## Explore Further

- [Quick Start Guide](/prompt-management/getting-started) - Set up your first prompt
- [Understanding Prompts](/prompt-management/data-model) - Learn about prompt structure
- [Analytics](/prompt-management/features/essential/analytics) - Monitor prompt performance
- [Version Control](/prompt-management/features/essential/version-control) - Manage prompt versions
- [Link to Traces](/prompt-management/features/advanced/link-to-traces) - Connect prompts to execution traces
- [API Reference](/api-reference/prompts/overview) - Programmatic access to prompts

---

# FILE: ./prompt-management/features/advanced/guaranteed-availability.mdx

---
title: "Guaranteed Availability"
description: "Ensure your prompts are always available, even in offline or air-gapped environments"
---

Guaranteed availability ensures your application can continue operating with prompts even when disconnected from the LangWatch platform. This is achieved through local prompt materialization using the [Prompts CLI](/prompt-management/cli).

## How It Works

When you use the Prompts CLI to manage dependencies, prompts are **materialized locally** as standard YAML files. The LangWatch SDKs automatically detect and use these materialized prompts when available, providing seamless fallback behavior.

**Benefits:**

- **Offline operation** - Your application works without internet connectivity
- **Air-gapped deployments** - Deploy in secure environments with no external access
- **Reduced latency** - No network calls for prompt retrieval
- **Guaranteed consistency** - Prompts are locked to specific versions in your deployment

## Setting Up Local Materialization

### 1. Initialize Prompt Dependencies

```bash
# Install CLI and authenticate
npm install -g langwatch
langwatch login

# Initialize in your project
langwatch prompt init
```

### 2. Add Prompt Dependencies

Add the prompts your application needs:

```bash
# Add specific prompts your app uses
langwatch prompt add customer-support-bot@5
langwatch prompt add data-analyzer@latest
langwatch prompt add error-handler@3
```

This creates a `prompts.json` file:

```json
{
  "prompts": {
    "customer-support-bot": "5",
    "data-analyzer": "latest",
    "error-handler": "3"
  }
}
```

### 3. Materialize Prompts Locally

```bash
# Fetch and materialize all prompts locally
langwatch prompt sync
```

This creates materialized YAML files:

```
prompts/
└── .materialized/
    ├── customer-support-bot.prompt.yaml
    ├── data-analyzer.prompt.yaml
    └── error-handler.prompt.yaml
```

### 4. Deploy with Materialized Prompts

Include the materialized prompts in your deployment package. Your application can now run completely offline.

## Using Materialized Prompts in Code

The SDKs automatically detect and use materialized prompts when available, falling back to API calls only when necessary.


  ### Python SDK

    ```python offline_app.py
    import langwatch
    from litellm import completion

    # Initialize LangWatch
    langwatch.setup()

    # The SDK will automatically use materialized prompts if available
    # No network call needed if prompt is materialized locally
    prompt = langwatch.prompts.get("customer-support-bot")

    # Compile prompt with variables
    compiled_prompt = prompt.compile(
        user_name="John Doe",
        user_email="john.doe@example.com",
        input="How do I reset my password?"
    )

    # Use with LiteLLM (no need to strip provider prefixes)
    response = completion(
        model=compiled_prompt.model,
        messages=compiled_prompt.messages
    )

    print(response.choices[0].message.content)
    ```

    **Behavior:**
    1. SDK checks for `./prompts/.materialized/customer-support-bot.prompt.yaml`
    2. If found, loads prompt from local file (no network call)
    3. If not found, attempts to fetch from LangWatch API
    4. Throws error if both local file and API are unavailable



  ### TypeScript SDK

    ```typescript offline_app.ts
    import { getPrompt, setupLangWatch } from "langwatch";


    // Initialize LangWatch
    await setupLangWatch();

    # Example 1: Basic usage
    prompt = langwatch.prompts.get("customer-support-bot")
    compiled_prompt = prompt.compile(
        user_name="John Doe",
        input="Help me with my account"
    )

    response = completion(
        model=compiled_prompt.model,
        messages=compiled_prompt.messages
    )

    # Example 2: With tracing
    @langwatch.trace()
    def generate_response():
        prompt = langwatch.prompts.get("customer-support-bot")
        compiled_prompt = prompt.compile(
            user_name="John Doe",
            input="Help me with my account"
        )

        response = completion(
            model=compiled_prompt.model,
            messages=compiled_prompt.messages
        )
        return response.choices[0].message.content

    # Example 3: Offline usage
    prompt = langwatch.prompts.get("customer-support-bot")
    compiled_prompt = prompt.compile(
        user_name="John Doe",
        input="Help me with my account"
    )

    response = completion(
        model=compiled_prompt.model,
        messages=compiled_prompt.messages
    )

    # Example 4: Final example
    prompt = langwatch.prompts.get("customer-support-bot")
    compiled_prompt = prompt.compile(
        user_name="John Doe",
        input="Help me with my account"
    )

    response = completion(
        model=compiled_prompt.model,
        messages=compiled_prompt.messages
    )
    ```

    **Behavior:**
    1. SDK checks for `./prompts/.materialized/customer-support-bot.prompt.yaml`
    2. If found, loads prompt from local file (no network call)
    3. If not found, attempts to fetch from LangWatch API
    4. Throws error if both local file and API are unavailable




## Air-Gapped Deployment

For completely air-gapped environments:

### 1. Prepare on Connected Environment

```bash
# On development machine with internet access
langwatch prompt sync

# Verify all prompts are materialized
ls prompts/.materialized/
```

### 2. Package for Deployment

Include these files in your deployment package:

- `prompts/.materialized/` directory (all YAML files)
- Your application code
- Dependencies

### 3. Deploy to Air-Gapped Environment

The application will run entirely offline, using only materialized prompts. No LangWatch API access required.
{/*
## Advanced Fetch Policies (Future Feature)

<Note>
  **Coming Soon**: Advanced fetch policies will provide fine-grained control
  over when prompts are fetched vs. using materialized versions.
</Note>


  ### Python SDK (Future)

    ```python fetch_policies.py
    import langwatch
    from langwatch.prompt import FetchPolicy

    # Always fetch from API, use materialized as fallback
    prompt = langwatch.prompt.get_prompt(
        "customer-support-bot",
        fetch_policy=FetchPolicy.ALWAYS_FETCH
    )

    # Fetch every 5 minutes, use materialized between fetches
    prompt = langwatch.prompt.get_prompt(
        "customer-support-bot",
        fetch_policy=FetchPolicy.CACHE_TTL,
        cache_ttl_minutes=5
    )

    # Never fetch, use materialized only (air-gapped mode)
    prompt = langwatch.prompt.get_prompt(
        "customer-support-bot",
        fetch_policy=FetchPolicy.MATERIALIZED_ONLY
    )

    # Default behavior: use materialized if available, otherwise fetch
    prompt = langwatch.prompt.get_prompt(
        "customer-support-bot",
        fetch_policy=FetchPolicy.MATERIALIZED_FIRST  # default
    )
    ```



  ### TypeScript SDK (Future)

    ```typescript fetch_policies.ts
    import { getPrompt, FetchPolicy } from "langwatch";

    // Always fetch from API, use materialized as fallback
    const prompt = await getPrompt("customer-support-bot", {
      fetchPolicy: FetchPolicy.ALWAYS_FETCH
    });

    // Fetch every 5 minutes, use materialized between fetches
    const prompt = await getPrompt("customer-support-bot", {
      fetchPolicy: FetchPolicy.CACHE_TTL,
      cacheTtlMinutes: 5
    });

    // Never fetch, use materialized only (air-gapped mode)
    const prompt = await getPrompt("customer-support-bot", {
      fetchPolicy: FetchPolicy.MATERIALIZED_ONLY
    });

    // Default behavior: use materialized if available, otherwise fetch
    const prompt = await getPrompt("customer-support-bot", {
      fetchPolicy: FetchPolicy.MATERIALIZED_FIRST // default
    });
    ```




### Fetch Policy Options

| Policy               | Behavior                                              | Use Case                                         |
| -------------------- | ----------------------------------------------------- | ------------------------------------------------ |
| `MATERIALIZED_FIRST` | Use local file if available, otherwise fetch from API | Default behavior, best for most applications     |
| `ALWAYS_FETCH`       | Always try API first, fall back to materialized       | Live updates with offline fallback               |
| `CACHE_TTL`          | Fetch every X minutes, use materialized between       | Hot deployments with controlled update frequency |
| `MATERIALIZED_ONLY`  | Never fetch, use materialized files only              | Air-gapped or strict offline environments        | */}

## CI/CD Integration

Integrate prompt materialization into your deployment pipeline:

```yaml .github/workflows/deploy.yml
name: Deploy with Prompts

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install LangWatch CLI
        run: npm install -g langwatch

      - name: Materialize prompts
        env:
          LANGWATCH_API_KEY: ${{ secrets.LANGWATCH_API_KEY }}
        run: langwatch prompt sync

      - name: Build application
        run: npm run build

      - name: Deploy with materialized prompts
        run: |
          # Deploy application including prompts/.materialized/
          # Your deployment commands here
```

---

# FILE: ./prompt-management/features/advanced/optimization-studio.mdx

---
title: "Using Prompts in the Optimization Studio"
description: "Use prompts in the Optimization Studio to test and optimize your prompts"
---

### Watch: Prompt Management Tutorial

Get a quick visual overview of how to use the prompt management features in LangWatch:

<Frame>
  <iframe
    width="100%"
    height="400"
    src="https://www.youtube.com/embed/F64y61v72CA"
    title="Prompt Management on LangWatch Optimization Studio"
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  ></iframe>
</Frame>

### Using Prompts in the Optimization Studio

<Frame>
  <img
    className="block"
    src="/images/prompts/prompt-versions-in-studio.png"
    alt="LangWatch Prompt Versions in Studio"
  />
</Frame>

To get started with prompt versioning in the Optimization Studio:

1. Create a new workflow or open an existing one
2. Drag a signature node onto the workspace
3. Click on the node to access configuration options in the right side panel
4. Make your desired changes to the prompt configuration
5. Save your changes as a new version

---

# FILE: ./prompt-management/features/advanced/a-b-testing.mdx

---
title: "A/B Testing"
description: "Implement A/B testing for your prompts using LangWatch's version control and analytics"
---

LangWatch enables A/B testing by allowing you to create different versions of your prompts and randomly alternate between them. Your application can test different prompt variants while LangWatch tracks performance metrics for each version.

## How It Works

1. **Create variants** as different versions of the same prompt
2. **Switch between versions** at runtime with an A/B testing strategy
3. **Track performance** using LangWatch's built-in analytics
4. **Compare results** to see which version performs better

## Implementation

### Create Prompt Variants

Create different versions of your prompt for testing:


  ### TypeScript SDK

    ```typescript
    import { LangWatch } from "langwatch";

    const langwatch = new LangWatch({
      apiKey: process.env.LANGWATCH_API_KEY
    });

    // Create base prompt
    const basePrompt = await langwatch.prompts.create({
      handle: "customer-support-bot",
      scope: "PROJECT",
      prompt: "You are a helpful customer support agent. Help with: {{input}}",
      inputs: [{ identifier: "input", type: "str" }],
      outputs: [{ identifier: "response", type: "str" }],
      model: "openai/gpt-4o-mini"
    });

    // Create variant A (friendly tone) - captures version number
    const variantA = await langwatch.prompts.update("customer-support-bot", {
      prompt: "You are a friendly and empathetic customer support agent. Use a warm, helpful tone. Help with: {{input}}"
    });

    // Create variant B (professional tone) - captures version number
    const variantB = await langwatch.prompts.update("customer-support-bot", {
      prompt: "You are a professional and efficient customer support agent. Be concise and solution-focused. Help with: {{input}}"
    });

    // Store version numbers for A/B testing
    const versions = {
      base: basePrompt.version,
      friendly: variantA.version,
      professional: variantB.version
    };

    console.log("Version numbers:", versions);
    ```



  ### Python SDK

    ```python
    import langwatch

    # Create base prompt
    base_prompt = langwatch.prompts.create(
        handle="customer-support-bot",
        scope="PROJECT",
        prompt="You are a helpful customer support agent. Help with: {{input}}",
        inputs=[{"identifier": "input", "type": "str"}],
        outputs=[{"identifier": "response", "type": "str"}]
    )

    # Create variant A (friendly tone) - captures version number
    variant_a = langwatch.prompts.update(
        "customer-support-bot",
        scope="PROJECT",
        prompt="You are a friendly and empathetic customer support agent. Use a warm, helpful tone. Help with: {{input}}"
    )

    # Create variant B (professional tone) - captures version number
    variant_b = langwatch.prompts.update(
        "customer-support-bot",
        scope="PROJECT",
        prompt="You are a professional and efficient customer support agent. Be concise and solution-focused. Help with: {{input}}"
    )

    # Store version numbers for A/B testing
    versions = {
        "base": base_prompt.version,
        "friendly": variant_a.version,
        "professional": variant_b.version
    }

    print("Version numbers:", versions)
    ```




### Run A/B Tests

Use the captured version numbers to switch between prompt versions at runtime (random sampling):


  ### TypeScript SDK

    ```typescript
    async function generateResponse(userInput: string) {
      // Use the captured version numbers
      const versions = {
        base: 1,
        friendly: 2,
        professional: 3
      };

      // Randomly select a variant
      const variants = [
        { version: versions.base, description: "Base version" },
        { version: versions.friendly, description: "Friendly tone" },
        { version: versions.professional, description: "Professional tone" }
      ];

      const randomVariant = variants[Math.floor(Math.random() * variants.length)];

      // Fetch the selected prompt version
      const prompt = await langwatch.prompts.get("customer-support-bot", {
        version: randomVariant.version
      });

      // Compile and use the prompt
      const compiledPrompt = prompt.compile({ input: userInput });

      // Use with your LLM client
      const result = await generateText({
        model: openai(prompt.model.replace("openai/", "")),
        messages: compiledPrompt.messages
      });

      return {
        response: result.text,
        version: randomVariant.version,
        description: randomVariant.description
      };
    }
    ```


  ### Python SDK

    ```python
    import random

    def generate_response(user_input):
        # Use the captured version numbers
        versions = {
            "base": 1,
            "friendly": 2,
            "professional": 3
        }

        # Randomly select a variant
        variants = [
            {"version": versions["base"], "description": "Base version"},
            {"version": versions["friendly"], "description": "Friendly tone"},
            {"version": versions["professional"], "description": "Professional tone"}
        ]

        random_variant = random.choice(variants)

        # Fetch the selected prompt version
        prompt = langwatch.prompts.get("customer-support-bot", version=random_variant["version"])

        # Compile and use the prompt
        compiled_prompt = prompt.compile(input=user_input)

        # Use with your LLM client
        response = completion(
            model=prompt.model,
            messages=compiled_prompt.messages
        )

        return {
            "response": response.choices[0].message.content,
            "version": random_variant["version"],
            "description": random_variant["description"]
        }
    ```




## Track Performance

LangWatch automatically tracks performance metrics for each prompt version:

- **Response latency** - Which version is faster?
- **Token usage** - Which version is more efficient?
- **Cost per request** - Which version is more cost-effective?
- **Quality scores** - Which version produces better responses?

## Analyze Results

Compare metrics between versions in the LangWatch UI to see which variant performs better. Use this data to make informed decisions about which prompt version to use in production.

---

# FILE: ./prompt-management/features/advanced/link-to-traces.mdx

---
title: "Link to Traces"
description: "Connect prompts to execution traces for performance monitoring and analysis"
---

Linking prompts to traces enables tracking of metrics and evaluations per prompt version. It's the foundation of improving prompt quality over time.

After linking prompts and traces, you will see information about the prompt in the trace's metadata.

<Frame>
  <img
    className="block"
    src="/images/prompts/view-prompt-trace-span.png"
    alt="Prompt information in trace span details"
  />
</Frame>

For more information about traces and spans, see the [Concepts](/concepts) guide.

## How to Link Prompts to Traces

When you use `langwatch.prompts.get()` within a trace context, LangWatch automatically links the prompt to the trace:


### Python SDK


```python
import langwatch
from litellm import completion

# Initialize LangWatch
langwatch.setup()

@langwatch.trace()
def customer_support_generation():
    # Autotrack LiteLLM calls
    langwatch.get_current_trace().autotrack_litellm_calls(litellm)

    # Get prompt (automatically linked to trace when API key is present)
    prompt = langwatch.prompts.get("customer-support-bot")

    # Compile prompt with variables
    compiled_prompt = prompt.compile(
        user_name="John Doe",
        user_email="john.doe@example.com",
        input="I need help with my account"
    )

    response = completion(
        model=prompt.model,
        messages=compiled_prompt.messages
    )

    return response.choices[0].message.content

# Call the function
result = customer_support_generation()
```


### TypeScript SDK


```typescript
import { LangWatch, getLangWatchTracer } from "langwatch";
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";

// Initialize LangWatch client
const langwatch = new LangWatch({
  apiKey: process.env.LANGWATCH_API_KEY,
});

const tracer = getLangWatchTracer("customer-support");

async function customerSupportGeneration() {
  return tracer.withActiveSpan("customer-support-generation", async () => {
    // Get prompt (automatically linked to trace when API key is present)
    const prompt = await langwatch.prompts.get("customer-support-bot");

    // Compile prompt with variables
    const compiledPrompt = prompt.compile({
      user_name: "John Doe",
      user_email: "john.doe@example.com",
      input: "I need help with my account",
    });

    // Use with AI SDK (native instrumentation support)
    const result = await generateText({
      model: openai(prompt.model.replace("openai/", "")),
      messages: compiledPrompt.messages,
      experimental_telemetry: { isEnabled: true },
    });

    return result.text;
  });
}

// Call the function
const result = await customerSupportGeneration();
```

For more detailed information about setting up tracing in your application, see the [Python Integration Guide](/integration/python/guide) or [TypeScript Integration Guide](/integration/typescript/guide).

---

[← Back to Prompt Management Overview](/prompt-management/overview)

---

# FILE: ./prompt-management/features/essential/version-control.mdx

---
title: "Version Control"
description: "Manage prompt versions and track changes over time"
---

# Prompt Version Control

LangWatch provides a robust version control system for managing your prompts. Each prompt can have multiple versions, allowing you to track changes, experiment with different approaches, and rollback when needed.

## Version Management

Every prompt in LangWatch automatically maintains a version history. When you create a new prompt, it starts with version 1, and each subsequent change creates a new version with an incremented number.

**Important**: You cannot delete individual versions - only entire prompts can be deleted. Each update operation creates a new version automatically.

## Scope and Conflicts

Prompts have two scope levels that affect version management and conflict resolution:

- **PROJECT scope** - Prompts are accessible only within the project. Changes are isolated to your project.
- **ORGANIZATION scope** - Prompts are shared across all projects in the organization. Changes can affect other projects and may require conflict resolution.

<Warning>
  **Scope Conflicts**: When updating an organization-scoped prompt, conflicts
  may arise if other projects have made changes. The system will provide
  conflict information to help resolve differences.
</Warning>

## Managing Versions


### UI


Use the LangWatch UI to manage prompt versions:

1. Navigate to the **Prompt Management** section
2. Select a prompt
3. Click on the version history icon at the bottom of the prompt editor
4. Use the version selector to switch between versions
5. Create new versions by making changes and saving


### TypeScript SDK


LangWatch's TypeScript SDK supports retrieving prompts and specific versions:

```typescript
import { LangWatch } from "langwatch";

// Initialize LangWatch client
const langwatch = new LangWatch({
  apiKey: process.env.LANGWATCH_API_KEY,
});

// Get a specific prompt (latest version by default)
const prompt = await langwatch.prompts.get("customer-support-bot");

// The prompt object contains version information
console.log(`Version: ${prompt.version}`);
console.log(`Version ID: ${prompt.versionId}`);

// Get a specific version of a prompt
const specificVersion = await langwatch.prompts.get("customer-support-bot", {
  version: "version_abc123",
});

// Compile with variables
const compiledPrompt = specificVersion.compile({
  user_name: "John Doe",
  user_email: "john.doe@example.com",
  input: "I need help with my account",
});
```


### REST API


Use the REST API to manage prompt versions:

```bash
# Get all versions of a prompt
curl --request GET \
  --url "https://app.langwatch.ai/api/prompts/{prompt_handle}/versions" \
  --header "X-Auth-Token: your-api-key"

# Get a specific prompt (latest version)
curl --request GET \
  --url "https://app.langwatch.ai/api/prompts/{prompt_handle}" \
  --header "X-Auth-Token: your-api-key"

# Create a new version
curl --request POST \
  --url "https://app.langwatch.ai/api/prompts/{prompt_handle}/versions" \
  --header "X-Auth-Token: your-api-key" \
  --header "Content-Type: application/json" \
  --data '{
    "prompt": "Updated prompt text...",
    "model": "openai/gpt-5",
    "commitMessage": "Improved customer support prompt",

    "temperature": 0.7,
    "maxTokens": 1000,
    "responseFormat": {"type": "text"},
    "inputs": [{"identifier": "input", "type": "str"}],
    "outputs": [{"identifier": "response", "type": "str"}],
    "demonstrations": null,
    "promptingTechnique": null
  }'

# Get a specific version
curl --request GET \
  --url "https://app.langwatch.ai/api/prompts/{prompt_handle}?version=2" \
  --header "X-Auth-Token: your-api-key"
```

## CRUD Operations

The SDK provides comprehensive CRUD operations for managing prompts programmatically:

<Note>
**Field Structure**: All examples show the essential fields. Additional optional fields like `temperature`, `maxTokens`, `responseFormat`, `inputs`, `outputs`, `demonstrations`, and `promptingTechnique` can also be set. See the [Data Model](/prompt-management/data-model) page for complete field documentation.
</Note>

### Create Prompts

Create new prompts with templates and variables:

<Warning>
  **System Message Conflict**: You cannot set both a `prompt` (system message)
  and `messages` array with a system role in the same operation. Choose one
  approach to avoid errors.
</Warning>


  ### TypeScript SDK

    ```typescript create_prompt.ts
         // Create a new prompt with a system prompt
     const prompt = await langwatch.prompts.create({
       handle: "customer-support-bot",                    // Required
       scope: "PROJECT",                                  // Required
       prompt: "You are a helpful customer support agent. Help with: {{input}}", // Required
       model: "openai/gpt-4o-mini",                      // Required

       // Optional fields:
       temperature: 0.7,                                  // Optional: Model temperature (0.0-2.0)
       maxTokens: 1000,                                   // Optional: Maximum tokens to generate
       // messages: [...],                                // Optional: Messages array in OpenAI format { role: "system" | "user" | "assistant", content: "..." }
     });

    console.log(`Created prompt with handle: ${prompt.handle}`);
    ```



  ### Python SDK

    ```python create_prompt.py
         # Create a new prompt
     prompt = langwatch.prompts.create(
         handle="customer-support-bot",                    # Required
         scope="PROJECT",                                  # Required
         prompt="You are a helpful customer support agent. Help with: {{input}}", # Required
         model="openai/gpt-4o-mini",                      # Required

         # Optional fields:
         temperature=0.7,                                  # Optional: Model temperature (0.0-2.0)
         max_tokens=1000,                                  # Optional: Maximum tokens to generate
         # messages=[...],                                 # Optional: Messages array in OpenAI format { role: "system" | "user" | "assistant", content: "..." }
     )

         print(f"Created prompt with handle: {prompt.handle}")
    ```




### Update Prompts (Creates New Versions)

Modify existing prompts while maintaining version history:

<Warning>
  **System Message Conflict**: Same rule applies - you cannot set both a
  `prompt` and `messages` array with a system role in the same operation.
</Warning>

<Note>
You must include at least one field to update the prompt.
</Note>


  ### TypeScript SDK

    ```typescript update_prompt.ts
         // Update prompt content (creates new version automatically)
     const updatedPrompt = await langwatch.prompts.update("customer-support-bot", {
       // All fields are optional for updates - only specify what you want to change
       prompt: "You are an expert customer support agent. Help with: {{input}}",

       // Optional fields:
       model: "openai/gpt-4o",                            // Optional: Change the model
       temperature: 0.5,                                  // Optional: Adjust temperature
       maxTokens: 2000,                                  // Optional: Change max tokens
       // messages: [...],                                 // Optional: Messages array in OpenAI format { role: "system" | "user" | "assistant", content: "..." }
     });

         console.log(`Updated prompt: ${updatedPrompt.handle}, New version: ${updatedPrompt.version}`);
    ```



  ### Python SDK

    ```python update_prompt.py
         # Update prompt content (creates new version automatically)
     updated_prompt = langwatch.prompts.update(
         "customer-support-bot",
         # All fields are optional for updates - only specify what you want to change
         prompt="You are an expert customer support agent. Help with: {{input}}",
         model="openai/gpt-4o",                            # Optional: Change the model
         temperature=0.5,                                  # Optional: Adjust temperature
         max_tokens=2000,                                  # Optional: Change max tokens
         # messages=[...],                                 # Optional: Messages array in OpenAI format { role: "system" | "user" | "assistant", content: "..." }
     )

         print(f"Updated prompt: {updated_prompt.handle}, New version: {updated_prompt.version}")
    ```




### Delete Prompts

Remove entire prompts and all their versions:

<Warning>
  **Permanent Deletion**: Deleting a prompt removes ALL versions permanently.
  This action cannot be undone.
</Warning>


  ### TypeScript SDK

    ```typescript delete_prompt.ts
    // Delete by handle (removes all versions)
    const result = await langwatch.prompts.delete("customer-support-bot");

    console.log(`Deletion result: ${result.success}`);
    ```



  ### Python SDK

    ```python delete_prompt.py
    # Delete by handle (removes all versions)
    result = langwatch.prompts.delete("customer-support-bot")

    print(f"Deletion result: {result}")
    ```




## Important Caveats

### System Message Conflicts

<Warning>
  **Critical**: You cannot set both a `prompt` field and a `messages` array
  containing a system role in the same operation. This will throw an error.
</Warning>

**Valid approaches:**

1. **Use `prompt` field only** - Sets the system message directly
2. **Use `messages` array only** - Define the full conversation structure
3. **Mix both** - Use `prompt` for system message and `messages` for user/assistant messages (but no system role in messages)

## Advanced Prompt Capabilities

Beyond basic prompt creation, LangWatch provides powerful features for optimizing and managing your AI interactions:

### Response Format Control
- **Structured Output**: Use `responseFormat: { type: "json_schema" }` to get consistent, parseable responses
- **Text Generation**: Default `responseFormat: { type: "text" }` for free-form responses
- **Custom Schemas**: Define exact output structures for integration with your systems

### Few-Shot Learning
- **Demonstrations**: Use the `demonstrations` field to provide example input/output pairs to improve response quality

### Input/Output Validation
- **Type Safety**: Define expected input types (`str`, `float`, `bool`, `list[str]`, etc.)
- **Output Constraints**: Specify exact output formats and validation rules
- **Variable Management**: Automatically handle prompt variable substitution and validation

### Model Optimization
- **Temperature Control**: Fine-tune creativity vs. consistency (0.0-2.0)
- **Token Limits**: Set `maxTokens` to control response length and costs
- **Model Selection**: Choose the best model for your specific use case

<Tip>
These advanced features are particularly powerful when combined with LangWatch's optimization studio,
where you can A/B test different configurations and measure their impact on performance metrics.
</Tip>

### Optimization Studio Integration

The optimization studio leverages these advanced prompt capabilities to help you:

- **A/B Testing**: Compare different prompt versions, models, and configurations
- **Performance Metrics**: Measure response quality, latency, and cost across variations
- **Automated Optimization**: Let the system find the best combination of settings
- **Version Management**: Track which configurations perform best over time
- **Team Collaboration**: Share optimized prompts across your organization

<Card title="Explore Optimization Studio" icon="rocket" href="/optimization-studio/overview">
Learn how to use advanced prompt features to improve your AI application performance.
</Card>

## Version History

<Frame>
  <img
    className="block"
    src="/images/prompts/version-history.png"
    alt="Prompt version history showing multiple versions with timestamps"
  />
</Frame>

- **Version List**: See all versions with timestamps and commit messages
- **Rollback**: Easily revert to previous versions
{/* TODO: - **Diff View**: Compare changes between versions */}
{/* TODO: - **Branching**: Create experimental versions without affecting production */}

---

[← Back to Prompt Management Overview](/prompt-management/overview)

---

# FILE: ./prompt-management/features/essential/github-integration.mdx

---
title: "GitHub Integration"
description: "Version your prompts in GitHub repositories and automatically sync with LangWatch"
---

LangWatch's prompt management integrates seamlessly with GitHub through the [Prompts CLI](/prompt-management/cli), enabling you to version control your prompts alongside your code and automatically sync changes with the LangWatch platform.

## How It Works

The CLI creates standard YAML files that work perfectly with Git workflows:
- **Local prompts** are stored as `.prompt.yaml` files in your repository
- **Remote prompts** are materialized locally but gitignored (fetched fresh on each sync)
- **Dependencies** are declared in `prompts.json` and locked in `prompts-lock.json`

## Setup for GitHub

### 1. Initialize Prompts in Your Repository

```bash
# Install the CLI
npm install -g langwatch

# Authenticate
langwatch login

# Initialize prompts in your repo
langwatch prompt init
```

This creates the essential files:
```
your-repo/
├── prompts/
│   └── .materialized/      # Add to .gitignore
├── prompts.json            # Commit to Git
└── prompts-lock.json       # Commit to Git
```

### 2. Configure .gitignore

Add the materialized directory to your `.gitignore`:

```gitignore
# LangWatch prompts
prompts/.materialized/
```

This ensures remote prompts are fetched fresh and not committed to your repository.

### 3. Create and Version Your Prompts

Create local prompts that will be versioned with your code:

```bash
# Create a prompt for your feature
langwatch prompt create features/user-onboarding

# Edit the prompt file
vim prompts/features/user-onboarding.prompt.yaml

# Sync to LangWatch platform
langwatch prompt sync
```

Commit your prompt files:
```bash
git add prompts/features/user-onboarding.prompt.yaml prompts.json prompts-lock.json
git commit -m "Add user onboarding prompt"
```

## GitHub Actions Integration

Automatically sync prompts on every push or pull request using GitHub Actions.

Create `.github/workflows/langwatch-sync.yml`:

```yaml
name: LangWatch Prompt Sync

on:
  push:
    branches: [main, develop]
    paths: ['prompts/**', 'prompts.json']
  pull_request:
    branches: [main]
    paths: ['prompts/**', 'prompts.json']

jobs:
  sync-prompts:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install LangWatch CLI
        run: npm install -g langwatch

      - name: Sync prompts
        env:
          LANGWATCH_API_KEY: ${{ secrets.LANGWATCH_API_KEY }}
        run: langwatch prompt sync

      - name: Verify sync
        run: |
          echo "✅ Prompts synced successfully"
          echo "View your prompts at https://app.langwatch.ai"
```

### Setting Up the API Key

1. Go to your [LangWatch project settings](https://app.langwatch.ai/settings)
2. Create new API credentials
3. In your GitHub repository, go to **Settings** → **Secrets and variables** → **Actions**
4. Add a new secret named `LANGWATCH_API_KEY` with your API key value

## Learn More

For complete documentation on all CLI commands, advanced workflows, conflict resolution, and detailed usage examples, see the [Prompts CLI documentation](/prompt-management/cli).

---

# FILE: ./prompt-management/features/essential/analytics.mdx

---
title: "Analytics"
description: "Monitor prompt performance and usage with comprehensive analytics"
---

LangWatch provides analytics to help you understand how your prompts are performing in production.

<Frame>
  <img
    className="block"
    src="/images/prompts/view-prompt-analytics.png"
    alt="Prompt Analytics Dashboard"
  />
</Frame>

## Overview Metrics

Track key usage statistics:

- **Traces**: Total number of prompt executions
- **Threads**: Number of conversation threads
- **Users**: Number of unique users

## LLM Metrics

Monitor your AI model usage:

- **LLM Calls**: Number of API calls made
- **Total Cost**: Cost of all API calls
- **Tokens**: Total tokens consumed

## Version Tracking

- Track prompt behavior by version, compare different versions
- Filter messages, plot usage, cost, conversion on different prompts

## Evaluations Metrics

- Run real-time evaluations on the traces to measure prompt performance
- Use real-time evaluators for classification of prompt outputs

## Custom Graphs

- Create custom bar, line, pie, scatter, and more charts with any captured metrics
- Compare different prompts and versions

---

[← Back to Prompt Management Overview](/prompt-management/overview)

---

# FILE: ./prompt-management/getting-started.mdx

---
title: "Get Started"
description: "Create your first prompt and use it in your application"
---

Learn how to create your first prompt in LangWatch and use it in your application with dynamic variables. This enables your team to update AI interactions without code changes.

## Get API keys

1. Create a LangWatch [account](https://app.langwatch.ai) or set up [self-hosted LangWatch](https://github.com/langwatch/langwatch?tab=readme-ov-file#self-hosted-%EF%B8%8F)
2. Create new API credentials in your [project settings](https://app.langwatch.ai/settings)
3. Note your API key for use in the steps below

## Create a prompt


  ### LangWatch UI

    Use the LangWatch UI to create a new prompt or update an existing one.

    1. Navigate to your project dashboard
    2. Go to **Prompt Management** in the sidebar
    3. Click **"Create New Prompt"**
    4. Fill in the prompt details and save

    <Frame>
      <img
        className="block"
        src="/images/prompts/view-editing-the-prompt.png"
        alt="Editing a prompt in LangWatch UI"
      />
    </Frame>



  ### TypeScript SDK

    ```typescript create_prompt.ts
    import { LangWatch } from "langwatch";

    // Initialize LangWatch client
    const langwatch = new LangWatch({
      apiKey: process.env.LANGWATCH_API_KEY
    });

    // Create a new prompt
    const prompt = await langwatch.prompts.create({
      handle: "customer-support-bot",
      scope: "PROJECT",
      prompt: "You are a helpful customer support agent. Help the customer with their inquiry: {{input}}",
      model: "openai/gpt-4o-mini"
    });

    console.log(`Created prompt with handle: ${prompt.handle}`);
    ```



  ### Python SDK

    ```python create_prompt.py
    import langwatch

    # Create a new prompt
    prompt = langwatch.prompts.create(
        handle="customer-support-bot",
        scope="PROJECT",
        prompt="You are a helpful customer support agent. Help the customer with their inquiry: {{input}}",
        model="openai/gpt-4o-mini"
    )

    print(f"Created prompt with handle: {prompt.handle}")
    ```



  ### REST API

    Use the REST API to create a new prompt:

    ```bash create_prompt.sh
    # Create a new prompt (this creates the prompt with an initial version)
    curl -X POST "https://app.langwatch.ai/api/prompts" \
      -H "Content-Type: application/json" \
      -H "X-Auth-Token: your-api-key" \
      -d '{
        "handle": "customer-support-bot",
        "scope": "PROJECT",
        "prompt": "You are a helpful customer support agent. Help the customer with their inquiry: {{input}}",
        "model": "openai/gpt-4o-mini"
      }'
    ```




## Use prompt

At runtime, you can fetch the latest version of your prompt from LangWatch using the prompt handle.


  ### Python SDK

    ```python use_prompt.py
    import langwatch
    from litellm import completion

    # Get the latest prompt by handle
    prompt = langwatch.prompts.get("customer-support-bot")

    # Compile prompt with variables
    compiled_prompt = prompt.compile(
        user_name="John Doe",
        user_email="john.doe@example.com",
        input="How do I reset my password?"
    )

    # Use with LiteLLM (unified interface to multiple providers)
    response = completion(
        model=prompt.model,  # LiteLLM handles provider prefixes automatically
        messages=compiled_prompt.messages
    )

    print(response.choices[0].message.content)
    ```



  ### TypeScript SDK

    ```typescript use_prompt.ts
    import { LangWatch } from "langwatch";
    import { openai } from "@ai-sdk/openai";
    import { generateText } from "ai";

    // Initialize LangWatch client
    const langwatch = new LangWatch({
      apiKey: process.env.LANGWATCH_API_KEY
    });

    // Get the latest prompt by handle
    const prompt = await langwatch.prompts.get("customer-support-bot");

    // Compile prompt with variables
    const compiledPrompt = prompt.compile({
      user_name: "John Doe",
      user_email: "john.doe@example.com",
      input: "How do I reset my password?"
    });

    // Use with AI SDK
    const result = await generateText({
      model: openai(prompt.model.replace("openai/", "")),
      messages: compiledPrompt.messages,
      experimental_telemetry: { isEnabled: true },
    });

    console.log(result.text);
    ```



  ### REST API

    ```bash use_prompt.sh
    # Get prompt by handle
    curl -X GET "https://app.langwatch.ai/api/prompts/customer-support-bot" \
      -H "X-Auth-Token: your-api-key"
    ```



## Link with LangWatch Tracing

You can link your prompt to LLM generation traces to track performance and see which prompt versions work best. For detailed information about linking prompts to traces, see the [Link to Traces](/prompt-management/features/advanced/link-to-traces) page.


  ### Python SDK

    ```python tracing.py
    import langwatch
    from litellm import completion

    # Initialize LangWatch
    langwatch.setup()

    # Create a trace function
    @langwatch.trace()
    def customer_support_generation():
        # Get prompt (automatically linked to trace when API key is present)
        prompt = langwatch.prompts.get("customer-support-bot")

        # Compile prompt with variables
        compiled_prompt = prompt.compile(
            user_name="John Doe",
            user_email="john.doe@example.com",
            input="I need help with my account"
        )

        # Use with LiteLLM (unified interface to multiple providers)
        response = completion(
            model=prompt.model,  # LiteLLM handles provider prefixes automatically
            messages=compiled_prompt.messages
        )

        return response.choices[0].message.content

    # Call the function
    result = customer_support_generation()
    ```



  ### TypeScript SDK

    ```typescript tracing.ts
    import { LangWatch, getLangWatchTracer } from "langwatch";
    import { openai } from "@ai-sdk/openai";
    import { generateText } from "ai";

    // Initialize LangWatch client
    const langwatch = new LangWatch({
      apiKey: process.env.LANGWATCH_API_KEY
    });

    const tracer = getLangWatchTracer("customer-support");

    async function customerSupportGeneration() {
      return tracer.withActiveSpan("customer-support-generation", async () => {
        // Get prompt (automatically linked to trace when API key is present)
        const prompt = await langwatch.prompts.get("customer-support-bot");

        // Compile prompt with variables
        const compiledPrompt = prompt.compile({
          user_name: "John Doe",
          user_email: "john.doe@example.com",
          input: "I need help with my account",
        });

        // Use with AI SDK
        const { text } = await generateText({
          model: openai(prompt.model.replace("openai/", "")),
          messages: compiledPrompt.messages
        });

        return text;
      });
    }

    // Call the function
    const result = await customerSupportGeneration();
    ```



---

[← Back to Prompt Management Overview](/prompt-management/overview)

---

# FILE: ./prompt-management/data-model.mdx

---
title: "Data Model"
description: "Understand the structure of prompts in LangWatch"
---

# Prompt Data Model

This page explains the structure of prompts in LangWatch and how they're organized.

## Overview

Prompts in LangWatch contain all the information needed to generate AI responses, including the prompt text, model configuration, and optimization settings.

## Complete Prompt Structure

When you retrieve a prompt, you get all the configuration in a single response:

```json
{
  "id": "prompt_TrYXZLsiTJkn9N6PiZiae",
  "handle": "customer-support-bot",
  "scope": "PROJECT",
  "projectId": "proj_123",
  "organizationId": null,
  "version": 1,
  "versionId": "version_abc123",
  "createdAt": "2024-01-15T10:30:00Z",
  "updatedAt": "2024-01-15T10:30:00Z",
  "authorId": "user_789",
  "deletedAt": null,

  "prompt": "You are a helpful customer support agent...",
  "model": "openai/gpt-4o-mini",
  "temperature": 0.7,
  "maxTokens": 1000,
  "messages": [
    { "role": "system", "content": "You are a helpful customer support agent..." },
    { "role": "user", "content": "I need help with my account" }
  ],
  "inputs": [
    { "name": "user_name", "type": "string" },
    { "name": "user_email", "type": "string" }
  ],
  "outputs": [
    { "name": "user_name", "type": "string" },
    { "name": "user_email", "type": "string" }
  ],
  "demonstrations": {
    "columns": [
      { "id": "input", "name": "User Input", "type": "string" },
      { "id": "output", "name": "Expected Output", "type": "string" }
    ],
    "rows": [
      { "id": "example_1", "input": "I need help with my account", "output": "I'd be happy to help you with your account. What specific issue are you experiencing?" }
    ]
  },
  "promptingTechnique": "chain_of_thought"
}
```

## Field Descriptions

### Core Fields

<ResponseField name="id" type="string" required>
Unique identifier for the prompt
</ResponseField>

<ResponseField name="handle" type="string" required>
Human-readable identifier for the prompt (primary way to reference prompts)
</ResponseField>

<ResponseField name="projectId" type="string" required>
The project that owns the prompt
</ResponseField>

<ResponseField name="organizationId" type="string | null">
The organization that owns the prompt
</ResponseField>

<ResponseField name="version" type="integer" required>
Current version number
</ResponseField>

<ResponseField name="versionId" type="string" required>
Unique identifier for this version
</ResponseField>

<ResponseField name="createdAt" type="timestamp" required>
When this version was created
</ResponseField>

<ResponseField name="updatedAt" type="timestamp" required>
When the prompt was last updated
</ResponseField>

<ResponseField name="authorId" type="string" required>
The user who created this version
</ResponseField>

<ResponseField name="deletedAt" type="timestamp | null">
Soft delete timestamp (null if not deleted)
</ResponseField>

### Scope and Access

<ResponseField name="scope" type="string" required default="PROJECT">
- `PROJECT` - Prompts are only accessible within the project
- `ORGANIZATION` - Prompts are shared across all projects in the organization
</ResponseField>


### Model Configuration

<ResponseField name="model" type="string" required>
The LLM model to use (e.g., "openai/gpt-4o-mini"). Model names follow the litellm structure ("provider/model")
</ResponseField>

<ResponseField name="temperature" type="float" required>
Fine-tune creativity vs. consistency (0.0 = deterministic, 2.0 = very creative)
</ResponseField>

<ResponseField name="maxTokens" type="integer" required>
Control response length and costs
</ResponseField>

### Content Fields

<ResponseField name="prompt" type="string" required>
The main prompt text (system message)
</ResponseField>

<ResponseField name="messages" type="array">
Array of chat messages with roles and content (alternative to prompt field)
</ResponseField>

## Variable System

### Variable Formatting

Prompts use `{{ variable_name }}` syntax for dynamic content:

```text
You are a helpful customer support agent. The user is {{user_name}} and their email is {{user_email}}.

Please help them with: {{input}}
```

### Supported Variable Types

- **Strings**: `{{user_name}}`
- **Numbers**: `{{count}}`
- **Booleans**: `{{is_premium}}`
- **Lists**: `{{items}}`
- **Objects**: `{{user_data}}` (will be converted to string)

## Optimization Features

<Note>
**Studio Only**: These advanced optimization features require the optimization studio interface for proper experimentation,
performance measurement, and A/B testing. They cannot be configured via the API.
</Note>

### Input/Output Definitions

<ResponseField name="inputs" type="array">
Array of input variable definitions with identifiers and types
</ResponseField>

<ResponseField name="outputs" type="array">
Array of output variable definitions with identifiers and types
</ResponseField>

### Type System

<AccordionGroup>
<Accordion title="Input Types">
- `"str"` - String values
- `"float"` - Floating point numbers
- `"bool"` - Boolean values
- `"image"` - Image data
- `"list[str]"` - List of strings
- `"list[float]"` - List of floats
- `"list[int]"` - List of integers
- `"list[bool]"` - List of booleans
- `"dict"` - Dictionary/object
</Accordion>

<Accordion title="Output Types">
- `"str"` - String responses
- `"float"` - Numeric responses
- `"bool"` - Boolean responses
- `"json_schema"` - Structured JSON responses
</Accordion>
</AccordionGroup>

### Advanced Features
<ResponseField name="promptingTechnique" type="object">
Configuration for advanced prompting strategies

The prompting strategy to use.
  - `"few_shot"` - Few-shot learning with examples
  - `"in_context"` - In-context learning approach
  - `"chain_of_thought"` - Chain-of-thought reasoning

</ResponseField>

<ResponseField name="demonstrations" type="object">
Few-shot examples with columns and rows structure
</ResponseField>

### Demonstrations Structure

```json
{
  "demonstrations": {
    "columns": [
      { "id": "input", "name": "User Input", "type": "string" },
      { "id": "output", "name": "Expected Output", "type": "string" }
    ],
    "rows": [
      {
        "id": "example_1",
        "input": "I need help with my account",
        "output": "I'd be happy to help you with your account. What specific issue are you experiencing?"
      }
    ]
  }
}
```
---

# FILE: ./features/batch-evaluations.mdx

---
title: Batch Evaluations
---

If you intend to conduct batch evaluations on the datasets you've created in LangWatch, we offer a Python SKD to facilitate this process. This guide aims to provide comprehensive instructions on leveraging our Python SDK to execute batch evaluations effectively.

### Usage

After adding records to your dataset, created within the dataset section of LangWatch, you can proceed to select the dataset for batch evaluation along with the desired evaluations. You have the option to choose from predefined evaluations or any custom evaluations you've set up in the Evaluation and Guardrails section of LangWatch.

### Screenshots examples

In the below in screenshot you will see the datasets section in LangWatch, you can get your batch evaluation python snippet by clicking on on the Batch Evaluation button.

<Frame>
<img
  className="block"
  src="/images/screenshot-datasets-page.png"
  alt="LangWatch"
/>
</Frame>

In the below screenshot you will see where you can select the dataset you want to evaluate on as well as selecting which evaluations you would like to run. Each tab has different evaluation you can choose from.

<Frame>
<img
  className="block"
  src="/images/screenshot-batch-evaluation-drawer.png"
  alt="LangWatch"
/>
</Frame>

In the screenshot below, you'll find a Python code snippet ready for execution to perform your batch processing. The parameters passed into the `BatchEvaluation` include your chosen dataset and an array of selected evaluations to run against it.

<Frame>
<img
  className="block"
  src="/images/screenshot-batch-evaluation-python.png"
  alt="LangWatch"
/>
</Frame>
We've streamlined the process by setting up pandas for you, enabling seamless evaluation of datasets directly on the results object. This means you can leverage the power of pandas' data manipulation and analysis capabilities effortlessly within your evaluation workflow. With pandas at your disposal, you can efficiently explore, analyze, and manipulate your data to derive valuable insights without the need for additional setup or configuration.

### Python snippet

When executing the snippet, you'll encounter a callback function at your disposal. This function contains the original entry data, allowing you to run it against your own Large Language Model (LLM). You can utilize this response to compare results within your evaluation process.

Ensure that you return the `output` as some evaluations may require it. As you create your code snippet in the evaluations tab, you'll notice indications of which evaluations necessitate particular information. Utilize this guidance as a reference to kickstart your workflow effectively.

---

# FILE: ./features/triggers.mdx

---
title: Alerts and Triggers
description: Be alerted when something goes wrong and trigger actions automatically
---

## Create triggers based on LangWatch filters

LangWatch offers you the possibility to create triggers based on your selected filters. You can use these triggers to send notifications to either Slack or selected team email adresses.

#### Usage

To create a trigger in the LangWatch dashboard, follow these steps:

- Click the filter button located at the top right of the LangWatch dashboard.
- After creating a filter, a trigger button will appear.
- Click the trigger button to open a popout drawer.
- In the drawer, you can configure your trigger with the desired settings.

<Frame>
<img
  className="block"
  src="/images/trigger-screenshot-button.png"
  alt="LangWatch"
/>
</Frame>

**Trigger actions**

<Frame>
<img
  className="block"
  src="/images/trigger-screenshot-drawer.png"
  alt="LangWatch"
/>
</Frame>

Once the trigger is created, you will receive an alert whenever a message meets the criteria of the trigger. These trigger checks are run on the minute but not instantaneously, as the data needs time to be processed. You can find the created triggers under the Settings section, where you can deactivate or delete a trigger to stop receiving notifications.

**Trigger settings**

<Frame>
<img
  className="block"
  src="/images/trigger-screenshot-settings.png"
  alt="LangWatch"
/>
</Frame>

---

# FILE: ./features/embedded-analytics.mdx

---
title: Exporting Analytics
description: Build and integrate LangWatch graphs on your own systems and applications
---

## Export Analytics with REST Endpoint

LangWatch offers you the possibility to build and integrate LangWatch graph's on your own systems and applications, to display it to your customers in another interface.

On LangWatch dashboard, you can use our powerful custom chart builder tool, to plot any data collected and generated by LangWatch, and customize the way you want to display it. You can then use our REST API to fetch the graph data.

**Usage:**
You will need to obtain your JSON payload from the custom graph section in our application. You can find this on the Analytics page > Custom Reports > Add chart.

    1. Pick the custom graph you want to get the analytics for.
    2. Prepare your JSON data. Make sure it's is the same format that is showing in the LangWatch application.
    3. Use the `curl` command to get you analytics data. Here is a basic template:

```bash
# Set your API key and endpoint URL
API_KEY="your_langwatch_api_key"
ENDPOINT="https://app.langwatch.ai/api/analytics"

# Use curl to send the POST request, e.g.:
curl -X POST "$ENDPOINT" \
    -H "X-Auth-Token: $API_KEY" \
    -H "Content-Type: application/json" \
    -d @- <<EOF
    {
     "startDate": 1708434000000,
     "endDate": 1710939600000,
     "filters": {},
     "series": [
       {
         "metric": "metadata.user_id",
         "aggregation": "cardinality"
       }
     ],
     "timeScale": 1
   }
EOF
```

    4. Execute the `curl` command. If successful, LangWatch will return the custom analytics data in the response.

## Screenshots on how to get the JSON data.

On the right menu button above the graph you will see the **Show API** menu link. Click that and a modal will then popup.

<Frame>
<img className="block" src="/images/screenshot-show-json.png" alt="Custom graph in the LangWatch dashboard" />
</Frame>

Within this modal, you'll find the JSON payload required for the precise custom analytics
data. Simply copy this payload and paste it into the body of your REST POST request.

<Frame>
<img
  className="block"
  src="/images/screenshot-json-modal.png"
  alt="Model showing the example cURL request to request a view of the custom graph"
/>
</Frame>

Now you're fully prepared to access your customized analytics and seamlessly integrate
them into your specific use cases.

If you encounter any hurdles or have questions, our support team is eager to assist you.

---

# FILE: ./features/annotations.mdx

---
title: Annotations
description: Collaborate with domain experts using annotations
---

# Create annotations on messages

With annotations, you can add additional information to messages. This can be useful to comment on or add any other information that you want to add to a message for further analysis.

We have also implemented the option to add a scoring system for each annotation, more information about this can be found in the [Annotation Scoring](/features/annotations#annotation-scoring) section

If you want to add an annotation to a queue, you can do so by clicking on the add to queue button to send the messages to the queue for later analysis. You can create queues and add members to them on the the main annotations page. More information about this can be found in the [Annotation Queues](/features/annotations#annotation-queues) section.

## Usage

To create an annotation, follow these steps:

1) Click the message you want to annotate on and a [Trace](/concepts#traces) details drawer will open.
2) On the top right, click the annotation button.
3) Here you will be able to add a comment, a link or any other information that you want to add to the message.

<Frame>
<img className="block" src="/images/annotations-trace.png" alt="LangWatch" />
</Frame>

Once you have created an annotation, you will see it next to to the message.

<Frame>
<img className="block" src="/images/annotations-comment.png" alt="LangWatch" />
</Frame>

# Annotation Queues

To get started with annotation queues, follow these steps:

1) Go to the annotations page.
2) Click the plus button to create a new queue.
3) Add a name for your queue, description, members and click on the "Save" button.

<Frame>
<img className="block" src="/images/annotations-create-queue.png" alt="LangWatch" />
</Frame>

Once you have created your queue, you will be able to select this when creating an annotation and send the messages to the queue or directly to a project member for later analysis.

<Frame>
<img className="block" src="/images/annotation-add-to-queue.png" alt="LangWatch" />
</Frame>

Once you add an item to the queue, you can view it in the annotations section, whether it's in a queue or sent directly to you.

<Frame>
<img className="block" src="/images/annotation-queues.png" alt="LangWatch" />
</Frame>

When clicking on a queue item, you will be directed to the message where you can add an annotation. Once happy with your annotation, you can click on the "Done" button and move on to the next item.

<Frame>
<img className="block" src="/images/annotation-queue-items.png" alt="LangWatch" />
</Frame>

Once you’ve completed the final item in the queue, you’ll see that all tasks are done. That’s it! Happy annotating!

<Frame>
<img className="block" src="/images/annotation-queue-items-complete.png" alt="LangWatch" />
</Frame>


# Annotation Scoring

We have developed a customized scoring system for each annotation. To get started, you will need to create your scores on the settings page.

There are two types of score data you can choose from:

- **Checkbox**: To add multiple selectable options.
- **Multiple Choice**: To add a single selectable option.


<Frame>
<img className="block" src="/images/annotation-add-score.png" alt="LangWatch" />
</Frame>

After you have created your scores, you can activate or deactivate them on the settings page.

<Frame>
<img className="block" src="/images/annotation-view-scores.png" alt="LangWatch" />
</Frame>

Once your scores are activated, you will see them in the annotations tab. For each annotation you create, the score options will be available, allowing you to add more detailed information to your annotations.
When annotating a message, you will see the score options below the comment input. Once you have added a score, you will be asked for an optional reason for the score.

<div style={{ display: 'flex', gap: '20px' }}>
  <Frame caption="Score selection">
  <img className="block" src="/images/annotation-score-selection.png" alt="LangWatch" />
  </Frame>
  <Frame caption="Score reason">
  <img className="block" src="/images/annotation-score-reason.png" alt="LangWatch" />
  </Frame>
</div>

Thats it! You can now annotate messages and add your custom score metrics to them.


---

# FILE: ./evaluations/custom-evaluator-integration.mdx

---
title: Instrumenting Custom Evaluator
description: Add your own evaluation results into LangWatch trace
---

If you have a custom evaluator built in-house which run on your own code, either during the LLM pipeline or after, you can still capture the evaluation results
and connect it back to the trace to visualize it together with the other LangWatch evaluators.

### Python


You can capture the evaluation results of your custom evaluator on the current trace or span by using the `.add_evaluation` method:

```python
import langwatch

@langwatch.span(type="evaluation")
def evaluation_step():
    ... # your custom evaluation logic

    langwatch.get_current_span().add_evaluation(
        name="custom evaluation", # required
        passed=True,
        score=0.5,
        label="category_detected",
        details="explanation of the evaluation results",
    )
```

The evaluation `name` is required and must be a string. The other fields are optional, but at least one of `passed`, `score` or `label` must be provided.


### TypeScript


You can capture the evaluation results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

```typescript
import { type LangWatchTrace } from "langwatch";

async function llmStep({ message, trace }: { message: string, trace: LangWatchTrace }): Promise<string> {
    const span = trace.startLLMSpan({ name: "llmStep" });

    // ... your existing code

    span.addEvaluation({
        name: "custom evaluation",
        passed: true,
        score: 0.5,
        label: "category_detected",
        details: "explanation of the evaluation results",
    });
}
```

The evaluation `name` is required and must be a string. The other fields are optional, but at least one of `passed`, `score` or `label` must be provided.


### REST API


## REST API Specification

### Endpoint

`POST /api/collector`

### Headers

- `X-Auth-Token`: Your LangWatch API key.

### Request Body

```javascript
{
  "trace_id": "id of the message the evaluation was run on",
  "evaluations": [{
    "evaluation_id": "evaluation-id-123", // optional unique id for identifying the evaluation, if not provided, a random id will be generated
    "name": "custom evaluation", // required
    "passed": true, // optional
    "score": 0.5, // optional
    "label": "category_detected", // optional
    "details": "explanation of the evaluation results", // optional
    "error": { // optional to capture error details in case evaluation had an error
      "message": "error message",
      "stacktrace": [],
    },
    "timestamps": { // optional
      "created_at": "1723411698506", // unix timestamp in milliseconds
      "updated_at": "1723411698506" // unix timestamp in milliseconds
    }
  }]
}
```


---

# FILE: ./evaluations/evaluation-by-thread.mdx

---
title: Evaluation by Thread
description: Evaluate your LLM applications by thread
---

With LangWatch, you can evaluate your LLM applications by thread. This approach is useful for analyzing the performance of your LLM applications across entire conversation threads, helping you identify which threads are performing well or poorly.

To set up evaluation by thread, toggle the thread-based mapping option when creating an evaluation.

<Frame>
<img className="block" src="/images/dataset-thread-evaluation.png" alt="LangWatch Evaluation by Thread" />
</Frame>

This enables thread-based evaluation where each time a trace is evaluated, the full thread context is retrieved and passed to the evaluation function. This approach builds upon the complete conversation thread rather than individual traces.

By default, we include the trace INPUT and OUTPUT fields in the evaluation. You can add additional fields to the evaluation by including them in your dataset.
---
