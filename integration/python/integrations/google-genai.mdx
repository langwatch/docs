---
title: Google GenAI Instrumentation
sidebarTitle: Google GenAI
description: Learn how to instrument Google GenAI API calls with the LangWatch Python SDK
keywords: google genai, gemini, instrumentation, autotrack, openinference, openllmetry, LangWatch, Python
---

LangWatch offers robust integration with Google GenAI, allowing you to capture detailed information about your Gemini API calls automatically. The recommended approach is to use OpenInference instrumentation, which provides comprehensive tracing for Google GenAI API calls and integrates seamlessly with LangWatch.

## Using OpenInference Instrumentation

The recommended approach for instrumenting Google GenAI calls with LangWatch is to use the OpenInference instrumentation library, which provides comprehensive tracing for Google GenAI API calls.

## Installation and Setup

If you prefer to use broader OpenTelemetry-based instrumentation, or are already using libraries like `OpenInference` or `OpenLLMetry`, LangWatch can seamlessly integrate with them. These libraries provide instrumentors that automatically capture data from various LLM providers, including Google GenAI.

There are two main ways to integrate these:

### 1. Via `langwatch.setup()`

You can pass an instance of the instrumentor (e.g., `GoogleGenAIInstrumentor` from OpenInference or OpenLLMetry) to the `instrumentors` list in the `langwatch.setup()` call. LangWatch will then manage the lifecycle of this instrumentor.

```python
import langwatch
import google.generativeai as genai
import os

# Example using OpenInference's GoogleGenAIInstrumentor
from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor

# Initialize LangWatch with the GoogleGenAIInstrumentor
langwatch.setup(
    instrumentors=[GoogleGenAIInstrumentor()]
)

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

@langwatch.trace(name="Google GenAI Call with Community Instrumentor")
def generate_text_with_community_instrumentor(prompt: str):
    # No need to call autotrack explicitly, the community instrumentor handles Google GenAI calls globally.
    response = model.generate_content(prompt)
    return response.text

if __name__ == "__main__":
    user_query = "Tell me a joke about Python programming."
    response = generate_text_with_community_instrumentor(user_query)
    print(f"User: {user_query}")
    print(f"AI: {response}")
```

<Note>
Ensure you have the respective community instrumentation library installed (e.g., `pip install openinference-instrumentation-google-genai` or `pip install opentelemetry-instrumentation-google-genai`).
</Note>

### 2. Direct Instrumentation

If you have an existing OpenTelemetry `TracerProvider` configured in your application (or if LangWatch is configured to use the global provider), you can use the community instrumentor's `instrument()` method directly. LangWatch will automatically pick up the spans generated by these instrumentors as long as its exporter is part of the active `TracerProvider`.

```python
import langwatch
import google.generativeai as genai
import os
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter

from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor

langwatch.setup()

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

# Instrument Google GenAI directly using the community library
GoogleGenAIInstrumentor().instrument()

@langwatch.trace(name="Google GenAI Call with Direct Community Instrumentation")
def get_story_ending(beginning: str):
    response = model.generate_content(
        f"You are a creative writer. Complete the story: {beginning}"
    )
    return response.text

if __name__ == "__main__":
    story_start = "In a land of dragons and wizards, a young apprentice found a mysterious map..."
    ending = get_story_ending(story_start)
    print(f"Story Start: {story_start}")
    print(f"AI's Ending: {ending}")
```

### Key points for community instrumentors:
- These instrumentors often patch Google GenAI at a global level, meaning all Google GenAI calls from any client instance will be captured once instrumented.
- If using `langwatch.setup(instrumentors=[...])`, LangWatch handles the setup.
- If instrumenting directly (e.g., `GoogleGenAIInstrumentor().instrument()`), ensure that the `TracerProvider` used by the instrumentor is the same one LangWatch is exporting from. This usually means LangWatch is configured to use an existing global provider or one you explicitly pass to `langwatch.setup()`.

## Advanced Usage Examples

### Using Google GenAI with Function Calling

When using Google GenAI's function calling capabilities, the instrumentation will capture both the initial request and the function execution:

```python
import langwatch
import google.generativeai as genai
import os

langwatch.setup()

# Configure Google GenAI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

@langwatch.trace(name="Google GenAI Function Call")
def get_weather_with_functions(city: str):
    # Define the function schema
    function_declarations = [
        {
            "name": "get_weather",
            "description": "Get the current weather for a city",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "The city name"}
                },
                "required": ["city"]
            }
        }
    ]
    
    response = model.generate_content(
        f"What's the weather like in {city}?",
        generation_config=genai.types.GenerationConfig(
            function_calling_config=genai.types.FunctionCallingConfig(
                function_declarations=function_declarations
            )
        )
    )
    return response

if __name__ == "__main__":
    result = get_weather_with_functions("San Francisco")
    print(f"Response: {result}")
```

<Note>
### Which Approach to Choose?

- **`autotrack_google_genai_calls()`** is ideal for targeted instrumentation within specific traces or when you want fine-grained control over which Google GenAI client instances are tracked. It's simpler if you're not deeply invested in a separate OpenTelemetry setup.
- **Community Instrumentors** are powerful if you're already using OpenTelemetry, want to capture Google GenAI calls globally across your application, or need to instrument other libraries alongside Google GenAI with a consistent OpenTelemetry approach. They provide a more holistic observability solution if you have multiple OpenTelemetry-instrumented components.

Choose the method that best fits your existing setup and instrumentation needs. Both approaches effectively send Google GenAI call data to LangWatch for monitoring and analysis.
</Note> 