---
title: Vercel AI SDK
description: LangWatch Vercel AI SDK integration guide
sidebarTitle: Vercel AI SDK
keywords: vercel ai sdk, langwatch, tracing, observability, vercel, ai, sdk, langwatch, tracing, observability
---

import TypeScriptIntro from "/snippets/typescript-intro.mdx";

<TypeScriptIntro />

## Integration


The Vercel AI SDK supports tracing via Next.js OpenTelemetry integration. By using the `LangWatchExporter`, you can automatically collect those traces to LangWatch.

First, you need to install the necessary dependencies:

```bash
npm install @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```

Then, set up the OpenTelemetry for your application, follow one of the tabs below depending whether you are using AI SDK with Next.js or on Node.js:

<Tabs>
<Tab title="Next.js">
You need to enable the `instrumentationHook` in your `next.config.js` file if you haven't already:

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    instrumentationHook: true,
  },
};

module.exports = nextConfig;
```

Next, you need to create a file named `instrumentation.ts` (or `.js`) in the __root directory__ of the project (or inside `src` folder if using one), with `LangWatchExporter` as the traceExporter:

<CodeGroup>
```typescript
import { registerOTel } from '@vercel/otel';
import { LangWatchExporter } from 'langwatch';

export function register() {
  registerOTel({
    serviceName: 'next-app',
    traceExporter: new LangWatchExporter({
      apiKey: process.env.LANGWATCH_API_KEY
    }),
  })
}
```
</CodeGroup>

(Read more about Next.js OpenTelemetry configuration [on the official guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry#manual-opentelemetry-configuration))

Finally, enable `experimental_telemetry` tracking on the AI SDK calls you want to trace:

```typescript
import { attributes } from 'langwatch';

const result = await generateText({
  model: openai('gpt-5'),
  prompt: 'Explain why a chicken would make a terrible astronaut, be creative and humorous about it.',
  experimental_telemetry: {
    isEnabled: true,
    // optional metadata
    metadata: {
      "langwatch.user.id": "myuser-123",
      "langwatch.thread.id": "mythread-123",
    },
  },
});
```
</Tab>
<Tab title="Node.js">
For Node.js, start by following the official OpenTelemetry guide:

- [OpenTelemetry Node.js Getting Started](https://opentelemetry.io/docs/languages/js/getting-started/nodejs/)

Once you have set up OpenTelemetry, you can use `setupObservability` from the LangWatch
SDK to automatically instrument your application and send your traces to LangWatch:

```typescript
import { setupObservability } from 'langwatch/observability/setup/node';

const handle = await setupObservability({
  langwatch: {
    apiKey: process.env.LANGWATCH_API_KEY, // optional, defaults to LANGWATCH_API_KEY env var
  },
  serviceName: "my-service",
});
```
</Tab>
</Tabs>

That's it! Your messages will now be visible on LangWatch:

![Vercel AI SDK](/images/integration/vercel-ai-sdk.png)

## Example Project

You can find a full example project with a more complex pipeline and Vercel AI SDK and LangWatch integration [on our GitHub](https://github.com/langwatch/langwatch/tree/main/typescript-sdk/examples/vercel-ai).
