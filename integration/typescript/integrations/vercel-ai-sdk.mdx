---
title: Vercel AI SDK
description: LangWatch Vercel AI SDK integration guide
sidebarTitle: Vercel AI SDK
keywords: vercel ai sdk, langwatch, tracing, observability, vercel, ai, sdk
---

import TypeScriptIntro from "/snippets/typescript-intro.mdx";

<TypeScriptIntro />

## Installation

<CodeGroup>
```bash npm
npm i langwatch ai @ai-sdk/openai
```

```bash pnpm
pnpm add langwatch ai @ai-sdk/openai
```

```bash yarn
yarn add langwatch ai @ai-sdk/openai
```

```bash bun
bun add langwatch ai @ai-sdk/openai
```
</CodeGroup>

## Usage

Set up observability and enable telemetry on your Vercel AI SDK calls:

```typescript
import { setupObservability } from "langwatch/observability/node";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

setupObservability({ serviceName: "<project_name>" });

async function main(message: string): Promise<string> {
  const response = await generateText({
    model: openai("gpt-5-mini"),
    prompt: message,
    experimental_telemetry: { isEnabled: true },
  });
  return response.text;
}

console.log(await main("Hey, tell me a joke"));
```

The Vercel AI SDK automatically sends traces to LangWatch when `experimental_telemetry.isEnabled` is set to `true`. For Next.js applications, configure OpenTelemetry in your `instrumentation.ts` file using `LangWatchExporter`.

## Related

- [Capturing RAG](/integration/typescript/tutorials/capturing-rag) - Learn how to capture RAG data from retrievers and tools
- [Capturing Metadata and Attributes](/integration/typescript/tutorials/capturing-metadata) - Add custom metadata and attributes to your traces and spans
- [Capturing Evaluations & Guardrails](/integration/python/tutorials/capturing-evaluations-guardrails) - Log evaluations and implement guardrails in your Vercel AI SDK applications
