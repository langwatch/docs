---
title: TypeScript Integration Guide
sidebarTitle: TypeScript/JS
description: LangWatch TypeScript SDK integration guide
---

import LLMsTxtProtip from "/snippets/llms-txt-protip.mdx";

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

Integrate LangWatch into your TypeScript/JavaScript application to start observing your LLM interactions. This guide covers the setup and basic usage of the LangWatch TypeScript SDK.

<LLMsTxtProtip />

## Get your LangWatch API Key

First, you need a LangWatch API key. Sign up at [app.langwatch.ai](https://app.langwatch.ai) and find your API key in your project settings. The SDK will automatically use the `LANGWATCH_API_KEY` environment variable if it is set.

## Start Instrumenting

First, ensure you have the SDK installed:

```bash
npm install langwatch @opentelemetry/sdk-node @opentelemetry/context-async-hooks @opentelemetry/core
```

Initialize LangWatch early in your Node.js application, typically where you configure services:

```typescript
import { setup } from "langwatch/node";

await setup(); // This will setup the SDK, and setup the observability SDK.
```

<Note>
  If you have an existing OpenTelemetry setup in your application, please see the [Already using OpenTelemetry?](#already-using-opentelemetry) section below.
</Note>

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.

## Capturing Messages

<Tip>
  For most use cases, we recommend using `withActiveSpan` as it automatically handles error recording, status setting, and span ending. Use `startActiveSpan` when you need more control over span lifecycle, and `startSpan` only when you need complete manual control.
</Tip>

Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces). A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline. Spans can be nested to capture the pipeline structure.

### Creating a Trace

To capture an end-to-end operation, like processing a user message, you can use the `withActiveSpan` method. This is a helper method provided by the LangWatch SDK (not part of standard OpenTelemetry) that automatically creates a span for the entire operation and handles error recording, status setting, and span ending.

#### Tracer Best Practices

The tracer should be created once and reused throughout your application. Here are some recommended patterns:

<CodeGroup>
```typescript Module-level (Recommended)
// tracer.ts
import { getLangWatchTracer } from "langwatch";

export const tracer = getLangWatchTracer("my-service");
```

```typescript Class-based
class MyService {
  private tracer = getLangWatchTracer("my-service");
  
  async handleRequest() {
    return await this.tracer.withActiveSpan("handle-request", async (span) => {
      // ... your logic
    });
  }
}
```

```typescript Function-level (Not recommended)
async function handleRequest() {
  const tracer = getLangWatchTracer("my-service"); // Creates new tracer each time
  return await tracer.withActiveSpan("handle-request", async (span) => {
    // ... your logic
  });
}
```
</CodeGroup>

<Tip>
  Create your tracer at the module level and export it for reuse across your application. This ensures consistent service naming and avoids unnecessary tracer creation overhead.
</Tip>

#### Basic Usage

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

async function handleMessage() {
  return await tracer.withActiveSpan("handle-message", async (span) => {
    // This whole function execution is now a single trace
    span.setType("llm");
    
    // ... rest of your message handling logic ...
    return "response";
  });
}
```

You can also add initial attributes and metadata:

```typescript
await tracer.withActiveSpan(
  "handle-message", 
  { 
    attributes: { 
      "user.id": "user-123",
      "thread.id": "thread-456" 
    } 
  }, 
  async (span) => {
    // ... your logic ...
  }
);
```

### Capturing a Span

To instrument specific parts of your pipeline within a trace (like an LLM operation, RAG retrieval, or external API call), use the `startActiveSpan` method.

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

async function ragRetrieval(query: string) {
  return await tracer.withActiveSpan("rag-retrieval", { 
    attributes: { type: "rag" } 
  }, async (span) => {
    // ... logic to retrieve documents ...
    const searchResults = [
      { id: "doc-1", content: "..." },
      { id: "doc-2", content: "..." }
    ];

    // Add specific context data to the span
    span.setRAGContexts(
      searchResults.map(doc => ({
        document_id: doc.id,
        chunk_id: doc.id,
        content: doc.content
      }))
    );

    return searchResults;
  });
}

async function handleMessage() {
  return await tracer.withActiveSpan("handle-message", async (span) => {
    // ... other logic ...
    const retrievedDocs = await ragRetrieval("user query"); // This creates a nested span
    // ... rest of logic ...
  });
}
```

### Manual Span Management

For more control, you can manually manage spans. There are three approaches:

#### Using startActiveSpan (Recommended for manual control)

`startActiveSpan` is the standard OpenTelemetry method that creates a span, sets it in context (so child spans are nested), but requires manual error handling and ending:

```typescript
tracer.startActiveSpan("my-operation", (span) => {
  try {
    span.setType("llm");
    span.setInput("Hello, world!");
    
    // ... your logic ...
    
    span.setOutput("Hello! How can I help you?");
    span.setStatus({ code: SpanStatusCode.OK });
  } catch (error) {
    span.setStatus({ 
      code: SpanStatusCode.ERROR, 
      message: error.message 
    });
    span.recordException(error);
    throw error;
  } finally {
    span.end();
  }
});
```

#### Using startSpan (Complete manual control)

`startSpan` is the standard OpenTelemetry method that creates a span but does NOT set it in context, so child spans won't be nested. Use this only when you need complete control:

```typescript
const span = tracer.startSpan("my-operation");
try {
  span.setType("llm");
  span.setInput("Hello, world!");
  
  // ... your logic ...
  
  span.setOutput("Hello! How can I help you?");
  span.setStatus({ code: SpanStatusCode.OK });
} catch (error) {
  span.setStatus({ 
    code: SpanStatusCode.ERROR, 
    message: error.message 
  });
  span.recordException(error);
  throw error;
} finally {
  span.end();
}
```

### Recording LLM Interactions

For LLM operations, you can use the specialized GenAI event methods to capture the conversation flow:

<CodeGroup>
```typescript Complete Example
await tracer.withActiveSpan("llm-call", async (span) => {
  span.setType("llm");
  span.setRequestModel("gpt-4");
  
  // Record system message
  span.addGenAISystemMessageEvent({
    content: "You are a helpful assistant.",
    role: "system"
  });
  
  // Record user message
  span.addGenAIUserMessageEvent({
    content: "What is the weather today?",
    role: "user"
  });
  
  // Record assistant response
  span.addGenAIAssistantMessageEvent({
    content: "I don't have access to real-time weather data.",
    role: "assistant"
  });
  
  // Record the model's choice
  span.addGenAIChoiceEvent({
    finish_reason: "stop",
    index: 0,
    message: {
      content: "I don't have access to real-time weather data.",
      role: "assistant"
    }
  });
});
```

```typescript Individual Events
// Record system message
span.addGenAISystemMessageEvent({
  content: "You are a helpful assistant.",
  role: "system"
});

// Record user message
span.addGenAIUserMessageEvent({
  content: "What is the weather today?",
  role: "user"
});

// Record assistant response
span.addGenAIAssistantMessageEvent({
  content: "I don't have access to real-time weather data.",
  role: "assistant"
});

// Record tool calls (if applicable)
span.addGenAIAssistantMessageEvent({
  content: "Let me check the weather for you.",
  role: "assistant",
  tool_calls: [{
    function: {
      name: "get_weather",
      arguments: '{"location": "New York"}'
    },
    id: "call_123",
    type: "function"
  }]
});

// Record tool response
span.addGenAIToolMessageEvent({
  content: '{"temperature": 72, "condition": "sunny"}',
  id: "call_123",
  role: "tool"
});
```
</CodeGroup>

### Setting Input and Output

You can record structured input and output data:

<CodeGroup>
```typescript JSON Input/Output
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record JSON input
  span.setInput({ 
    prompt: "Hello", 
    temperature: 0.7 
  });
  
  // Record JSON output
  span.setOutput({ 
    response: "I'm doing well!", 
    confidence: 0.95 
  });
});
```

```typescript String Input/Output
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record string input
  span.setInputString("Hello, how are you?");
  
  // Record string output
  span.setOutputString("I'm doing well!");
});
```

```typescript Mixed Types
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record JSON input
  span.setInput({ 
    prompt: "Hello", 
    temperature: 0.7 
  });
  
  // Record string output
  span.setOutputString("I'm doing well!");
});
```
</CodeGroup>

### Adding Metrics

Track token usage and costs:

<CodeGroup>
```typescript Basic Metrics
await tracer.withActiveSpan("llm-call", async (span) => {
  span.setMetrics({
    promptTokens: 150,
    completionTokens: 50,
    cost: 0.002
  });
});
```

```typescript Partial Metrics
await tracer.withActiveSpan("llm-call", async (span) => {
  // Only track tokens
  span.setMetrics({
    promptTokens: 150,
    completionTokens: 50
  });
});
```

```typescript Cost Only
await tracer.withActiveSpan("llm-call", async (span) => {
  // Only track cost
  span.setMetrics({
    cost: 0.002
  });
});
```
</CodeGroup>

## Integrations

LangWatch offers seamless integrations with a variety of popular Python libraries and frameworks. These integrations provide automatic instrumentation, capturing relevant data from your LLM applications with minimal setup.

Below is a list of currently supported integrations. Click on each to learn more about specific setup instructions and available features:

- [AWS Bedrock](/integration/typescript/integrations/aws-bedrock)
- [Azure AI](/integration/typescript/integrations/azure-ai)
- [Langchain](/integration/typescript/integrations/langchain)
- [LangGraph](/integration/typescript/integrations/langgraph)
- [Mastra](/integration/typescript/integrations/mastra)
- [n8n](/integration/typescript/integrations/n8n)
- [OpenAI Agents](/integration/typescript/integrations/open-ai-agents)
- [OpenAI Azure](/integration/typescript/integrations/open-ai-azure)
- [OpenAI](/integration/typescript/integrations/open-ai)
- [Vercel AI SDK](/integration/typescript/integrations/vercel-ai-sdk)
- [Other Frameworks](/integration/typescript/integrations/other)

## Full Setup
