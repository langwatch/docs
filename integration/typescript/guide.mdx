---
title: TypeScript Integration Guide
sidebarTitle: Guide
---

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

import LLMsTxtProtip from "/snippets/llms-txt-protip.mdx";

<LLMsTxtProtip />

import Prerequisites from "/snippets/prerequests-ts.mdx";

<Prerequisites />


## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Integration

<Tabs>
<Tab title="Vercel AI SDK">
The Vercel AI SDK supports tracing via Next.js OpenTelemetry integration. By using the `LangWatchExporter`, you can automatically collect those traces to LangWatch.

First, you need to install the necessary dependencies:

```bash
npm install @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```

Then, set up the OpenTelemetry for your application, follow one of the tabs below depending whether you are using AI SDK with Next.js or on Node.js:

<Tabs>
<Tab title="Next.js">
You need to enable the `instrumentationHook` in your `next.config.js` file if you haven't already:

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    instrumentationHook: true,
  },
};

module.exports = nextConfig;
```

Next, you need to create a file named `instrumentation.ts` (or `.js`) in the __root directory__ of the project (or inside `src` folder if using one), with `LangWatchExporter` as the traceExporter:

```typescript
import { registerOTel } from '@vercel/otel'
import { LangWatchExporter } from 'langwatch'

export function register() {
  registerOTel({
    serviceName: 'next-app',
    traceExporter: new LangWatchExporter({
      apiKey: process.env.LANGWATCH_API_KEY
    })
  })
}
```

(Read more about Next.js OpenTelemetry configuration [on the official guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry#manual-opentelemetry-configuration))

Finally, enable `experimental_telemetry` tracking on the AI SDK calls you want to trace:

```typescript
const result = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: 'Explain why a chicken would make a terrible astronaut, be creative and humorous about it.',
  experimental_telemetry: {
    isEnabled: true,
    // optional metadata
    metadata: {
      userId: "myuser-123",
      threadId: "mythread-123",
    },
  },
});
```
</Tab>
<Tab title="Node.js">
For Node.js, start by following the official OpenTelemetry guide:

- [OpenTelemetry Node.js Getting Started](https://opentelemetry.io/docs/languages/js/getting-started/nodejs/)

Once you have set up OpenTelemetry, you can use the `LangWatchExporter` to automatically send your traces to LangWatch:

```typescript
import { LangWatchExporter } from 'langwatch'

const sdk = new NodeSDK({
  traceExporter: new LangWatchExporter({
    apiKey: process.env.LANGWATCH_API_KEY
  }),
  // ...
});
```
</Tab>
</Tabs>

That's it! Your messages will now be visible on LangWatch:

![Vercel AI SDK](/images/integration/vercel-ai-sdk.png)

## Example Project

You can find a full example project with a more complex pipeline and Vercel AI SDK and LangWatch integration [on our GitHub](https://github.com/langwatch/langwatch/blob/main/typescript-sdk/example/lib/chat/vercel-ai.tsx).

## Manual Integration

The docs from here below are for manual integration, in case you are not using the Vercel AI SDK OpenTelemetry integration,
you can manually start a trace to capture your messages:

```typescript
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});
```

Then, you can start an LLM span inside the trace with the input about to be sent to the LLM.

```typescript
import { convertFromVercelAIMessages } from 'langwatch'

const span = trace.startLLMSpan({
  name: "llm",
  model: model,
  input: {
    type: "chat_messages",
    value: convertFromVercelAIMessages(messages)
  },
});
```

This will capture the LLM input and register the time the call started. Once the LLM call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation, e.g.:

```typescript
span.end({
  output: {
    type: "chat_messages",
    value: convertFromVercelAIMessages(output), // assuming output is Message[]
  },
  metrics: {
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  },
});
```

</Tab>
<Tab title="OpenAI">
Start by initializing LangWatch client and creating a new trace to capture your messages:

```typescript
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});
```

Then to capture your LLM calls, you can start an LLM span inside the trace with the input about to be sent to the LLM.

First, define the model and the messages you are going to use for your LLM call separately, so you can capture them:

```typescript
import { OpenAI } from "openai";

// Model to be used and messages that will be sent to the LLM
const model = "gpt-4o"
const messages : OpenAI.Chat.ChatCompletionMessageParam[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
]
```

Then, start the LLM span from the trace, giving it the model and input messages:

```typescript
const span = trace.startLLMSpan({
  name: "llm",
  model: model,
  input: {
    type: "chat_messages",
    value: messages
  },
});
```

This will capture the LLM input and register the time the call started. Now, continue with the LLM call normally, using the same parameters:

```typescript
const openai = new OpenAI();
const chatCompletion = await openai.chat.completions.create({
  messages: messages,
  model: model,
});
```

Finally, after the OpenAI call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation:

```typescript
span.end({
  output: {
    type: "chat_messages",
    value: [chatCompletion.choices[0]!.message],
  },
  metrics: {
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  },
});
```
</Tab>
<Tab title="Azure">
Start by initializing LangWatch client and creating a new trace to capture your messages:

```typescript
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});
```

Then to capture your LLM calls, you can start an LLM span inside the trace with the input about to be sent to the LLM.

First, define the model and the messages you are going to use for your LLM call separately, so you can capture them:

```typescript
import { AzureOpenAI } from "openai";

// Model to be used and messages that will be sent to the LLM
const model = "gpt-4-turbo-2024-04-09"
const messages : OpenAI.Chat.ChatCompletionMessageParam[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
]
```

Then, start the LLM span from the trace, giving it the model and input messages:

```typescript
const span = trace.startLLMSpan({
  name: "llm",
  model: model,
  input: {
    type: "chat_messages",
    value: messages
  },
});
```

This will capture the LLM input and register the time the call started. Now, continue with the LLM call normally, using the same parameters:

```typescript
const openai = new AzureOpenAI({
  apiKey: process.env.AZURE_OPENAI_API_KEY,
  apiVersion: "2024-02-01",
  endpoint: process.env.AZURE_OPENAI_ENDPOINT,
});
const chatCompletion = await openai.chat.completions.create({
  messages: messages,
  model: model,
});
```

Finally, after the OpenAI call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation:

```typescript
span.end({
  output: {
    type: "chat_messages",
    value: [chatCompletion.choices[0]!.message],
  },
  metrics: {
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  },
});
```
</Tab>
<Tab title="LangChain.js">
Start by initializing LangWatch client and creating a new trace to capture your chain:

```typescript
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});
```

Then, to capture your LLM calls and all other chain steps, LangWatch provides a callback hook for LangChain.js that automatically tracks everything for you.

First, define your chain as you would normally do:

```typescript
import { StringOutputParser } from '@langchain/core/output_parsers'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { ChatOpenAI } from '@langchain/openai'

const prompt = ChatPromptTemplate.fromMessages([
  ['system', 'Translate the following from English into Italian'],
  ['human', '{input}']
])
const model = new ChatOpenAI({ model: 'gpt-3.5-turbo' })
const outputParser = new StringOutputParser()

const chain = prompt.pipe(model).pipe(outputParser)
```

Now, when calling your chain either with `invoke` or `stream`, pass in `trace.getLangChainCallback()` as one of the callbacks:

```typescript
const stream = await chain.stream(
  { input: message },
  { callbacks: [trace.getLangChainCallback()] }
)
```

That's it! The full trace with all spans for each chain step will be sent automatically to LangWatch in the background on periodic intervals. After capturing your first LLM Span, go to [LangWatch Dashboard](https://app.langwatch.ai), your message should be there!
</Tab>
</Tabs>

<Note>
On short-live environments like Lambdas or Serverless Functions, be sure to call <br /> `await trace.sendSpans();` to wait for all pending requests to be sent before the runtime is destroyed.
</Note>

## Capture a RAG Span

Appart from LLM spans, another very used type of span is the RAG span. This is used to capture the retrieved contexts from a RAG that will be used by the LLM, and enables a whole new set of RAG-based features evaluations for RAG quality on LangWatch.

import TypeScriptRAG from "/snippets/typescript-rag.mdx";

<TypeScriptRAG />

## Capture an arbritary Span

You can also use generic spans to capture any type of operation, its inputs and outputs, for example for a function call:

```typescript
// before the function starts
const span = trace.startSpan({
  name: "weather_function",
  input: {
    type: "json",
    value: {
      city: "Tokyo",
    },
  },
});

// ...after the function ends
span.end({
  output: {
    type: "json",
    value: {
      weather: "sunny",
    },
  },
});
```

You can also nest spans one inside the other, capturing your pipeline structure, for example:

```typescript
const span = trace.startSpan({
  name: "pipeline",
});

const nestedSpan = span.startSpan({
  name: "nested_pipeline",
});

nestedSpan.end()

span.end()
```

Both LLM and RAG spans can also be nested like any arbritary span.

## Capturing Exceptions

To capture also when your code throws an exception, you can simply wrap your code around a try/catch, and update or end the span with the exception:

```typescript
try {
  throw new Error("unexpected error");
} catch (error) {
  span.end({
    error: error,
  });
}
```

## Capturing custom evaluation results

[LangWatch Evaluators](/evaluations/overview) can run automatically on your traces, but if you have an in-house custom evaluator, you can also capture the evaluation
results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

import TypeScriptCustomEvaluation from "/snippets/typescript-custom-evaluation.mdx"

<TypeScriptCustomEvaluation />