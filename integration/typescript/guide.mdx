---
title: TypeScript Integration Guide
sidebarTitle: TypeScript/JS
description: LangWatch TypeScript SDK integration guide
---

import LLMsTxtProtip from "/snippets/llms-txt-protip.mdx";

<div className="not-prose" style={{display: "flex", gap: "8px", padding: "0"}}>
  <div>
  <a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
    <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat&logo=Github" noZoom alt="LangWatch TypeScript Repo" />
  </a>
  </div>

  <div>
  <a href="https://www.npmjs.com/package/langwatch" target="_blank">
    <img src="https://img.shields.io/npm/v/langwatch?color=007EC6" noZoom alt="LangWatch TypeScript SDK version" />
  </a>
  </div>
</div>

Integrate LangWatch into your TypeScript/JavaScript application to start observing your LLM interactions. This guide covers the setup and basic usage of the LangWatch TypeScript SDK.

<LLMsTxtProtip />

## Get your LangWatch API Key

First, you need a LangWatch API key. Sign up at [app.langwatch.ai](https://app.langwatch.ai) and find your API key in your project settings. The SDK will automatically use the `LANGWATCH_API_KEY` environment variable if it is set.

## Start Instrumenting

First, ensure you have the SDK installed:

```bash
npm install langwatch @opentelemetry/sdk-node @opentelemetry/context-async-hooks
```

Initialize LangWatch early in your Node.js application, typically where you configure services:

```typescript
import { setup } from "langwatch/node";

await setup(); // This will setup the SDK, and setup the observability SDK.
```

<Note>
  If you have an existing OpenTelemetry setup in your application, please see the [Already using OpenTelemetry?](#already-using-opentelemetry) section below.
</Note>

## Basic Concepts

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.

## Integrations

LangWatch offers seamless integrations with a variety of popular Python libraries and frameworks. These integrations provide automatic instrumentation, capturing relevant data from your LLM applications with minimal setup.

Below is a list of currently supported integrations. Click on each to learn more about specific setup instructions and available features:

- [AWS Bedrock](/integration/typescript/integrations/aws-bedrock)
- [Azure AI](/integration/typescript/integrations/azure-ai)
- [Langchain](/integration/typescript/integrations/langchain)
- [LangGraph](/integration/typescript/integrations/langgraph)
- [Mastra](/integration/typescript/integrations/mastra)
- [n8n](/integration/typescript/integrations/n8n)
- [OpenAI Agents](/integration/typescript/integrations/open-ai-agents)
- [OpenAI Azure](/integration/typescript/integrations/open-ai-azure)
- [OpenAI](/integration/typescript/integrations/open-ai)
- [Vercel AI SDK](/integration/typescript/integrations/vercel-ai-sdk)
- [Other Frameworks](/integration/typescript/integrations/other)

## Capturing Messages

<Tip>
  For most use cases, we recommend using `withActiveSpan` as it automatically handles error recording, status setting, and span ending. Use `startActiveSpan` when you need more control over span lifecycle, and `startSpan` only when you need complete manual control.
</Tip>

Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces). A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline. Spans can be nested to capture the pipeline structure.

### Creating a Trace

To capture an end-to-end operation, like processing a user message, you can use the `withActiveSpan` method. This is a helper method provided by the LangWatch SDK (not part of standard OpenTelemetry) that automatically creates a span for the entire operation and handles error recording, status setting, and span ending.

#### Tracer Best Practices

The tracer should be created once and reused throughout your application. Here are some recommended patterns:

<CodeGroup>
```typescript Module-level (Recommended)
// tracer.ts
import { getLangWatchTracer } from "langwatch";

export const tracer = getLangWatchTracer("my-service");
```

```typescript Class-based
class MyService {
  private tracer = getLangWatchTracer("my-service");
  
  async handleRequest() {
    return await this.tracer.withActiveSpan("handle-request", async (span) => {
      // ... your logic
    });
  }
}
```

```typescript Function-level (Not recommended)
async function handleRequest() {
  const tracer = getLangWatchTracer("my-service"); // Creates new tracer each time
  return await tracer.withActiveSpan("handle-request", async (span) => {
    // ... your logic
  });
}
```
</CodeGroup>

<Tip>
  Create your tracer at the module level and export it for reuse across your application. This ensures consistent service naming and avoids unnecessary tracer creation overhead.
</Tip>

#### Basic Usage

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

async function handleMessage() {
  return await tracer.withActiveSpan("handle-message", async (span) => {
    // This whole function execution is now a single trace
    span.setType("llm");
    
    // ... rest of your message handling logic ...
    return "response";
  });
}
```

You can also add initial attributes and metadata:

```typescript
await tracer.withActiveSpan(
  "handle-message", 
  { 
    attributes: { 
      "user.id": "user-123",
      "thread.id": "thread-456" 
    } 
  }, 
  async (span) => {
    // ... your logic ...
  }
);
```

### Capturing a Span

To instrument specific parts of your pipeline within a trace (like an LLM operation, RAG retrieval, or external API call), use the `startActiveSpan` method.

```typescript
import { getLangWatchTracer } from "langwatch";

const tracer = getLangWatchTracer("my-service");

async function ragRetrieval(query: string) {
  return await tracer.withActiveSpan("rag-retrieval", async (span) => {
    span.setType("rag");

    // ... logic to retrieve documents ...
    const searchResults = [
      { id: "doc-1", content: "..." },
      { id: "doc-2", content: "..." }
    ];

    // Add specific context data to the span
    span.setRAGContexts(
      searchResults.map(doc => ({
        document_id: doc.id,
        chunk_id: doc.id,
        content: doc.content
      })),
    );

    return searchResults;
  });
}

async function handleMessage() {
  return await tracer.withActiveSpan("handle-message", async (span) => {
    // ... other logic ...
    const retrievedDocs = await ragRetrieval("user query"); // This creates a nested span
    // ... rest of logic ...
  });
}
```

### Manual Span Management

For more control, you can manually manage spans. There are three approaches:

#### Using startActiveSpan (Recommended for manual control)

`startActiveSpan` is the standard OpenTelemetry method that creates a span, sets it in context (so child spans are nested), but requires manual error handling and ending:

```typescript
tracer.startActiveSpan("my-operation", (span) => {
  try {
    span.setType("llm");
    span.setInput("Hello, world!");
    
    // ... your logic ...
    
    span.setOutput("Hello! How can I help you?");
    span.setStatus({ code: SpanStatusCode.OK });
  } catch (error) {
    span.setStatus({ 
      code: SpanStatusCode.ERROR, 
      message: error.message 
    });
    span.recordException(error);
    throw error;
  } finally {
    span.end();
  }
});
```

#### Using startSpan (Complete manual control)

`startSpan` is the standard OpenTelemetry method that creates a span but does NOT set it in context, so child spans won't be nested. Use this only when you need complete control:

```typescript
const span = tracer.startSpan("my-operation");
try {
  span.setType("llm");
  span.setInput("Hello, world!");
  
  // ... your logic ...
  
  span.setOutput("Hello! How can I help you?");
  span.setStatus({ code: SpanStatusCode.OK });
} catch (error) {
  span.setStatus({ 
    code: SpanStatusCode.ERROR, 
    message: error.message 
  });
  span.recordException(error);
  throw error;
} finally {
  span.end();
}
```

### Setting Input and Output

You can record structured input and output data:

<CodeGroup>
```typescript JSON Input/Output
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record JSON input
  span.setInput({ 
    prompt: "Hello", 
    temperature: 0.7 
  });
  
  // Record JSON output
  span.setOutput({ 
    response: "I'm doing well!", 
    confidence: 0.95 
  });
});
```

```typescript String Input/Output
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record string input
  span.setInputString("Hello, how are you?");
  
  // Record string output
  span.setOutputString("I'm doing well!");
});
```

```typescript Mixed Types
await tracer.withActiveSpan("my-operation", async (span) => {
  // Record JSON input
  span.setInput({ 
    prompt: "Hello", 
    temperature: 0.7 
  });
  
  // Record string output
  span.setOutputString("I'm doing well!");
});
```
</CodeGroup>

### Adding Metrics

Track token usage and costs:

<CodeGroup>
```typescript Basic Metrics
await tracer.withActiveSpan("llm-call", async (span) => {
  span.setMetrics({
    promptTokens: 150,
    completionTokens: 50,
    cost: 0.002
  });
});
```

```typescript Partial Metrics
await tracer.withActiveSpan("llm-call", async (span) => {
  // Only track tokens
  span.setMetrics({
    promptTokens: 150,
    completionTokens: 50
  });
});
```

```typescript Cost Only
await tracer.withActiveSpan("llm-call", async (span) => {
  // Only track cost
  span.setMetrics({
    cost: 0.002
  });
});
```
</CodeGroup>

## Full Setup

Here's a complete example showing how to integrate LangWatch with a typical LLM application using the Vercel AI SDK, including RAG retrieval, generation, and data storage.

### Complete Example

<CodeGroup>
```typescript app.ts
import { setup } from "langwatch/node";
import { getLangWatchTracer } from "langwatch";
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";
import { z } from "zod";

// Initialize LangWatch
await setup({
  apiKey: process.env.LANGWATCH_API_KEY, // Optional: defaults to environment variable `LANGWATCH_API_KEY`
  endpoint: process.env.LANGWATCH_ENDPOINT, // Optional: defaults to `LANGWATCH_ENDPOINT` or hosted cloud endpoint
  baseAttributes: {
    "service.name": "chatbot-service",
    "service.version": "1.0.0",
    "deployment.environment": process.env.NODE_ENV,
  },
  spanExcludeRules: [{
    fieldName: ["span_name"],
    matchValue: "health-check",
    matchOperation: "exact_match"
  }]
});

// Create a reusable tracer
const tracer = getLangWatchTracer("chatbot-service");

// Mock database for storing conversation data
const conversationStore = new Map<string, any[]>();

// Mock RAG retrieval function
async function retrieveRelevantDocuments(query: string) {
  return await tracer.withActiveSpan("rag-retrieval", async (span) => {
    span.setType("rag");

    // Simulate document retrieval
    const documents = [
      { id: "doc-1", content: "LangWatch is an observability platform for LLM applications." },
      { id: "doc-2", content: "It provides tracing, evaluation, and monitoring capabilities." },
      { id: "doc-3", content: "The platform supports multiple programming languages and frameworks." }
    ];

    // Filter documents based on query (simplified)
    const relevantDocs = documents.filter(doc => 
      doc.content.toLowerCase().includes(query.toLowerCase())
    );

    // Record RAG contexts
    span.setRAGContexts(
      relevantDocs.map(doc => ({
        document_id: doc.id,
        chunk_id: doc.id,
        content: doc.content
      }))
    );

    span.setInputString(query);
    
    return relevantDocs;
  });
}

// Main message handling function
async function handleUserMessage(
  userId: string, 
  threadId: string, 
  message: string
) {
  return await tracer.withActiveSpan("handle-user-message", {
    attributes: {
      "user.id": userId,
      "thread.id": threadId,
      "message.type": "user"
    }
  }, async (span) => {
    // 1. Retrieve relevant documents
    const documents = await retrieveRelevantDocuments(message);
    
    // 2. Generate response using Vercel AI SDK
    const response = await generateText({
      model: openai("gpt-4"),
      prompt: `Based on the following context, answer the user's question:

Context:
${documents.map(doc => doc.content).join('\n')}

User Question: ${message}

Answer:`,
      maxTokens: 500,
      temperature: 0.7,
      // This is required for LangWatch to capture the LLM input and output, and other data.
      experimental_telemetry: { isEnabled: true },
    });

    // 3. Store conversation data
    const conversationData = {
      timestamp: new Date().toISOString(),
      userMessage: message,
      assistantResponse: response.text,
      retrievedDocuments: documents,
      threadId,
      userId
    };

    // Store in mock database
    const userConversations = conversationStore.get(userId) || [];
    userConversations.push(conversationData);
    conversationStore.set(userId, userConversations);

    // 4. Record the complete interaction
    span.setType("llm");

    // 5. Record metrics
    span.setMetrics({
      promptTokens: response.usage?.promptTokens || 0,
      completionTokens: response.usage?.completionTokens || 0,
      cost: response.usage?.cost || 0
    });

    // 6. Record input, the pure LLM input is automatically captured via the Vercel AI SDK.
    span.setInput({
      userMessage: message,
      retrievedDocuments: documents,
      threadId,
      userId
    });

    // 6. Record output, the pure LLM output is automatically captured via the Vercel AI SDK.
    span.setOutput({
      assistantResponse: response.text,
      conversationId: conversationData.timestamp
    });

    return {
      response: response.text,
      conversationId: conversationData.timestamp,
      retrievedDocuments: documents
    };
  });
}

// API endpoint handler (Express.js example)
app.post('/api/chat', async (req, res) => {
  const { userId, threadId, message } = req.body;

  try {
    const result = await handleUserMessage(userId, threadId, message);
    res.json(result);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Get conversation history
app.get('/api/conversations/:userId', (req, res) => {
  const { userId } = req.params;
  const conversations = conversationStore.get(userId) || [];
  res.json(conversations);
});

// Health check endpoint
app.get('/api/health', (req, res) => {
  tracer.withActiveSpan("health-check", async (span) => {
    span.setStatus({ code: SpanStatusCode.OK });
    res.json({ status: 'ok' });
  });
});
```

```bash .env
LANGWATCH_API_KEY=your_api_key_here
NODE_ENV=production
```

```json package.json
{
  "dependencies": {
    "@ai-sdk/openai": "^1.3.23",
    "@opentelemetry/context-async-hooks": "^2.0.1",
    "@opentelemetry/sdk-node": "^0.203.0",
    "ai": "^4.3.19",
    "express": "^4.18.0",
    "langwatch": "^0.3.0",
    "zod": "^4.0.10"
  }
}
```

```typescript instrumentation.ts
import { registerOTel } from '@vercel/otel';

export function register() {
  registerOTel({
    serviceName: 'next-app',
    // Note: LangWatch SDK automatically sets up the trace exporter,
    // so no manual configuration is needed here
  });
}
```
</CodeGroup>

### Key Features Demonstrated

- **Tracing**: Complete request flow from user message to response
- **RAG Context**: Document retrieval with proper context recording
- **LLM Events**: Structured conversation flow with system, user, and assistant messages
- **Metrics**: Token usage and cost tracking
- **Error Handling**: Proper error recording and status setting
- **Data Storage**: Conversation persistence with trace correlation
- **Thread Management**: Conversation grouping with thread IDs
- **User Analytics**: User ID tracking for analytics
- **Custom Attributes**: Service metadata and environment information

This example shows how LangWatch can capture every aspect of your LLM application, from the initial user request through document retrieval, LLM generation, and data storage, providing complete observability into your AI pipeline.
