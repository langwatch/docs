---
title: Introduction to Agent Testing
sidebarTitle: Introduction
keywords: langwatch, agent simulations, agent testing, agent development, agent development, agent testing
---

# What are Agent Simulations?

Agent simulations are a powerful approach to testing AI agents that goes beyond traditional evaluation methods. Unlike static input-output testing, simulations test your agent's behavior in realistic, multi-turn conversations that mimic how real users would interact with your system.

<img src="/images/simulations-hero.gif" alt="Agent Simulations" />

## The Three Levels of Agent Quality

For comprehensive agent testing, you need all three levels:

- **Level 1: Unit tests**\
  Traditional unit and integration software tests to guarantee that e.g. the agent tools are working correctly from a software point of view

- **Level 2: Evals, Finetuning and Prompt Optimization**\
  Measuring the performance of individual non-deterministic components of the agent, for example maximizing RAG accuracy with evals, or approximating human preference with GRPO

- **Level 3: Agent Simulations**\
  End-to-end testing of the agent in different scenarios and edge cases, guaranteeing the whole agent achieves more than the sum of its parts, simulating a wide range of situations

Simulations complement evaluations by testing the **agent as a whole system** rather than isolated parts.

## Why Traditional Evaluation Isn't Enough for Agents

Most evaluations are based on dataset, with a static set of cases, those are hard to get specially when you are just getting started, they often require a great amount of examples to be valuable, and an expected answer to be provided, but more than anything, they are static, like input to output, or query to expected_contexts.

Agents, however, aren't simple input-output functions. They are processes. An agent behaves like a program, executing a sequence of operations, using tools, and maintaining state.

### Evaluation dataset (single input-output pairs):

| query                            | expected_answer                                                                                              |
| -------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| What is your refund policy?      | We offer a 30-day money-back guarantee on all purchases.                                                     |
| How do I cancel my subscription? | You can cancel your subscription by logging into your account and clicking the "Cancel Subscription" button. |

❌ Doesn't consider the conversational flow\
❌ Can't specify how middle steps should be evaluated\
❌ Hard to interpret and debug\
❌ Ignores user experience aspects\
❌ Hard to come up with a good dataset

### Agent simulation (full multi-turn descriptions):

```python
script=[
  scenario.user("hey I have a problem with my order"),
  scenario.agent(),
  expect_ticket_created()
  expect_ticket_label("ecommerce")
  scenario.user("i want a refund!"),
  scenario.agent()
  expect_tool_call("search_policy")
  scenario.user("this is ridiculous! let me talk to a human being")
  scenario.agent()
  expect_tool_call("escalate_to_human")
]
```

✅ Describes the entire conversation\
✅ Explicitly evaluates in-between steps\
✅ Easy to interpret and debug\
✅ Easy to replicate and reproduce an issue found in production\
✅ Can run in autopilot for simulating a variety of inputs

**This doesn't mean you should stop doing evaluations**, in fact, having evaluations and simulations together is what composes your full agent test suite:

- Use evaluations for testing the smaller parts that compose the agent, where a more "machine learning" approach is required, for optimizing a specific LLM call or retrieval for example.

- Use simulation-based testing for proving the agent's behavior is correct end-to-end, replicate specific edge cases, and guide your agent's development without regressions.

## Why Use LangWatch Scenario?

[Scenario](https://langwatch.ai/scenario/) is the most advanced agent testing framework available. It provides:

- **Powerful simulations** - Test real agent behavior by simulating users in different scenarios and edge cases
- **Flexible evaluations** - Judge agent behavior at any point in conversations, combine with evals, test error recovery, and complex workflows
- **Framework agnostic** - Works with any AI agent framework
- **Simple integration** - Just implement one `call()` method
- **Multi-language support** - Python, TypeScript, and Go

## Two Ways to Create Simulations

LangWatch offers two approaches to agent testing:

### On-Platform Scenarios (No Code)

Create and run simulations directly in the LangWatch UI:
- Define situations and evaluation criteria visually
- Run against HTTP agents or managed prompts
- Ideal for quick iteration and non-technical team members

[Get started with On-Platform Scenarios →](/scenarios/overview)

### Scenario Library (Code-Based)

Write simulations in code for maximum control:
- Full programmatic control over conversation flow
- Complex assertions and tool call verification
- CI/CD integration for automated testing
- **Trace-based evaluation** via OpenTelemetry integration

[Get started with the Scenario library →](/agent-simulations/getting-started)

Both approaches produce simulations that appear in the same visualizer, so you can mix and match based on your needs.

## Visualizing Simulations in LangWatch

The Simulations visualizer helps you analyze results from both On-Platform Scenarios and code-based tests:

- **Organize simulations** into sets and batches
- **Debug agent behavior** by stepping through conversations
- **Track performance** over time with run history
- **Collaborate** with your team on agent improvements

<img
  src="/images/simulations/simulation-set-overview.png"
  alt="Simulations Sets"
  width="100%"
/>

## Next Steps

### On-Platform Scenarios
- [Overview](/scenarios/overview) - Create scenarios in the UI
- [Creating Scenarios](/scenarios/creating-scenarios) - Write effective test cases
- [Running Scenarios](/scenarios/running-scenarios) - Execute and analyze results

### Scenario Library
- [Visualizer Overview](/agent-simulations/overview) - Learn about the simulation visualizer
- [Library Getting Started](/agent-simulations/getting-started) - Set up your first code-based simulation
- [Individual Run Analysis](/agent-simulations/individual-run) - Debug specific scenarios
- [Batch Runs](/agent-simulations/batch-runs) - Organize multiple tests
- [Scenario Documentation](https://langwatch.ai/scenario/) - Deep dive into the testing library
