---
title: Instrumenting Custom Evaluator
---

If you have a custom evaluator built in-house which run on your own code, either during the LLM pipeline or after, you can still capture the evaluation results
and connect it back to the trace to visualize it together with the other LangWatch evaluators.

import PythonCustomEvaluation from "/snippets/python-custom-evaluation.mdx"
import TypeScriptCustomEvaluation from "/snippets/typescript-custom-evaluation.mdx"

<Tabs>
<Tab title="Python">

You can capture the evaluation results of your custom evaluator on the current trace or span by using the `.add_evaluation` method:

<PythonCustomEvaluation />

</Tab>
<Tab title="TypeScript">

You can capture the evaluation results of your custom evaluator on the current trace or span by using the `.addEvaluation` method:

<TypeScriptCustomEvaluation />

</Tab>
<Tab title="REST API">

## REST API Specification

### Endpoint

`POST /api/collector`

### Headers

- `X-Auth-Token`: Your LangWatch API key.

### Request Body

```javascript
{
  "trace_id": "id of the message the evaluation was run on",
  "evaluations": [{
    "evaluation_id": "evaluation-id-123", // optional unique id for identifying the evaluation, if not provided, a random id will be generated
    "name": "custom evaluation", // required
    "passed": true, // optional
    "score": 0.5, // optional
    "label": "category_detected", // optional
    "details": "explanation of the evaluation results", // optional
    "error": { // optional to capture error details in case evaluation had an error
      "message": "error message",
      "stacktrace": [],
    },
    "timestamps": { // optional
      "created_at": "1723411698506", // unix timestamp in milliseconds
      "updated_at": "1723411698506" // unix timestamp in milliseconds
    }
  }]
}
```
</Tab>
</Tabs>