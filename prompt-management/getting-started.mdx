---
title: "Get Started"
description: "Create your first prompt and use it in your application"
---

Learn how to create your first prompt in LangWatch and use it in your application with dynamic variables. This enables your team to update AI interactions without code changes.

## Get API keys

1. Create a LangWatch [account](https://app.langwatch.ai) or set up [self-hosted LangWatch](https://github.com/langwatch/langwatch?tab=readme-ov-file#self-hosted-%EF%B8%8F)
2. Create new API credentials in your [project settings](https://app.langwatch.ai/settings)
3. Note your API key for use in the steps below

## Create a prompt

<Tabs>
  <Tab title="LangWatch UI">
    Use the LangWatch UI to create a new prompt or update an existing one.

    1. Navigate to your project dashboard
    2. Go to **Prompt Management** in the sidebar
    3. Click **"Create New Prompt"**
    4. Fill in the prompt details and save

  </Tab>

  <Tab title="REST API">
    Use the REST API to create a new prompt:

    ```bash create_prompt.sh
    # Create a new prompt (this creates the prompt with an initial version)
    curl -X POST "https://app.langwatch.ai/api/prompts" \
      -H "Content-Type: application/json" \
      -H "X-Auth-Token: your-api-key" \
      -d '{
        "name": "customer-support",
        "handle": "customer-support-bot",
        "scope": "project"
      }'
    ```

    **Note**: To create versions with custom prompt content, you'll need to use the API directly. See the [API Reference](/api-reference/prompts/overview) for version management endpoints.

  </Tab>
</Tabs>

## Use prompt

At runtime, you can fetch the latest version of your prompt from LangWatch using either the prompt handle or ID.

<Tabs>
  <Tab title="Python SDK">
    ```python use_prompt.py
    import langwatch
    from openai import OpenAI

    # Get the latest prompt (can use handle or ID)
    prompt = langwatch.prompt.get_prompt("customer-support-bot")  # by handle
    # or
    # prompt = langwatch.prompt.get_prompt("prompt_TrYXZLsiTJkn9N6PiZiae")  # by ID

    # Format messages with variables
    messages = prompt.format_messages(
        user_name="John Doe",
        user_email="john.doe@example.com",
        input="How do I reset my password?"
    )

    # Use with OpenAI client
    client = OpenAI()
    completion = client.chat.completions.create(
        model=prompt.model.split("openai/")[1],  # Remove "openai/" prefix
        messages=messages
    )

    print(completion.choices[0].message.content)
    ```

  </Tab>

  <Tab title="TypeScript SDK">
    ```typescript use_prompt.ts
    import { getPrompt, setupLangWatch } from "langwatch";
    import OpenAI from "openai";

    // Initialize LangWatch
    await setupLangWatch();

    // Get the latest prompt (can use handle or ID)
    const prompt = await getPrompt("customer-support-bot"); // by handle
    // or
    // const prompt = await getPrompt("prompt_TrYXZLsiTJkn9N6PiZiae"); // by ID

    // Compile prompt with variables
    const compiledPrompt = prompt.compile({
      user_name: "John Doe",
      user_email: "john.doe@example.com",
      input: "How do I reset my password?"
    });

    // Use with OpenAI client
    const openai = new OpenAI();
    const completion = await openai.chat.completions.create({
      model: prompt.model.replace("openai/", ""), // Remove "openai/" prefix
      messages: compiledPrompt.messages
    });

    console.log(completion.choices[0].message.content);
    ```

  </Tab>

  <Tab title="REST API">
    ```bash use_prompt.sh
    # Get by handle
    curl -X GET "https://app.langwatch.ai/api/prompts/customer-support-bot" \
      -H "X-Auth-Token: your-api-key"
    
    # Or get by ID
    curl -X GET "https://app.langwatch.ai/api/prompts/prompt_TrYXZLsiTJkn9N6PiZiae" \
      -H "X-Auth-Token: your-api-key"
    ```
  </Tab>
</Tabs>

## Link with LangWatch Tracing

You can link your prompt to LLM generation traces to track performance and see which prompt versions work best. For detailed information about linking prompts to traces, see the [Link to Traces](/prompt-management/features/advanced/link-to-traces) page.

<Tabs>
  <Tab title="Python SDK">
    ```python tracing.py
    import langwatch
    from openai import OpenAI

    # Initialize LangWatch
    langwatch.setup()

    # Create a trace function
    @langwatch.trace()
    def customer_support_generation():
        # Get prompt (automatically linked to trace when API key is present)
        prompt = langwatch.prompt.get_prompt("customer-support-bot")

        # Format messages with variables
        messages = prompt.format_messages(
            user_name="John Doe",
            user_email="john.doe@example.com",
            input="I need help with my account"
        )

        # Use with OpenAI client
        client = OpenAI()
        completion = client.chat.completions.create(
            model=prompt.model.split("openai/")[1],
            messages=messages
        )

        return completion.choices[0].message.content

    # Call the function
    result = customer_support_generation()
    ```

  </Tab>

  <Tab title="TypeScript SDK">
    ```typescript tracing.ts
    import { getPrompt, setupLangWatch, getLangWatchTracer } from "langwatch";
    import { openai } from "@ai-sdk/openai";
    import { generateText } from "ai";

    // Initialize LangWatch
    await setupLangWatch();

    const tracer = getLangWatchTracer("customer-support");

    async function customerSupportGeneration() {
      return tracer.withActiveSpan("customer-support-generation", async () => {
        // Get prompt (automatically linked to trace when API key is present)
        const prompt = await getPrompt("customer-support-bot");

        // Compile prompt with variables
        const compiledPrompt = prompt.compile({
          user_name: "John Doe",
          user_email: "john.doe@example.com",
          input: "I need help with my account",
        });

        // Use with AI SDK (native instrumentation support)
        const result = await generateText({
          model: openai(prompt.model.replace("openai/", "")),
          messages: compiledPrompt.messages,
          experimental_telemetry: { isEnabled: true },
        });

        return result.text;
      });
    }

    // Call the function
    const result = await customerSupportGeneration();
    ```

  </Tab>

</Tabs>

---

[‚Üê Back to Prompt Management Overview](/prompt-management/overview)
