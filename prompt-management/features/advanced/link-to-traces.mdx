---
title: "Link to Traces"
description: "Connect prompts to execution traces for performance monitoring and analysis"
---

Linking prompts to traces enables tracking of metrics and evaluations per prompt version. It's the foundation of improving prompt quality over time.

After linking prompts and traces, you will see information about the prompt in the trace's metadata.

<Frame>
  <img
    className="block"
    src="/images/prompts/view-prompt-trace-span.png"
    alt="Prompt information in trace span details"
  />
</Frame>

For more information about traces and spans, see the [Concepts](/concepts) guide.

## How to Link Prompts to Traces

When you use `getPrompt()` within a trace context, LangWatch automatically links the prompt to the trace:

<Tabs>
<Tab title="Python SDK">

```python
import langwatch
from openai import OpenAI

# Initialize LangWatch
langwatch.setup()

@langwatch.trace()
def customer_support_generation():
    # Get prompt (automatically linked to trace when API key is present)
    prompt = langwatch.prompts.get("customer-support-bot")

    # Format messages with variables
    messages = prompt.format_messages(
        user_name="John Doe",
        user_email="john.doe@example.com",
        input="I need help with my account"
    )

    # Use with OpenAI client
    client = OpenAI()
    completion = client.chat.completions.create(
        model=prompt.model.split("openai/")[1],
        messages=messages
    )

    return completion.choices[0].message.content

# Call the function
result = customer_support_generation()
```

</Tab>
<Tab title="TypeScript SDK">

```typescript
import { getPrompt, setupLangWatch } from "langwatch";
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";

// Initialize LangWatch for tracing and instrumentation
await setupLangWatch();

async function customerSupportGeneration() {
  // Get prompt (automatically linked to trace when API key is present)
  const prompt = await getPrompt("customer-support-bot");

  // Compile prompt with variables - you can pass variables as the second argument
  const compiledPrompt = prompt.compile({
    user_name: "John Doe",
    user_email: "john.doe@example.com",
    input: "I need help with my account",
  });

  // Use with AI SDK (native instrumentation support)
  const result = await generateText({
    model: openai(prompt.model.replace("openai/", "")),
    messages: compiledPrompt.messages,
    experimental_telemetry: { isEnabled: true },
  });

  return result.text;
}

// Call the function
const result = await customerSupportGeneration();
```

</Tab>
</Tabs>

For more detailed information about setting up tracing in your application, see the [Python Integration Guide](/integration/python/guide) or [TypeScript Integration Guide](/integration/typescript/guide).

---

[‚Üê Back to Prompt Management Overview](/prompt-management/overview)
