---
title: Docker Images
description: Overview of LangWatch Docker images and their endpoints
---

# LangWatch Docker Images

LangWatch is composed of three main Docker images that work together to provide the complete LLM observability platform. Each image serves a specific purpose and exposes different endpoints for various functionalities.

## Image Overview

### 1. `langwatch/langwatch` - Main Application

The primary LangWatch application that provides the web interface and core functionality.

**Purpose:**
- Web-based user interface for LangWatch
- API endpoints for trace collection and management
- User authentication and session management
- Dashboard and analytics interface
- Background workers for processing traces and evaluations

**Key Features:**
- Trace collection and storage
- Real-time monitoring dashboard
- User management and authentication
- API endpoints for SDK integration
- Background job processing


### 2. `langwatch/langwatch_nlp` - NLP Backend Service

The NLP backend service that handles advanced language processing tasks and optimization studio workflows.

**Purpose:**
- Optimization studio workflows
- Advanced NLP processing
- Topic clustering and analysis

**Key Features:**
- Optimization studio backend
- Topic clustering service
- Advanced NLP processing


### 3. `langwatch/langevals` - Evaluation Service

The dedicated evaluation service that provides built-in evaluators and evaluation processing capabilities.

**Purpose:**
- Built-in evaluator execution
- Evaluation result processing
- Model-based evaluations
- Safety and quality checks
- Evaluation API endpoints

**Key Features:**
- Comprehensive evaluator library
- LLM-as-a-judge evaluations
- Safety evaluators (content safety, jailbreak detection)
- Quality evaluators (relevance, accuracy, etc.)
- RAG-specific evaluators


## Service Communication

The three services communicate with each other through HTTP endpoints:

```mermaid
graph TD
    A[langwatch/langwatch<br/>:5560] --> B[langwatch/langwatch_nlp<br/>:5561]
    A --> C[langwatch/langevals<br/>:5562]
    B --> C
    A --> D[PostgreSQL]
    A --> E[Elasticsearch/OpenSearch]
    A --> F[Redis]
```

## Docker Compose Configuration

Here's a minimal Docker Compose configuration for LangWatch:

```yaml
version: "3.8"

services:
  app:
    image: langwatch/langwatch:latest
    environment:
      DATABASE_URL: postgresql://prisma:prisma@postgres:5432/mydb?schema=mydb
      ELASTICSEARCH_NODE_URL: http://opensearch:9200
      IS_OPENSEARCH: true
      REDIS_URL: redis://redis:6379
      LANGWATCH_NLP_SERVICE: http://langwatch_nlp:5561
      LANGEVALS_ENDPOINT: http://langevals:5562
      NODE_ENV: production
    ports:
      - "5560:5560"
    depends_on:
      - postgres
      - redis
      - opensearch

  langwatch_nlp:
    image: langwatch/langwatch_nlp:latest
    ports:
      - "5561:5561"
    environment:
      - LANGWATCH_ENDPOINT=http://app:5560

  langevals:
    image: langwatch/langevals:latest
    ports:
      - "5562:5562"

  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: mydb
      POSTGRES_USER: prisma
      POSTGRES_PASSWORD: prisma
    volumes:
      - db-data:/var/lib/postgresql/data

  redis:
    image: redis:alpine
    volumes:
      - redis-data:/data

  opensearch:
    image: langwatch/opensearch-lite:latest
    environment:
      - discovery.type=single-node
      - DISABLE_SECURITY_PLUGIN=true
      - OPENSEARCH_JAVA_OPTS=-Xms256m -Xmx256m
    ports:
      - "9200:9200"
    volumes:
      - opensearch-data:/usr/share/opensearch/data

volumes:
  db-data:
  redis-data:
  opensearch-data:
```

## Health Checks

Each service provides health check endpoints:

- **Main App:** `GET http://localhost:5560/api/health`
- **NLP Service:** `GET http://localhost:5561/health`
- **LangEvals:** `GET http://localhost:5562/health`

## Scaling Considerations

- **Main App:** Can be scaled horizontally for high availability
- **NLP Service:** Can be scaled based on evaluation workload
- **LangEvals:** Can be scaled based on evaluation volume

## Security Notes

- All services should be deployed behind a reverse proxy in production
- Use environment variables for sensitive configuration
- Consider using secrets management for API keys
- Implement proper network segmentation between services

For more detailed configuration options, see the [Environment Variables](/self-hosting/env-variables) page. 