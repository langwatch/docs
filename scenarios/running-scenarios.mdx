---
title: Running Scenarios
description: Execute scenarios against HTTP agents or prompts and analyze results
sidebarTitle: Running Scenarios
---

Once you've created a scenario, you can run it against your agent to test its behavior.

## Choosing What to Test

When you run a scenario, you select what to test against:

| Option | Description |
|--------|-------------|
| **HTTP Agent** | An external API endpoint (your deployed agent) |
| **Prompt** | A versioned prompt from [Prompt Management](/prompt-management/overview) |

The selector shows both options grouped by type:

<Frame>
  <img src="/images/scenarios/target-selector.png" alt="Selector showing HTTP agents and prompts" />
</Frame>

## Running Against an HTTP Agent

Use HTTP agents to test agents deployed as API endpoints. This is the most common option for testing production or staging environments.

### Creating an HTTP Agent

1. In the selector dropdown, click **Add New Agent**
2. Configure the HTTP settings:

<Frame>
  <img src="/images/scenarios/http-agent-form.png" alt="HTTP Agent configuration form" />
</Frame>

### Configuration Options

| Field | Description |
|-------|-------------|
| **Name** | A descriptive name for this agent |
| **URL** | The endpoint to call (e.g., `https://api.example.com/chat`) |
| **Authentication** | Bearer token, API key, basic auth, or none |
| **Body Template** | JSON body with `{{messages}}` placeholder |
| **Response Path** | JSONPath to extract the response |

### Body Template

Use `{{messages}}` to inject the conversation history:

```json
{
  "messages": {{messages}},
  "stream": false
}
```

The placeholder is replaced with an OpenAI-format message array:

```json
[
  {"role": "user", "content": "Hello!"},
  {"role": "assistant", "content": "Hi! How can I help?"}
]
```

Other available variables: `{{input}}` (latest message as string), `{{threadId}}` (conversation ID).

### Response Extraction

Use JSONPath to extract the response from your API:

```
// For: { "choices": [{ "message": { "content": "Hello!" } }] }
$.choices[0].message.content

// For: { "response": "Hello!" }
$.response
```

## Running Against a Prompt

Use prompts to test directly against an LLM using your project's configured model providers. This is useful for:

- Testing prompt changes before deployment
- Quick iteration without infrastructure
- Comparing different prompt versions

### Selecting a Prompt

1. In the selector dropdown, choose from the **Prompts** section
2. Only published prompts (version > 0) appear

<Frame>
  <img src="/images/scenarios/prompt-selector.png" alt="Prompt selector" />
</Frame>

When you run against a prompt, the platform uses the prompt's configured model, system message, and temperature settings with your project's API keys.

<Tip>
  Don't have a prompt yet? Click **Add New Prompt** to open Prompt Management
  in a new tab, create your prompt, then return to select it.
</Tip>

## Executing a Run

From the Scenario Editor, use the **Save and Run** menu:

<Frame>
  <img src="/images/scenarios/save-and-run.png" alt="Save and Run menu" />
</Frame>

1. Click **Save and Run** to open the selector
2. Choose an HTTP Agent or Prompt
3. The scenario runs immediately

The platform:
1. Sends the Situation to the User Simulator
2. Runs a multi-turn conversation between the User Simulator and your agent
3. Passes the conversation to the Judge with your Criteria
4. Records the verdict and reasoning

## Viewing Results

After a run completes, you're taken to the Simulations visualizer.

<Frame>
  <img src="/images/scenarios/run-visualizer.png" alt="Run Visualizer" />
</Frame>

### Conversation View

The main panel shows the full conversation:

- **User messages** - Generated by the User Simulator based on your Situation
- **Assistant messages** - Responses from your agent
- **Tool calls** - If your agent uses tools

### Results Panel

The side panel shows:

| Field | Description |
|-------|-------------|
| **Status** | Pass, Fail, or Error |
| **Criteria Results** | Each criterion with pass/fail and reasoning |
| **Run Duration** | Total execution time |

### Criteria Breakdown

Each criterion shows the Judge's reasoning:

<Frame>
  <img src="/images/scenarios/criteria-results.png" alt="Criteria results" />
</Frame>

<Tip>
  Read the reasoning carefully. It explains exactly what the Judge observed
  and why it made its decision.
</Tip>

## Analyzing Failed Runs

When a scenario fails:

### 1. Read the Failed Criteria

| Reasoning Says... | Likely Issue |
|-------------------|--------------|
| "Agent did not acknowledge..." | Missing empathy |
| "Agent asked 4 questions, exceeding limit of 2" | Too verbose |
| "No mention of refund policy" | Missing information |
| "Conversation ended without resolution" | Incomplete flow |

### 2. Review the Conversation

Step through messages to find where things went wrong:
- Did the agent misunderstand the user?
- Did it get stuck repeating itself?
- Did an error interrupt the flow?

### 3. Fix and Re-run

| Pattern | Fix |
|---------|-----|
| Ignores constraints | Update system prompt to emphasize listening |
| Too verbose | Add brevity instructions |
| Wrong tone | Add tone guidelines |
| Missing info | Add to knowledge base or prompt |

## Run History

Access past runs from the **Simulations** section in the sidebar.

<Frame>
  <img src="/images/scenarios/simulations-list.png" alt="Simulations list" />
</Frame>

The visualizer shows all runs with:
- Pass/fail status
- Timestamps and duration
- Quick navigation to details

Use history to:
- Track progress as you iterate
- Compare runs before and after changes
- Identify regressions
- Share results with your team

## Relationship to Simulations

On-Platform Scenarios and the [Simulations visualizer](/agent-simulations/overview) work together:

1. **Scenarios** define test cases (situation, criteria)
2. **Running a scenario** produces a **simulation**
3. **Simulations** appear in the visualizer

Both On-Platform Scenarios and the [Scenario SDK](https://langwatch.ai/scenario/) produce simulations in the same visualizer, so you can mix approaches.

## Coming Soon

<CardGroup cols={2}>
  <Card title="Suites" icon="layer-group">
    Run multiple scenarios against multiple agents in batch
  </Card>
  <Card title="Turn Builder" icon="list-timeline">
    Create custom conversation scripts
  </Card>
  <Card title="Dataset Mode" icon="database">
    Run scenarios with inputs from a dataset
  </Card>
  <Card title="AI Generation" icon="wand-magic-sparkles">
    Generate scenarios from agent descriptions
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Simulations Visualizer" icon="chart-line" href="/agent-simulations/overview">
    Learn more about analyzing results
  </Card>
  <Card title="Creating Scenarios" icon="plus" href="/scenarios/creating-scenarios">
    Write more scenarios
  </Card>
  <Card title="Scenario SDK" icon="code" href="https://langwatch.ai/scenario/">
    Run scenarios in CI/CD
  </Card>
  <Card title="Prompt Management" icon="file-lines" href="/prompt-management/overview">
    Create versioned prompts
  </Card>
</CardGroup>
