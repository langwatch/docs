---
title: Running Scenarios
description: Execute scenarios against HTTP agents or prompts and analyze results
sidebarTitle: Running Scenarios
---

Once you've created a scenario, you can run it against your agent to test its behavior.

## Choosing What to Test

When you run a scenario, you select what to test against:

| Option | Description | Learn More |
|--------|-------------|------------|
| **HTTP Agent** | An external API endpoint (your deployed agent) | [HTTP Agents →](/agents/http-agents) |
| **Prompt** | A versioned prompt using your project's model providers | [Prompt Management →](/prompt-management/overview) |

The selector shows both options grouped by type:

<Frame>
  <img src="/images/scenarios/target-selector.png" alt="Selector showing HTTP agents and prompts" />
</Frame>

## Running Against an HTTP Agent

Use [HTTP Agents](/agents/http-agents) to test agents deployed as API endpoints. This is the most common option for testing production or staging environments.

To create an HTTP Agent, click **Add New Agent** in the selector dropdown. See [HTTP Agents](/agents/http-agents) for configuration details including:
- URL and authentication setup
- Body templates with message placeholders
- Response extraction with JSONPath

## Running Against a Prompt

Use prompts to test directly against an LLM using your project's configured model providers. This is useful for:

- Testing prompt changes before deployment
- Quick iteration without infrastructure
- Comparing different prompt versions

To use a prompt:
1. In the selector dropdown, choose from the **Prompts** section
2. Only published prompts (version > 0) appear

When you run against a prompt, the platform uses the prompt's configured model, system message, and temperature settings with your project's API keys.

<Tip>
  Don't have a prompt yet? Click **Add New Prompt** to open
  [Prompt Management](/prompt-management/getting-started) in a new tab.
</Tip>

## Executing a Run

From the Scenario Editor, use the **Save and Run** menu:

<Frame>
  <img src="/images/scenarios/save-and-run.png" alt="Save and Run menu" />
</Frame>

1. Click **Save and Run** to open the selector
2. Choose an HTTP Agent or Prompt
3. The scenario runs immediately

The platform:
1. Sends the Situation to the User Simulator
2. Runs a multi-turn conversation between the User Simulator and your agent
3. Passes the conversation to the Judge with your Criteria
4. Records the verdict and reasoning

## Viewing Results

After a run completes, you're taken to the Simulations visualizer.

<Frame>
  <img src="/images/scenarios/run-visualizer.png" alt="Run Visualizer" />
</Frame>

### Conversation View

The main panel shows the full conversation:

- **User messages** - Generated by the User Simulator based on your Situation
- **Assistant messages** - Responses from your agent
- **Tool calls** - If your agent uses tools

### Results Panel

The side panel shows:

| Field | Description |
|-------|-------------|
| **Status** | Pass, Fail, or Error |
| **Criteria Results** | Each criterion with pass/fail and reasoning |
| **Run Duration** | Total execution time |

### Criteria Breakdown

Each criterion shows the Judge's reasoning:

<Frame>
  <img src="/images/scenarios/criteria-results.png" alt="Criteria results" />
</Frame>

<Tip>
  Read the reasoning carefully. It explains exactly what the Judge observed
  and why it made its decision.
</Tip>

## Analyzing Failed Runs

When a scenario fails:

### 1. Read the Failed Criteria

| Reasoning Says... | Likely Issue |
|-------------------|--------------|
| "Agent did not acknowledge..." | Missing empathy |
| "Agent asked 4 questions, exceeding limit of 2" | Too verbose |
| "No mention of refund policy" | Missing information |
| "Conversation ended without resolution" | Incomplete flow |

### 2. Review the Conversation

Step through messages to find where things went wrong:
- Did the agent misunderstand the user?
- Did it get stuck repeating itself?
- Did an error interrupt the flow?

### 3. Fix and Re-run

| Pattern | Fix |
|---------|-----|
| Ignores constraints | Update system prompt to emphasize listening |
| Too verbose | Add brevity instructions |
| Wrong tone | Add tone guidelines |
| Missing info | Add to knowledge base or prompt |

## Run History

Access past runs from the **Simulations** section in the sidebar.

<Frame>
  <img src="/images/scenarios/simulations-list.png" alt="Simulations list" />
</Frame>

The visualizer shows all runs with:
- Pass/fail status
- Timestamps and duration
- Quick navigation to details

Use history to:
- Track progress as you iterate
- Compare runs before and after changes
- Identify regressions
- Share results with your team

## Relationship to Simulations

On-Platform Scenarios and the [Simulations visualizer](/agent-simulations/overview) work together:

1. **Scenarios** define test cases (situation, criteria)
2. **Running a scenario** produces a **simulation**
3. **Simulations** appear in the visualizer

Both On-Platform Scenarios and the [Scenario library](https://langwatch.ai/scenario/) produce simulations in the same visualizer, so you can mix approaches.

## Coming Soon

<CardGroup cols={2}>
  <Card title="Suites" icon="layer-group">
    Run multiple scenarios against multiple agents in batch
  </Card>
  <Card title="Turn Builder" icon="list-timeline">
    Create custom conversation scripts
  </Card>
  <Card title="Dataset Mode" icon="database">
    Run scenarios with inputs from a dataset
  </Card>
  <Card title="AI Generation" icon="wand-magic-sparkles">
    Generate scenarios from agent descriptions
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="HTTP Agents" icon="globe" href="/agents/http-agents">
    Configure HTTP agent endpoints
  </Card>
  <Card title="Prompt Management" icon="file-lines" href="/prompt-management/overview">
    Create versioned prompts
  </Card>
  <Card title="Simulations Visualizer" icon="chart-line" href="/agent-simulations/overview">
    Learn more about analyzing results
  </Card>
  <Card title="Scenario library" icon="code" href="https://langwatch.ai/scenario/">
    Run scenarios in CI/CD
  </Card>
</CardGroup>
