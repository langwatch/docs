---
title: Configuring Targets
description: Set up HTTP, LLM, or Prompt Config targets for your scenarios
---

# Configuring Targets

A **Target** defines how the LangWatch platform invokes your agent during a scenario run. You can configure three types of targets:

| Target Type | Use Case |
|-------------|----------|
| **HTTP** | External API endpoints (production agents, staging environments) |
| **LLM** | Direct model calls for testing prompts |
| **Prompt Config** | Versioned prompts from Prompt Management |

## Accessing the Target Drawer

From the Scenario Editor, click **Configure Target** to open the Target Drawer.

<img src="/images/scenarios/target-drawer.png" alt="Target Drawer" />

## HTTP Target

Use HTTP targets to test agents deployed as API endpoints.

### Configuration

| Field | Description |
|-------|-------------|
| **URL** | The endpoint to call (e.g., `https://api.example.com/chat`) |
| **Method** | HTTP method (typically `POST`) |
| **Headers** | Request headers (authentication, content-type) |
| **Body Template** | JSON body with `{{messages}}` placeholder |

<img src="/images/scenarios/http-target-form.png" alt="HTTP Target Form" />

### Body Template

The body template supports variable interpolation. Use `{{messages}}` to inject the conversation history:

```json
{
  "messages": {{messages}},
  "stream": false
}
```

The `{{messages}}` placeholder is replaced with the OpenAI-format message array:

```json
[
  {"role": "user", "content": "Hello!"},
  {"role": "assistant", "content": "Hi! How can I help?"},
  {"role": "user", "content": "I need a refund"}
]
```

### Authentication

Add authentication headers as needed:

```
Authorization: Bearer sk-your-api-key
X-API-Key: your-api-key
```

<Warning>
  Store sensitive API keys securely. Consider using environment variables or a
  secrets manager for production deployments.
</Warning>

### Expected Response Format

Your endpoint should return a response with the assistant's message:

```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "I'd be happy to help with your refund..."
      }
    }
  ]
}
```

Or a simple string response:

```json
{
  "response": "I'd be happy to help with your refund..."
}
```

## LLM Target

Use LLM targets to test prompts directly against a model using your project's provider keys.

### Configuration

| Field | Description |
|-------|-------------|
| **Model** | The model to use (e.g., `gpt-4`, `claude-3-opus`) |
| **System Prompt** | The system message for the agent |
| **Temperature** | Sampling temperature (0-2) |

<img src="/images/scenarios/llm-target-form.png" alt="LLM Target Form" />

### Model Selection

Select from any model configured in your project's Model Providers. The platform uses your existing provider API keys.

### System Prompt

Define the agent's behavior with a system prompt:

```
You are a helpful customer service agent for Acme Corp. You help customers
with orders, returns, and product questions. Always be polite and empathetic.
If you can't resolve an issue, offer to escalate to a human agent.
```

<Tip>
  LLM targets are great for rapid iteration on prompts. Test different system
  prompts without deploying changes to your production agent.
</Tip>

## Prompt Config Target

Use Prompt Config targets to test versioned prompts from [Prompt Management](/prompt-management/overview).

### Configuration

| Field | Description |
|-------|-------------|
| **Prompt** | Select a prompt from your project |
| **Version** | Select a specific version or use latest |

<img src="/images/scenarios/prompt-config-form.png" alt="Prompt Config Target Form" />

### Benefits

- **Version Control**: Test specific prompt versions
- **A/B Testing**: Compare different prompt versions
- **Consistency**: Ensure scenarios use the same prompt as production

## Choosing a Target Type

| Scenario | Recommended Target |
|----------|-------------------|
| Testing a deployed agent | HTTP |
| Iterating on a prompt | LLM |
| Regression testing prompts | Prompt Config |
| Testing agent tools/integrations | HTTP |
| Quick prototyping | LLM |

## Multiple Targets

You can run the same scenario against multiple targets to compare behavior. This is useful for:

- **A/B testing** different prompt versions
- **Regression testing** after changes
- **Benchmarking** different models

<Note>
  Suites for running scenarios against multiple targets are coming in M2 (Jan 31).
</Note>

## Next Steps

<CardGroup cols={2}>
  <Card title="Running Scenarios" icon="play" href="/scenarios/running-scenarios">
    Execute scenarios and analyze results
  </Card>
  <Card title="Prompt Management" icon="file-lines" href="/prompt-management/overview">
    Learn about versioned prompts
  </Card>
</CardGroup>
