---
title: LLM Evaluation Overview
sidebarTitle: Overview
description: Overview of LLM evaluation features in LangWatch
---

Evaluate and monitor your LLM applications with LangWatch. Explore our evaluation APIs, offline and real-time evaluation on the platform with the Evaluation Wizard, built-in evaluators, datasets, and annotation features to ensure quality and safety for your AI workflows.

<CardGroup cols={2}>
  <Card
    title="Evaluation Tracking API"
    description="Visualize your evaluation metrics in LangWatch without changing your workflow."
    icon="code"
    href="/llm-evaluation/offline/code/evaluation-api"
  />
  <Card
    title="Evaluators List"
    description="Browse built-in evaluators for quality, safety, and more."
    icon="list"
    href="/llm-evaluation/list"
  />
  <Card
    title="Offline Evaluation"
    description="Systematically test LLM quality before production."
    icon="link"
    href="/llm-evaluation/offline/platform/answer-correctness"
  />
  <Card
    title="Real-Time Evaluation"
    description="Monitor and guardrail your LLMs in production with live evaluations."
    icon="link"
    href="/llm-evaluation/realtime/setup"
  />
  <Card
    title="Custom Evaluator Integration"
    description="Instrument your own evaluators and connect them to LangWatch."
    icon="code"
    href="/evaluations/custom-evaluator-integration"
  />
  <Card
    title="Datasets"
    description="Create, manage, and use datasets for evaluation."
    icon="table"
    href="/datasets/overview"
  />
  <Card
    title="Annotations"
    description="Add and manage annotations for deeper analysis."
    icon="pencil"
    href="/features/annotations"
  />
</CardGroup>